{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee251a30",
   "metadata": {},
   "source": [
    "# 토큰화\n",
    "\n",
    "토큰화(tokenization)는 자연어를 **모델이 이해할 수 있는 또는 모델이 다룰 수있는 기본 단위(Token)** 분할하는 과정을 말한다.   \n",
    "토큰으로 나누는 단위는 설계에 따라 문장, 어절, 형태소, 서브워드, 문자, 자모/알파벳 등 다양한 방식으로 나눌 수 있다.   \n",
    "- 예\n",
    "```bash\n",
    "원문: \"자연어 처리는 재미있다\"\n",
    "토큰화: [\"자연어\", \"처리\", \"는\", \"재미있다\"]\n",
    "```\n",
    "\n",
    "## 토큰화 방식\n",
    "- **단어 기반 토큰화(Word-Level Tokenization)**\n",
    "    - 어절(공백으로 구분) 또는 형태소 단위로 단어를 나누는 전통적인 방식이다.\n",
    "    - **한국어**는 교착어로 하나의 단어에 다양한 조사/어미가 결합된다. 그래서 어절단위로 토큰화할 경우 어휘사전의 크기가 기하급수적으로 늘어나는 문제가 있다.\n",
    "      - 예) \"학교\", \"학교가\", \"학교를\", \"학교에\", \"학교에서\", \"학교로\", \"학교의\", ...\n",
    "    - 이로 인해 미등록어휘(OOV - Out of Vocabulary)의 증가, 같은 의미를 가지는 단어들이 Vocab에 중복 등록, 메모리 낭비, 학습효율성 저하 등 다양한 문제가 생긴다.\n",
    "    - 그래서 **한국어의 경우 형태소 단위 토큰화**가 필요하다.\n",
    "\n",
    "- **서브워드 기반(Subword-level) — BPE, WordPiece, Unigram**\n",
    "    - Transformer 기반 모델(BERT, GPT, LLaMA 등)에서 표준으로 사용하는 방식.\n",
    "    - 단어를 기준으로 토큰화하지 않고 **문자(character)와 단어(word)의 중간 수준인 서브워드(subword) 단위로 토큰화**한다.\n",
    "    - **동작 원리**:\n",
    "        - 자주 등장하는 문자열 조합(서브워드)을 하나의 토큰으로 구성한다.\n",
    "        - 빈도가 높은 단어는 하나의 토큰으로, 빈도가 낮거나 희귀한 단어는 여러 서브워드로 분할한다.\n",
    "    - **예시**:\n",
    "        ```bash\n",
    "        입력: \"나는 밥을 먹었습니다. 나는 어제 밥을 했습니다.\"\n",
    "        \n",
    "        서브워드 토큰화 결과 (예시):\n",
    "        [\"나는\", \"밥\", \"을\", \"먹\", \"었\", \"습니다\", \".\", \"나는\", \"어제\", \"밥\", \"을\", \"하\", \"었\", \"습니다\", \".\"]\n",
    "        ```\n",
    "    - **장점**:\n",
    "        - **미등록 단어(OOV) 문제 해결**: 모든 단어를 서브워드 조합으로 표현 가능\n",
    "        - **어휘 사전 크기 최적화**: 단어 단위보다 작고, 문자 단위보다 효율적\n",
    "        - **다국어 지원**: 언어에 구애받지 않는 범용적 토큰화\n",
    "        - **형태론적 의미 포착**: 접두사, 접미사 등의 의미를 학습 가능\n",
    "    - **주요 알고리즘**:\n",
    "        - **BPE (Byte Pair Encoding)**: 가장 빈번한 연속 바이트/문자 쌍을 반복적으로 병합\n",
    "        - **WordPiece**: BERT에서 사용, BPE와 유사하지만 likelihood 기반으로 병합\n",
    "        - **Unigram**: 확률 모델 기반으로 최적의 서브워드 분할 선택\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c1a834",
   "metadata": {},
   "source": [
    "# 한국어 형태소 분석기\n",
    "\n",
    "- kiwipiepy와 konlpy 는 대표적인 한국어 형태소 분석기이다.\n",
    "\n",
    "## kiwipiepy\n",
    "**kiwipiepy**는 C++로 구현된 한국어 형태소 분석기 Kiwi(Korean Intelligent Word Identifier)를 Python 환경에서 사용할 수 있도록 한 라이브러리이다. \n",
    "\n",
    "- 빠른 속도  \n",
    "- 최신 품사 체계 지원  \n",
    "- 사용자 사전 확장 용이  \n",
    "- 최근 가장 널리 쓰이는 한국어 토크나이저 중 하나이다.\n",
    "- https://github.com/bab2min/kiwipiepy\n",
    "  \n",
    "### 설치 방법\n",
    "\n",
    "```bash\n",
    "pip install kiwipiepy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd597bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m31 packages\u001b[0m \u001b[2min 409ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 67ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipywidgets\u001b[0m\u001b[2m==8.1.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyterlab-widgets\u001b[0m\u001b[2m==3.0.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwidgetsnbextension\u001b[0m\u001b[2m==4.0.15\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install ipykernel ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f87abcc",
   "metadata": {},
   "source": [
    "### 주요 클래스 및 함수\n",
    "\n",
    "#### Kiwi 클래스\n",
    "- Kiwi의 핵심 클래스이며, 형태소 분석과 토큰화 기능을 모두 제공한다.\n",
    "- Kiwi 품사는 세종 말뭉치를 기반으로 한다.\n",
    "  - 품사 시작 글자\n",
    "  - 체언(명사, 대명사): `N`, 용언(동사, 형용사): `V`, 수식언(관형사, 부사): `M`,  관계언(조사):`J`, 어미: `E`, 기호: `S`\n",
    "    - https://github.com/bab2min/kiwipiepy?tab=readme-ov-file#%ED%92%88%EC%82%AC-%ED%83%9C%EA%B7%B8\n",
    "- 메소드\n",
    "  - `tokenzie(text)`: 형태소 분석 기반 토큰화 수행\n",
    "  - `analyze(text)`: tokenize보다 좀 더 상세한 분석을 진행한다. 여러 분석결과를 조회할 수있다.\n",
    "  - `add_user_word(word, pos, score)`: 사전에 직접 단어 등록\n",
    "  - `space(text)`: 띄어 쓰기 교정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4b5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d348d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "from pprint import pprint # 자료구조 출력을 보기 좋게 print 해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad4e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 23ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mkiwipiepy\u001b[0m\u001b[2m==0.22.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!uv pip uninstall kiwipiepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dbce74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m5 packages\u001b[0m \u001b[2min 4.35s\u001b[0m\u001b[0m\n",
      "   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m kiwipiepy-model\u001b[2m==0.21.0\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m kiwipiepy \u001b[2m(2.3MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m kiwipiepy\n",
      "      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m kiwipiepy-model\u001b[2m==0.21.0\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 3.97s\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 27ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 10ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkiwipiepy\u001b[0m\u001b[2m==0.21.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mkiwipiepy-model\u001b[0m\u001b[2m==0.22.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkiwipiepy-model\u001b[0m\u001b[2m==0.21.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install kiwipiepy==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2877f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3458278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Token(form='나', tag='NP', start=0, len=1),\n",
      " Token(form='는', tag='JX', start=1, len=1),\n",
      " Token(form='자연어 처리', tag='NNP', start=3, len=6),\n",
      " Token(form='를', tag='JKO', start=9, len=1),\n",
      " Token(form='공부', tag='NNG', start=11, len=2),\n",
      " Token(form='하', tag='XSV', start=13, len=1),\n",
      " Token(form='ᆫ다', tag='EF', start=13, len=2),\n",
      " Token(form='.', tag='SF', start=15, len=1),\n",
      " Token(form='내일', tag='NNG', start=18, len=2),\n",
      " Token(form='은', tag='JX', start=20, len=1),\n",
      " Token(form='뭐', tag='NP', start=22, len=1),\n",
      " Token(form='ᆯ', tag='JKO', start=22, len=1),\n",
      " Token(form='공부', tag='NNG', start=24, len=2),\n",
      " Token(form='하', tag='XSV', start=26, len=1),\n",
      " Token(form='ᆯ까', tag='EF', start=26, len=2),\n",
      " Token(form='?', tag='SF', start=28, len=1)]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "\n",
    "text = \"나는 자연어 처리를 공부한다. \\n내일은 뭘 공부할까?\"\n",
    "tokens = kiwi.tokenize(text)\n",
    "pprint(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed8ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰문자열:나, 원형(lemma):나, 품사(tag): NP, 시작위치:0, 글자수:1, 토큰이 있는 행번호:0, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:0\n",
      "토큰문자열:는, 원형(lemma):는, 품사(tag): JX, 시작위치:1, 글자수:1, 토큰이 있는 행번호:0, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:0\n",
      "토큰문자열:자연어 처리, 원형(lemma):자연어 처리, 품사(tag): NNP, 시작위치:3, 글자수:6, 토큰이 있는 행번호:0, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:1\n",
      "토큰문자열:를, 원형(lemma):를, 품사(tag): JKO, 시작위치:9, 글자수:1, 토큰이 있는 행번호:0, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:2\n",
      "토큰문자열:공부, 원형(lemma):공부, 품사(tag): NNG, 시작위치:11, 글자수:2, 토큰이 있는 행번호:0, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:3\n",
      "토큰문자열:하, 원형(lemma):하, 품사(tag): XSV, 시작위치:13, 글자수:1, 토큰이 있는 행번호:0, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:3\n",
      "토큰문자열:ᆫ다, 원형(lemma):ᆫ다, 품사(tag): EF, 시작위치:13, 글자수:2, 토큰이 있는 행번호:0, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:3\n",
      "토큰문자열:., 원형(lemma):., 품사(tag): SF, 시작위치:15, 글자수:1, 토큰이 있는 행번호:0, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:3\n",
      "토큰문자열:내일, 원형(lemma):내일, 품사(tag): NNG, 시작위치:18, 글자수:2, 토큰이 있는 행번호:1, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:0\n",
      "토큰문자열:은, 원형(lemma):은, 품사(tag): JX, 시작위치:20, 글자수:1, 토큰이 있는 행번호:1, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:0\n",
      "토큰문자열:뭐, 원형(lemma):뭐, 품사(tag): NP, 시작위치:22, 글자수:1, 토큰이 있는 행번호:1, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:1\n",
      "토큰문자열:ᆯ, 원형(lemma):ᆯ, 품사(tag): JKO, 시작위치:22, 글자수:1, 토큰이 있는 행번호:1, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:1\n",
      "토큰문자열:공부, 원형(lemma):공부, 품사(tag): NNG, 시작위치:24, 글자수:2, 토큰이 있는 행번호:1, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:2\n",
      "토큰문자열:하, 원형(lemma):하, 품사(tag): XSV, 시작위치:26, 글자수:1, 토큰이 있는 행번호:1, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:2\n",
      "토큰문자열:ᆯ까, 원형(lemma):ᆯ까, 품사(tag): EF, 시작위치:26, 글자수:2, 토큰이 있는 행번호:1, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:2\n",
      "토큰문자열:?, 원형(lemma):?, 품사(tag): SF, 시작위치:28, 글자수:1, 토큰이 있는 행번호:1, 몇 번째 문장에 있는지 : 0, 문장에서 몇 번째 토큰인지:2\n"
     ]
    }
   ],
   "source": [
    "# Token 객체에서 속성값들 조회\n",
    "for token in tokens:\n",
    "    r = f\"토큰문자열:{token.form}, 원형(lemma):{token.lemma}, 품사(tag): {token.tag}, 시작위치:{token.start}, 글자수:{token.len}, 토큰이 있는 행번호:{token.line_number}, 몇 번째 문장에 있는지 : {token.sub_sent_position}, 문장에서 몇 번째 토큰인지:{token.word_position}\"\n",
    "    print(r) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64786b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<map at 0x17067b29090>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여러 문서를 토큰화 할 때는 list로 묶어서 전달.\n",
    "# 결과: Iterable -> 한 번에 한 문서의 결과를 반환.\n",
    "text_list = [\"나는 자연어 처리를 공부한다. 자연어처리는 nlp라고 한다.\", \"내일은 뭘 공부할까?\"]\n",
    "tokens = kiwi.tokenize(text_list)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c522133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Token(form='나', tag='NP', start=0, len=1), Token(form='는', tag='JX', start=1, len=1), Token(form='자연어 처리', tag='NNP', start=3, len=6), Token(form='를', tag='JKO', start=9, len=1), Token(form='공부', tag='NNG', start=11, len=2), Token(form='하', tag='XSV', start=13, len=1), Token(form='ᆫ다', tag='EF', start=13, len=2), Token(form='.', tag='SF', start=15, len=1), Token(form='자연어 처리', tag='NNP', start=17, len=5), Token(form='는', tag='JX', start=22, len=1), Token(form='nlp', tag='SL', start=24, len=3), Token(form='이', tag='VCP', start=27, len=0), Token(form='라고', tag='EC', start=27, len=2), Token(form='하', tag='VV', start=30, len=1), Token(form='ᆫ다', tag='EF', start=30, len=2), Token(form='.', tag='SF', start=32, len=1)]\n",
      "[Token(form='내일', tag='NNG', start=0, len=2), Token(form='은', tag='JX', start=2, len=1), Token(form='뭐', tag='NP', start=4, len=1), Token(form='ᆯ', tag='JKO', start=4, len=1), Token(form='공부', tag='NNG', start=6, len=2), Token(form='하', tag='XSV', start=8, len=1), Token(form='ᆯ까', tag='EF', start=8, len=2), Token(form='?', tag='SF', start=10, len=1)]\n"
     ]
    }
   ],
   "source": [
    "for token_list in tokens:\n",
    "    print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e8cc210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([Token(form='나', tag='NP', start=0, len=1),\n",
      "   Token(form='는', tag='JX', start=1, len=1),\n",
      "   Token(form='자연어 처리', tag='NNP', start=3, len=6),\n",
      "   Token(form='를', tag='JKO', start=9, len=1),\n",
      "   Token(form='공부', tag='NNG', start=11, len=2),\n",
      "   Token(form='하', tag='XSV', start=13, len=1),\n",
      "   Token(form='ᆫ다', tag='EF', start=13, len=2)],\n",
      "  -36.87178421020508),\n",
      " ([Token(form='나', tag='NP', start=0, len=1),\n",
      "   Token(form='는', tag='JX', start=1, len=1),\n",
      "   Token(form='자연어 처리', tag='NNP', start=3, len=6),\n",
      "   Token(form='를', tag='JKO', start=9, len=1),\n",
      "   Token(form='공부', tag='NNG', start=11, len=2),\n",
      "   Token(form='하', tag='XSV', start=13, len=1),\n",
      "   Token(form='ᆫ다', tag='EC', start=13, len=2)],\n",
      "  -42.1435661315918)]\n"
     ]
    }
   ],
   "source": [
    "# 상세한 토큰화 분석 - analyze()\n",
    "\n",
    "text1 = \"나는 자연어 처리를 공부한다\"\n",
    "result = kiwi.analyze(text1, top_n=2)\n",
    "pprint(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f86e8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([Token(form='나', tag='NP', start=0, len=1),\n",
      "   Token(form='는', tag='JX', start=1, len=1),\n",
      "   Token(form='자연어 처리', tag='NNP', start=3, len=6),\n",
      "   Token(form='를', tag='JKO', start=9, len=1),\n",
      "   Token(form='공부', tag='NNG', start=11, len=2),\n",
      "   Token(form='하', tag='XSV', start=13, len=1),\n",
      "   Token(form='ᆫ다', tag='EF', start=13, len=2),\n",
      "   Token(form='.', tag='SF', start=15, len=1),\n",
      "   Token(form='자연어 처리', tag='NNP', start=17, len=5),\n",
      "   Token(form='는', tag='JX', start=22, len=1),\n",
      "   Token(form='nlp', tag='SL', start=24, len=3),\n",
      "   Token(form='이', tag='VCP', start=27, len=0),\n",
      "   Token(form='라고', tag='EC', start=27, len=2),\n",
      "   Token(form='하', tag='VV', start=30, len=1),\n",
      "   Token(form='ᆫ다', tag='EF', start=30, len=2),\n",
      "   Token(form='.', tag='SF', start=32, len=1)],\n",
      "  -64.47059631347656),\n",
      " ([Token(form='나', tag='VV', start=0, len=1),\n",
      "   Token(form='는', tag='ETM', start=1, len=1),\n",
      "   Token(form='자연어 처리', tag='NNP', start=3, len=6),\n",
      "   Token(form='를', tag='JKO', start=9, len=1),\n",
      "   Token(form='공부', tag='NNG', start=11, len=2),\n",
      "   Token(form='하', tag='XSV', start=13, len=1),\n",
      "   Token(form='ᆫ다', tag='EF', start=13, len=2),\n",
      "   Token(form='.', tag='SF', start=15, len=1),\n",
      "   Token(form='자연어', tag='NNP', start=17, len=3),\n",
      "   Token(form='처리', tag='NNG', start=20, len=2),\n",
      "   Token(form='는', tag='JX', start=22, len=1),\n",
      "   Token(form='nlp', tag='SL', start=24, len=3),\n",
      "   Token(form='이', tag='VCP', start=27, len=0),\n",
      "   Token(form='라고', tag='EC', start=27, len=2),\n",
      "   Token(form='하', tag='VV', start=30, len=1),\n",
      "   Token(form='ᆫ다', tag='EF', start=30, len=2),\n",
      "   Token(form='.', tag='SF', start=32, len=1)],\n",
      "  -74.04469299316406)]\n",
      "[([Token(form='내일', tag='NNG', start=0, len=2),\n",
      "   Token(form='은', tag='JX', start=2, len=1),\n",
      "   Token(form='뭐', tag='NP', start=4, len=1),\n",
      "   Token(form='ᆯ', tag='JKO', start=4, len=1),\n",
      "   Token(form='공부', tag='NNG', start=6, len=2),\n",
      "   Token(form='하', tag='XSV', start=8, len=1),\n",
      "   Token(form='ᆯ까', tag='EF', start=8, len=2),\n",
      "   Token(form='?', tag='SF', start=10, len=1)],\n",
      "  -38.6114616394043),\n",
      " ([Token(form='내일', tag='NNG', start=0, len=2),\n",
      "   Token(form='은', tag='JX', start=2, len=1),\n",
      "   Token(form='뭐', tag='NP', start=4, len=1),\n",
      "   Token(form='ᆯ', tag='JKO', start=4, len=1),\n",
      "   Token(form='공부', tag='NNG', start=6, len=2),\n",
      "   Token(form='하', tag='XSV', start=8, len=1),\n",
      "   Token(form='ᆯ까', tag='EC', start=8, len=2),\n",
      "   Token(form='?', tag='SF', start=10, len=1)],\n",
      "  -41.34712219238281)]\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"나는 자연어 처리를 공부한다. 자연어처리는 nlp라고 한다.\", \"내일은 뭘 공부할까?\"]\n",
    "result = kiwi.analyze(text_list, top_n=2)\n",
    "for r in result:\n",
    "    pprint(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20b88905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'자연어 처리는 재미있는 분야이다. 또 재미있는 것은 뭐가 있을까?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 띄어쓰기 교정  space\n",
    "\n",
    "text = \"자연어처리는재미있는분야이다.또재미있는것은뭐가있을까?\"\n",
    "result = kiwi.space(text)\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45cdcf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자연어 처리는 재미있는 분야이다. 또 재미있는 것은 뭐가 있을까?\n",
      "아버지가 방에 들어가신다.\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"자연어처리는재미있는분야이다.또재미있는것은뭐가있을까?\", \"아버지가방에들어가신다.\"]\n",
    "result = kiwi.space(text_list)\n",
    "result\n",
    "for r in result:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73a9ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(text='비형은 아침나절부터 불쾌합니다 퇴직금 탈탈털어 차린 작은 사무소에는 일거리다운 일거리도 들어오지 않고, 아침부터 꿈자리는 사납고, 장난전화까지...', start=0, end=82, tokens=None, subs=[]),\n",
       " Sentence(text='빡쳐서 오늘은 닫고 휴업할까?', start=83, end=99, tokens=None, subs=[]),\n",
       " Sentence(text='하는 참에 누군가 사무실 문을 두들깁니다.', start=100, end=123, tokens=None, subs=[]),\n",
       " Sentence(text='들어온 건 안경을 쓴 신경질적인 남자였다', start=124, end=146, tokens=None, subs=[]),\n",
       " Sentence(text='비형은 웃으며 방문자를 맞았다.', start=146, end=163, tokens=None, subs=[]),\n",
       " Sentence(text='남자는 감정하듯 주위를 둘러보았다.', start=164, end=183, tokens=None, subs=[]),\n",
       " Sentence(text='여기가, 심부름센터 맞죠?', start=183, end=197, tokens=None, subs=[]),\n",
       " Sentence(text='그의 눈이 푹 가라앉은 접대용 소파에 꽂혔다.', start=198, end=223, tokens=None, subs=[]),\n",
       " Sentence(text='일처리는 확실합니까?', start=224, end=235, tokens=None, subs=[]),\n",
       " Sentence(text='그쯤 되자, 돈줄을 향한 비형의 환한 웃음도 조금 일그러졌다.', start=236, end=270, tokens=None, subs=[]),\n",
       " Sentence(text='불법적인 일만 아니면 뭐든 해드리죠.', start=271, end=291, tokens=None, subs=[]),\n",
       " Sentence(text='남자는 여전히 못 미더운 눈초리였다.', start=292, end=312, tokens=None, subs=[]),\n",
       " Sentence(text='그렇게 못 믿겠음', start=312, end=321, tokens=None, subs=[]),\n",
       " Sentence(text='딴데 가던가.', start=322, end=329, tokens=None, subs=[]),\n",
       " Sentence(text='속으로 궁시렁거린 비형은, 남자에게 무슨 일을 맡기실 것이냐 물었다.', start=330, end=368, tokens=None, subs=[]),\n",
       " Sentence(text='남자가 부탁한 것은 사람 뒷조사.', start=369, end=387, tokens=None, subs=[]),\n",
       " Sentence(text='그는 얼마 전 양녀로 들어온 자신의 여동생에 대한 뒷조사를 부탁한다.', start=388, end=426, tokens=None, subs=[]),\n",
       " Sentence(text='여동생에게 무슨 문제라도...?', start=427, end=444, tokens=None, subs=[]),\n",
       " Sentence(text='남자는 안경을 한번 치켜올리더니 날카로운 어투로 되받았다.', start=445, end=477, tokens=None, subs=[]),\n",
       " Sentence(text='알것 없습니다.', start=478, end=486, tokens=None, subs=[]),\n",
       " Sentence(text='당신은 단지 미화가 집에 오기까지 어디서 무엇을 했는지만 조사해 주면 됩니다.', start=487, end=530, tokens=None, subs=[]),\n",
       " Sentence(text='잘 차려입은 양복에 고급스러워 보이는 시계, 남자는 분명 부잣집 도련님일 것이다.', start=531, end=576, tokens=None, subs=[]),\n",
       " Sentence(text='게다가 최근 남자의 아버지가 들였다는 양녀...', start=577, end=603, tokens=None, subs=[]),\n",
       " Sentence(text='일을 시작하기 전에 편견을 가지는 것은 좋지 않지만, 비형은 일의 윤곽이 대강 그려지는 것 같았다.', start=604, end=659, tokens=None, subs=[]),\n",
       " Sentence(text='좋습니다.', start=660, end=665, tokens=None, subs=[]),\n",
       " Sentence(text='선금으로 십 정도 받죠.', start=666, end=679, tokens=None, subs=[]),\n",
       " Sentence(text='사흘 뒤면 될 것 같은데, 이쪽으로 오시겠습니까?', start=680, end=707, tokens=None, subs=[]),\n",
       " Sentence(text='아니면 제가 그쪽으로 갈까요?', start=708, end=724, tokens=None, subs=[]),\n",
       " Sentence(text='제가 들으러 오죠.', start=725, end=735, tokens=None, subs=[]),\n",
       " Sentence(text='남자는 더는 이런 불쾌한 곳에 있기 싫다는 듯, 수표 한장을 비형 앞에 던져놓고, 휭하니 나가 버렸다', start=736, end=792, tokens=None, subs=[]),\n",
       " Sentence(text='자료가 많을 수록 조사가 더 빨라질텐데..', start=793, end=816, tokens=None, subs=[]),\n",
       " Sentence(text='붙임성 없긴 그는 이번 달치 밥값이나 겨우 될까 싶은 수표로 잠시 손장난을 하다, 이내 파트너를 부르러 사무실을 나섰다', start=817, end=883, tokens=None, subs=[])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장분리 = split_into_sents()\n",
    "txt = \"비형은 아침나절부터 불쾌합니다 퇴직금 탈탈털어 차린 작은 사무소에는 일거리다운 일거리도 들어오지 않고, 아침부터 꿈자리는 사납고, 장난전화까지... 빡쳐서 오늘은 닫고 휴업할까? 하는 참에 누군가 사무실 문aaa불법적인 일만 아니면 뭐든 해드리죠. 남자는 여전히 못 미더운 눈초리였다.그렇게 못 믿겠음 딴데 가던가. 속으로 궁시렁거린 비형은, 남자에게 무슨 일을 맡기실 것이냐 물었다. 남자가 부탁한 것은 사람 뒷조사. 그는 얼마 전 양녀로 들어온 자신의 여동생에 대한 뒷조사를 부탁한다. 여동생에게 무슨 문제라도...? 남자는 안경을 한번 치켜올리더니 날카로운 어투로 되받았다. 알것 없습니다. 당신은 단지 미화가 집에 오기까지 어디서 무엇을 했는지만 조사해 주면 됩니다. 잘 차려입은 양복에 고급스러워 보이는 시계, 남자는 분명 부잣집 도련님일 것이다. 게다가 최근 남자의 아버지가 들였다는 양녀... 일을 시작하기 전에 편견을 가지는 것은 좋지 않지만, 비형은 일의 윤곽이 대강 그려지는 것 같았다. 좋습니다. 선금으로 십 정도 받죠. 사흘 뒤면 될 것 같은데, 이쪽으로 오시겠습니까? 아니면 제가 그쪽으로 갈까요? 제가 들으러 오죠. 남자는 더는 이런 불쾌한 곳에 있기 싫다는 듯, 수표 한장을 비형 앞에 던져놓고, 휭하니 나가 버렸다 자료가 많을 수록 조사가 더 빨라질텐데.. 붙임성 없긴 그는 이번 달치 밥값이나 겨우 될까 싶은 수표로 잠시 손장난을 하다, 이내 파트너를 부르러 사무실을 나섰다\"\n",
    "result = kiwi.split_into_sents(txt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bbdcbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비형은 아침나절부터 불쾌합니다 퇴직금 탈탈 털어 차린 작은 사무소에는 일거리다운 일거리도 들어오지 않고, 아침부터 꿈자리는 사납고, 장난 전화까지...\n",
      "빡 쳐서 오늘은 닫고 휴업할까?\n",
      "하는 참에 누군가 사무실 문을 두들깁니다.\n",
      "들어온 건 안경을 쓴 신경질적인 남자였다\n",
      "비형은 웃으며 방문자를 맞았다.\n",
      "남자는 감정하듯 주위를 둘러보았다.\n",
      "여기가, 심부름 센터 맞죠?\n",
      "그의 눈이 푹 가라앉은 접대용 소파에 꽂혔다.\n",
      "일 처리는 확실합니까?\n",
      "그쯤 되자, 돈줄을 향한 비형의 환한 웃음도 조금 일그러졌다.\n",
      "불법적인 일만 아니면 뭐든 해 드리죠.\n",
      "남자는 여전히 못 미더운 눈초리였다.\n",
      "그렇게 못 믿겠음\n",
      "딴 데 가던가.\n",
      "속으로 궁시렁거린 비형은, 남자에게 무슨 일을 맡기실 것이냐 물었다.\n",
      "남자가 부탁한 것은 사람 뒷조사.\n",
      "그는 얼마 전 양녀로 들어온 자신의 여동생에 대한 뒷조사를 부탁한다.\n",
      "여동생에게 무슨 문제라도...?\n",
      "남자는 안경을 한번 치켜올리더니 날카로운 어투로 되받았다.\n",
      "알 것 없습니다.\n",
      "당신은 단지 미화가 집에 오기까지 어디서 무엇을 했는지만 조사해 주면 됩니다.\n",
      "잘 차려 입은 양복에 고급스러워 보이는 시계, 남자는 분명 부잣집 도련님일 것이다.\n",
      "게다가 최근 남자의 아버지가 들였다는 양녀...\n",
      "일을 시작하기 전에 편견을 가지는 것은 좋지 않지만, 비형은 일의 윤곽이 대강 그려지는 것 같았다.\n",
      "좋습니다.\n",
      "선금으로 십 정도 받죠.\n",
      "사흘 뒤면 될 것 같은데, 이쪽으로 오시겠습니까?\n",
      "아니면 제가 그쪽으로 갈까요?\n",
      "제가 들으러 오죠.\n",
      "남자는 더는 이런 불쾌한 곳에 있기 싫다는 듯, 수표 한 장을 비형 앞에 던져 놓고, 휭하니 나가 버렸다\n",
      "자료가 많을 수록 조사가 더 빨라질 텐데..\n",
      "붙임성 없긴 그는 이번 달치 밥값이나 겨우 될까 싶은 수표로 잠시 손 장난을 하다, 이내 파트너를 부르러 사무실을 나섰다\n"
     ]
    }
   ],
   "source": [
    "# 위 분리된 문장에서 띄어쓰기 교정\n",
    "for sent in result :\n",
    "    print(kiwi.space(sent.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97fc453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(text=\"어제 친구가 '집에 가고 싶다.'라고 이야기 했다.\", start=0, end=28, tokens=None, subs=[Sentence(text='집에 가고 싶다.', start=8, end=17, tokens=None, subs=None)])]\n"
     ]
    }
   ],
   "source": [
    "txt1 = \"어제 친구가 '집에 가고 싶다.'라고 이야기 했다.\"\n",
    "result = kiwi.split_into_sents(txt1) # 결과 : list[Sentence]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab348041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어제 친구가 '집에 가고 싶다.'라고 이야기 했다.\n"
     ]
    }
   ],
   "source": [
    "print(result[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55b4ae59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "집에 가고 싶다.\n"
     ]
    }
   ],
   "source": [
    "sent = result[0]\n",
    "if sent.subs != None:\n",
    "    print(sent.subs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516e7499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(form='박새', tag='NNG', start=0, len=2),\n",
       " Token(form='로', tag='JKB', start=2, len=1),\n",
       " Token(form='이', tag='NNG', start=3, len=1),\n",
       " Token(form='가', tag='JKS', start=4, len=1),\n",
       " Token(form='오', tag='VV', start=6, len=1),\n",
       " Token(form='었', tag='EP', start=6, len=1),\n",
       " Token(form='다', tag='EF', start=7, len=1),\n",
       " Token(form='.', tag='SF', start=8, len=1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################################\n",
    "# 사전에 사용자단어 추가 - add_user_word(단어, 품사,score)\n",
    "# score : 토큰화 할 때 그 단어의 우선순위를 조절하는 가중치 값. 클 수록 더 선호하게 된다\n",
    "#        0: 중립값, 고유명사들은 0를 지정한다.\n",
    "# 딥러닝 : 5, 딥 : 10, 러닝 :10  >> 딥, 러닝으로 나누는 것을 더 선호하게 된다.\n",
    "#####################################################################################\n",
    "\n",
    "text2 = \"박새로이가 왔다.\"\n",
    "kiwi.tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f181f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(form='박새로이', tag='NP', start=0, len=4),\n",
       " Token(form='가', tag='JKS', start=4, len=1),\n",
       " Token(form='오', tag='VV', start=6, len=1),\n",
       " Token(form='었', tag='EP', start=6, len=1),\n",
       " Token(form='다', tag='EF', start=7, len=1),\n",
       " Token(form='.', tag='SF', start=8, len=1)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kiwi.add_user_word(\"박새로이\",\"NP\", 0)\n",
    "kiwi.tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4bacc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kiwipiepy.utils.Stopwords at 0x170728773e0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################\n",
    "# 불용어(Stop words) 처리 - 토큰화 했을 때 제거할 토큰(단어)들.\n",
    "#########################################\n",
    "\n",
    "# kiwi에서 제공하는 불용어\n",
    "\n",
    "from kiwipiepy.utils import Stopwords\n",
    "\n",
    "sw = Stopwords()\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cec7a999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('ᆫ', 'ETM'),\n",
       " ('ᆫ', 'JX'),\n",
       " ('ᆫ다', 'EF'),\n",
       " ('ᆯ', 'ETM'),\n",
       " ('가', 'JKS'),\n",
       " ('같', 'VA'),\n",
       " ('것', 'NNB'),\n",
       " ('게', 'EC'),\n",
       " ('겠', 'EP'),\n",
       " ('고', 'EC'),\n",
       " ('고', 'JKQ'),\n",
       " ('과', 'JC'),\n",
       " ('과', 'JKB'),\n",
       " ('그', 'MM'),\n",
       " ('그', 'NP'),\n",
       " ('기', 'ETN'),\n",
       " ('까지', 'JX'),\n",
       " ('나', 'NP'),\n",
       " ('년', 'NNB'),\n",
       " ('는', 'ETM'),\n",
       " ('는', 'JX'),\n",
       " ('다', 'EC'),\n",
       " ('다', 'EF'),\n",
       " ('다고', 'EC'),\n",
       " ('다는', 'ETM'),\n",
       " ('대하', 'VV'),\n",
       " ('더', 'MAG'),\n",
       " ('던', 'ETM'),\n",
       " ('도', 'JX'),\n",
       " ('되', 'VV'),\n",
       " ('되', 'XSV'),\n",
       " ('들', 'XSN'),\n",
       " ('등', 'NNB'),\n",
       " ('따르', 'VV'),\n",
       " ('때', 'NNG'),\n",
       " ('때문', 'NNB'),\n",
       " ('라', 'EC'),\n",
       " ('라는', 'ETM'),\n",
       " ('로', 'JKB'),\n",
       " ('를', 'JKO'),\n",
       " ('만', 'JX'),\n",
       " ('만', 'NR'),\n",
       " ('말', 'NNG'),\n",
       " ('며', 'EC'),\n",
       " ('면', 'EC'),\n",
       " ('면서', 'EC'),\n",
       " ('명', 'NNB'),\n",
       " ('받', 'VV'),\n",
       " ('보', 'VV'),\n",
       " ('부터', 'JX'),\n",
       " ('사람', 'NNG'),\n",
       " ('성', 'XSN'),\n",
       " ('수', 'NNB'),\n",
       " ('아니', 'VCN'),\n",
       " ('않', 'VX'),\n",
       " ('어', 'EC'),\n",
       " ('어', 'EF'),\n",
       " ('어서', 'EC'),\n",
       " ('어야', 'EC'),\n",
       " ('없', 'VA'),\n",
       " ('었', 'EP'),\n",
       " ('에', 'JKB'),\n",
       " ('에게', 'JKB'),\n",
       " ('에서', 'JKB'),\n",
       " ('와', 'JC'),\n",
       " ('와', 'JKB'),\n",
       " ('우리', 'NP'),\n",
       " ('원', 'NNB'),\n",
       " ('월', 'NNB'),\n",
       " ('위하', 'VV'),\n",
       " ('으로', 'JKB'),\n",
       " ('은', 'ETM'),\n",
       " ('은', 'JX'),\n",
       " ('을', 'ETM'),\n",
       " ('을', 'JKO'),\n",
       " ('의', 'JKG'),\n",
       " ('이', 'JKC'),\n",
       " ('이', 'JKS'),\n",
       " ('이', 'MM'),\n",
       " ('이', 'NP'),\n",
       " ('이', 'VCP'),\n",
       " ('일', 'NNB'),\n",
       " ('일', 'NNG'),\n",
       " ('있', 'VV'),\n",
       " ('있', 'VX'),\n",
       " ('적', 'XSN'),\n",
       " ('제', 'XPN'),\n",
       " ('주', 'VX'),\n",
       " ('중', 'NNB'),\n",
       " ('지', 'EC'),\n",
       " ('지', 'VX'),\n",
       " ('지만', 'EC'),\n",
       " ('지역', 'NNG'),\n",
       " ('통하', 'VV'),\n",
       " ('하', 'VV'),\n",
       " ('하', 'VX'),\n",
       " ('하', 'XSA'),\n",
       " ('하', 'XSV'),\n",
       " ('한', 'MM'),\n",
       " ('화', 'XSN')}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 불용어 조회\n",
    "\n",
    "sw.stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "701665ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sw.stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25f278c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(form='나', tag='NP', start=0, len=1),\n",
       " Token(form='는', tag='JX', start=1, len=1),\n",
       " Token(form='자연어 처리', tag='NNP', start=3, len=6),\n",
       " Token(form='를', tag='JKO', start=9, len=1),\n",
       " Token(form='공부', tag='NNG', start=11, len=2),\n",
       " Token(form='하', tag='XSV', start=13, len=1),\n",
       " Token(form='ᆫ다', tag='EF', start=13, len=2),\n",
       " Token(form='.', tag='SF', start=15, len=1),\n",
       " Token(form='자연어 처리', tag='NNP', start=17, len=6),\n",
       " Token(form='는', tag='JX', start=23, len=1),\n",
       " Token(form='NLP', tag='SL', start=25, len=3),\n",
       " Token(form='이', tag='VCP', start=28, len=0),\n",
       " Token(form='라고', tag='EC', start=28, len=2),\n",
       " Token(form='하', tag='VV', start=31, len=1),\n",
       " Token(form='ᆫ다', tag='EF', start=31, len=2),\n",
       " Token(form='.', tag='SF', start=33, len=1)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"나는 자연어 처리를 공부한다. 자연어 처리는 NLP라고 한다.\"\n",
    "result = kiwi.tokenize(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbcf3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(form='자연어 처리', tag='NNP', start=3, len=6),\n",
       " Token(form='공부', tag='NNG', start=11, len=2),\n",
       " Token(form='자연어 처리', tag='NNP', start=17, len=6),\n",
       " Token(form='NLP', tag='SL', start=25, len=3),\n",
       " Token(form='라고', tag='EC', start=28, len=2)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopwords 들 제거 - stopword객체.filter(토큰list)\n",
    "\n",
    "result2=sw.filter(result)\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fe7c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 추가/삭제\n",
    "# sw.add((\"단어\",\"품사태그\")) # 품사 tag 생략 >> NNP(고유명사)\n",
    "# sw.remove((\"단어\", \"품사Tag\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0cf1dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Token(form='이름', tag='NNG', start=0, len=2), Token(form='이', tag='JKS', start=2, len=1), Token(form='박새로이', tag='NP', start=4, len=4), Token(form='이', tag='VCP', start=8, len=1), Token(form='ᆸ니다', tag='EF', start=8, len=3), Token(form='.', tag='SF', start=11, len=1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Token(form='이름', tag='NNG', start=0, len=2),\n",
       " Token(form='박새로이', tag='NP', start=4, len=4),\n",
       " Token(form='ᆸ니다', tag='EF', start=8, len=3)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = kiwi.tokenize(\"이름이 박새로이입니다.\")\n",
    "print(result)\n",
    "sw.filter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50d89d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(form='이름', tag='NNG', start=0, len=2),\n",
       " Token(form='ᆸ니다', tag='EF', start=8, len=3)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw.add((\"박새로이\",\"NP\"))\n",
    "sw.filter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a025d927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(form='이름', tag='NNG', start=0, len=2),\n",
       " Token(form='박새로이', tag='NP', start=4, len=4),\n",
       " Token(form='ᆸ니다', tag='EF', start=8, len=3)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw.remove(('박새로이', 'NP'))\n",
    "sw.filter(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d553aee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(form='나', tag='NP', start=0, len=1),\n",
       " Token(form='공부', tag='NNG', start=7, len=2),\n",
       " Token(form='어제', tag='NNG', start=11, len=2),\n",
       " Token(form='시작', tag='NNG', start=16, len=2)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#명사만 추출\n",
    "\n",
    "result = kiwi.tokenize(\"나는 NLP 공부를 어제부터 시작했습니다.\")\n",
    "token_list = []\n",
    "for token in result:\n",
    "    if token.tag.startswith('N'): # 명사는 N으로 시작\n",
    "       token_list.append(token)\n",
    "\n",
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2299aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e09d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be9aee9d",
   "metadata": {},
   "source": [
    "## KoNLPy(코엔엘파이)\n",
    "- KoNLPY는 한국어 자연어 처리(Natural Language Processing) 파이썬 라이브러리이다.  한국어 처리를 위한 tokenize, 형태소 분석, 어간추출, 품사부착(POS Tagging) 등의 기능을 제공한다. \n",
    "- http://KoNLPy.org/ko/latest/\n",
    "- 기존의 개발된 다양한 형태소 분석기를 통합해서 동일한 interface로 호출 할 수 있게 해준다.\n",
    "\n",
    "### KoNLPy 설치\n",
    "- 설치 순서\n",
    "  1. Java 실행환경 설치\n",
    "  2. JPype1 설치\n",
    "  3. koNLPy 설치\n",
    "\n",
    "1. **Java 설치**\n",
    "  - https://www.oracle.com/java/technologies/downloads/\n",
    "    - OS에 맞는 설치 버전을 다운받아 설치한다.\n",
    "    - MAC: ARM일 경우: **ARM64 CPU** - ARM64 DMG Installer, **Intel CPU**: x64 DMG Installer\n",
    "  - 시스템 환경변수 설정\n",
    "      - `JAVA_HOME` : 설치 경로 지정\n",
    "      - `Path` : `설치경로\\bin` 경로 지정\n",
    "\n",
    "2. **JPype1 설치**\n",
    "   - 파이썬에서 자바 모듈을 호출하기 위한 연동 패키지\n",
    "   - 설치: `pip install JPype1`\n",
    "\n",
    "3. **KoNLPy 설치**\n",
    "- `pip install konlpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebf69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! uv pip install jpype1 konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a21250e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c7d17",
   "metadata": {},
   "source": [
    "### 형태소 분석기/사전\n",
    "- 형태소 사전을 내장하고 있으며 형태소 분석 함수들을 제공하는 모듈\n",
    "\n",
    "#### KoNLPy 제공 형태소 분석기\n",
    "- Open Korean Text\n",
    "    - 트위터에서 개발\n",
    "    - https://github.com/open-korean-text/open-korean-text\n",
    "- Hannanum(한나눔)\n",
    "    - KAIST Semantic Web Research Center 에서 개발\n",
    "    - http://semanticweb.kaist.ac.kr/hannanum/\n",
    "- Kkma(꼬꼬마)\n",
    "    - 서울대학교 IDS(Intelligent Data Systems) 연구실 개발.\n",
    "    - http://kkma.snu.ac.kr/\n",
    "- Komoran(코모란)\n",
    "    - Shineware에서 개발.\n",
    "    - 오픈소스버전과 유료버전이 있음\n",
    "    - https://github.com/shin285/KOMORAN\n",
    "- Mecab(메카브) \n",
    "    - 일본어용 형태소 분석기를 한국에서 사용할 수 있도록 수정\n",
    "    - windows에서는 설치가 안됨\n",
    "    - https://bitbucket.org/eunjeon/mecab-ko\n",
    "\n",
    "\n",
    "### 형태소 분석기 공통 메소드\n",
    "- `morphs(string)` : 형태소 단위로 토큰화(tokenize)\n",
    "- `nouns(string)` : 명사만 추출하여 토큰화(tokenize)    \n",
    "- `pos(string)`: 품사 부착\n",
    "    - 형태소 분석기 마다 사용하는 품사태그가 다르다.\n",
    "        - https://konlpy-ko.readthedocs.io/ko/v0.5.2/morph/\n",
    "- `tagset`: 형태소 분석기가 사용하는 품사태그 설명하는 속성. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a9426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a80e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 기반으로 토큰화\n",
    "txt = \"자연어 처리 공부를 어제 시작했습니다. 자연어 처리는 쉽습니다.\"\n",
    "\n",
    "result = okt.morphs(txt)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_nouns = okt.nouns(txt) # 명사만 추출해서 토큰화\n",
    "result_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ade2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 부착(POS Tagging)\n",
    "result_pos = okt.pos(txt)\n",
    "result_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조사 구두점 제거\n",
    "result = []\n",
    "for word, tag in result_pos:\n",
    "   if tag not in ['josa', 'Punctuation'] :\n",
    "      result.append((word, tag))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d66f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 tag 를 확인\n",
    "\n",
    "okt.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e00a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "komoran.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원형복원(표제어추출) - Okt 의 기능\n",
    "result = okt.morphs(txt, stem=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비속어 처리 - Okt 기능\n",
    "txt = \"반갑습니당.\"\n",
    "txt = \"이것도 되나욬ㅋㅋㅋㅋㅋㅋㅋ\"\n",
    "okt.morphs(txt, norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e26ee",
   "metadata": {},
   "source": [
    "# WordCloud\n",
    "\n",
    "WordCloud는 텍스트 데이터에서 단어의 등장 빈도를 시각적으로 표현한 그래픽이다.\n",
    "- 특징\n",
    "  - 자주 등장하는 단어일수록 글자 크기가 커진다.\n",
    "  - 텍스트 전체의 주제를 직관적으로 파악할 수 있다.\n",
    "  - 문서의 핵심키워드를 빠르게 파악할 수 있어 텍스트 분석에서 탐색적 데이터 분석(EDA) 단계에서 자주 활용된다.\n",
    "- 설치\n",
    "  - `pip install wordcloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b8a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw\n",
    "kolaw.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088404c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with kolaw.open('constitution.txt') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "txt[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 토큰화 - 명사\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "token_nouns = okt.nouns(txt)\n",
    "print(len(token_nouns))\n",
    "\n",
    "# 2글자 이상인 것만 남기기.\n",
    "token_nouns = [token for token in token_nouns if len(token)>=2]\n",
    "print(len(token_nouns))\n",
    "\n",
    "token_nouns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a81bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어의 개수를 카운트\n",
    "from collections import Counter\n",
    "\n",
    "freq = Counter(token_nouns) #{단어 : 출연개수}\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 높은 순서로 n개 조회 top - k\n",
    "freq.most_common(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc245e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud 정의\n",
    "# 한글폰트 경로가 필요. (코랩에 한글 폰트 설치)\n",
    "\n",
    "# !sudo apt install fonts-nanum # 코랩이서 사용한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bc2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 확인\n",
    "font_path = \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f2f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcloud 구현\n",
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(\n",
    "    font_path=font_path,\n",
    "    max_words = 150, #wordcloud에 나올 최대 단어수, 순서: 빈도수(순서: 빈도수)\n",
    "    min_font_size=10, #제일 빈도수가 적은 단어의 폰트 크기(최소 폰트 크기)\n",
    "    relative_scaling=0.5, # 단어 크기를 빈도수 비율에 얼마나 민감하게 반영할 지\n",
    "                         # 조절하는 값. 0~1 사이 실수 지정. 0: 빈도 차이를 거의 반영 안함.\n",
    "                         # 1: 빈도수 차이를 크게 반영. 적은 것은 더 작게 많은 것은 더 크게\n",
    "    width = 500, # 그래프 크기(pixel)\n",
    "    height = 400,\n",
    "    background_color = \"white\", # 배경색. 디폴트 : 블랙 # 컬러 헥사코드로 지정 가능\n",
    "    prefer_horizontal=0.5 # 가로 방향, 세로방향에서 가로방향으로 쓴 단어의 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c52c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t생성 - 값 : {단어:빈도수}\n",
    "wc_img = wc.generate_from_frequencies(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e827547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wc_img)\n",
    "# 파일로 저장\n",
    "wc_img.to_file(\"constitution_wc.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d07aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(wc_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a1c981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e78c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bdb17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "08_NLP-자연어처리",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
