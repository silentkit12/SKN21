{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2591659b-83e1-45d7-b166-2c85a2f74367",
   "metadata": {},
   "source": [
    "##  필요한 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f59b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets accelerate trl peft hf_transfer langchain python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064544a5-3c4e-4c32-b78e-ca81c1cff0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226ced1",
   "metadata": {},
   "source": [
    "# Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb0bd5-8cee-41df-8385-758601e8051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(load_dotenv(\"env\"))\n",
    "\n",
    "login(os.getenv('HUGGINGFACE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef323ce1-aeb4-446d-a331-6468e83d42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "user_id = \"kgmyh\"  # 본인 Huggingface 사용자명 입력\n",
    "data_id = f\"{user_id}/naver_economy_news_stock_instruct_dataset-100_samples\"\n",
    "dataset = load_dataset(data_id)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df11d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset['train']\n",
    "test_set = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e70e3c",
   "metadata": {},
   "source": [
    "# sLLM Model Load\n",
    "\n",
    "## Base Model Load\n",
    "- 한국어를 충분히 학습한 모델을 선택한다.\n",
    "\n",
    "- kakao의 kanana 모델을 base 모델로 파인튜닝을 진행한다.\n",
    "  - https://tech.kakao.com/author/Kanana\n",
    "  - https://github.com/kakao/kanana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212cd2a7-ac0b-4c8c-8926-c1d5d80f2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"kakaocorp/kanana-nano-2.1b-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\" \n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97add76a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "515577ed",
   "metadata": {},
   "source": [
    "### Base모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1fa57b-c012-4220-9259-b33d760b148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################################################################### \n",
    "#  입력 프롬프트 생성\n",
    "#\n",
    "#  - kanana, Llama 동일한 프롬프트 형식\n",
    "#    - Instruction 모델이 학습할 때 사용한 prompt 형식에 맞춰 입력데이터를 변환을 해야 한다.\n",
    "#  - `tokenizer.apply_chat_template()`: \n",
    "#       - {\"role\":\"역할\", \"content\":\"content\"} 구조로 입력을 하면 실제 모델의 모델의 입력형식으로 변환해 주는 메소드.\n",
    "###############################################################################################################################################\n",
    "\n",
    "content = \"오늘 서울 날씨 어때요?\"\n",
    "message = [  \n",
    "        {\"role\": \"system\", \"content\": \"당신은 인공지능 날씨 예보관입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "]\n",
    "\n",
    "# 모델 입력 형식에 맞게 프롬프트 변환.\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "    message,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6966af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# 토큰화\n",
    "########################\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd07ff0-f8b9-4b73-b73d-db28c9a2c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25256f7a-8401-4b24-ba07-e96d753340f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## \n",
    "# 답변 생성\n",
    "## 생성모델: model.generate() 메소드 사용\n",
    "##########################################\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100, # 답변 토큰 수 제한\n",
    "        do_sample=True,     # True: 확률 분포에 따라 다음 토큰을 무작위로 선택(확률 기준: top_p, temperature), False: 가장 높은 확률의 토큰만 선택\n",
    "        top_p=0.95,         # 다음에 올 확률 순으로 토큰들 정렬 → 누적 → 0.95(지정한값) 도달 시점까지 상위토큰만 남긴다. 낮으면(예: 0.5) 창의성↓, 너무 높으면(1.0) 창의성↑\n",
    "        temperature=0.8,    # 토큰의 다양성을 지정. 낮을수록(0에 가까울수록) 높은 확률의 토큰을 더욱 선택. 낮으면(예: 0.5) 너무 높으면(1.0) 창의성↑\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b83e1c-c5bb-497d-9063-8873e1aba0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33bf3f-83b0-4e8f-83f7-5c23a367dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1b3d25-c663-497a-9586-f548030fb64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e14dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답변 부분만 잘라서 디코딩\n",
    "response = tokenizer.decode(output[0][inputs[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2375848",
   "metadata": {},
   "source": [
    "# 학습 전에 답변 결과 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6f0af",
   "metadata": {},
   "source": [
    "### 추론용 프롬프트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc3fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#  System 프롬프트\n",
    "######################################\n",
    "\n",
    "system_prompt = '''# Instruction\n",
    "당신은 금융 뉴스의 핵심 내용을 요약해 설명하고, 뉴스가 특정 상장 종목에 미치는 긍정/부정 영향 여부, 이유, 근거 등을 분석하는 금융 분석 전문가입니다.\n",
    "사용자에 의해 입력된 뉴스 기사를 분석해서 **한국에 상장된 주식 종목에 영향을 주는지 판단**하고, Output Indicator에 제시된 기준에 따라 구조화된 JSON 형식으로 결과를 출력하세요.\n",
    "\n",
    "## 분석 기준\n",
    "1. 뉴스가 **한국 주식 종목에 영향을 주는지 판단**하세요.\n",
    "2. 영향을 준다면 다음 항목을 출력하세요.\n",
    "   - `\"is_stock_related\": true`\n",
    "   - 뉴스에 **긍정적** 영향을 받는 **회사이름들**\n",
    "   - 뉴스에 **부정적** 영향을 받는 **회사이름들**\n",
    "   - 뉴스가 각 회사에 **긍정적 또는 부정적 영향을 주는지 이유**\n",
    "     - 반드시 **뉴스기사에 언급된 내용 기반으로 작성한다.** 뉴스기사에 없는 내용을 꾸며서 임의로로 작성하지 않습니다.\n",
    "     - `None`, 유추, 추정, 일반 논평 금지합니다.\n",
    "   - 뉴스 요약 (3줄 이내)\n",
    "3. 뉴스가 한국 주식 종목에 영향을 주지 않는다면 다음 항목을 출력하세요.\n",
    "   - `\"is_stock_related\": false`\n",
    "   - 뉴스 요약 (3줄 이내)\n",
    "\n",
    "# 출력 지시사항 (Output Indicator)\n",
    "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "\n",
    "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
    "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
    "\n",
    "Here is the output schema:\n",
    "```\n",
    "{\"properties\": {\"is_stock_related\": {\"description\": \"한국 주식과 관련있는 뉴스인지 여부\", \"title\": \"Is Stock Related\", \"type\": \"boolean\"}, \"positive_stocks\": {\"description\": \"뉴스기사에 긍정적인 영향을 받는 회사들의 이름들.\", \"items\": {\"type\": \"string\"}, \"title\": \"Positive Stocks\", \"type\": \"array\"}, \"positive_reason\": {\"description\": \"뉴스내용 중 positive_stocks에 있는 각 회사들에 긍정적 영향을 주는 내용. {\\\"회사이름\\\":\\\"긍정적인 이유\\\"}\", \"items\": {\"additionalProperties\": {\"type\": \"string\"}, \"type\": \"object\"}, \"title\": \"Positive Reason\", \"type\": \"array\"}, \"negative_stocks\": {\"description\": \"뉴스기사에 부정적인 영향을 받는 회사들의 이름들.\", \"items\": {\"type\": \"string\"}, \"title\": \"Negative Stocks\", \"type\": \"array\"}, \"negative_reason\": {\"description\": \"뉴스내용 중 negative_stocks에 있는 각 회사들에 부정적 영향을 주는 내용. {\\\"회사이름\\\":\\\"부정적인 이유\\\"}\", \"items\": {\"additionalProperties\": {\"type\": \"string\"}, \"type\": \"object\"}, \"title\": \"Negative Reason\", \"type\": \"array\"}, \"summary\": {\"description\": \"뉴스기사 요약\", \"title\": \"Summary\", \"type\": \"string\"}}, \"required\": [\"is_stock_related\", \"positive_stocks\", \"positive_reason\", \"negative_stocks\", \"negative_reason\", \"summary\"]}\n",
    "```\n",
    "\n",
    "## 출력 조건:\n",
    "- 뉴스에 영향을 받은 회사들은 **반드시 한국 증시에 상장된 종목** 이어야 합니다.\n",
    "- 뉴스에 있는 내용만 출력결과에 포함시킵니다.\n",
    "- 긍정/부정 종목은 실제 뉴스기사에 영향을 받는 회사들만 포함하세요.\n",
    "- 모든 문자열은 큰따옴표(`\"`)로 감쌉니다.\n",
    "- 문자열 안에 따옴표가 필요하면 작은따옴표(`'`)를 사용합니다.\n",
    "- 모든 키(Key)는 출력 지시사항에 명시된 property들과 정확히 일치해야 합니다.\n",
    "- `\"positive_reasons\"` 및 `\"negative_reasons\"`의 값은 `None`이 될 수 없습니다.\n",
    "- json format을 잘 지켜 응답데이터를 만듭니다. 배열이나 object의 마지막 항목 뒤에 `,` 를 붙이지 마세요.\n",
    "- 오직 유효한 JSON 문자열(UTF-8, RFC8259 준수)만 출력합니다.\n",
    "- 절대 다른 텍스트, 주석, 설명, 코드 블록 표기(```), 또는 따옴표 외의 문자열을 추가하면 안 됩니다.\n",
    "\n",
    "## 출력 예시 (Examples)\n",
    "\n",
    "### 뉴스가 특정 주식종목들에 **긍정적 영향이 주는 경우**:\n",
    "{'is_stock_related': True,\n",
    " 'negative_reasons': [],\n",
    " 'negative_stocks': [],\n",
    " 'positive_reasons': [{'세라젬': '루게릭병 환우 지원 캠페인 후원과 의료가전 지원 등 사회공헌활동을 통해 기업 이미지와 브랜드 가치가 긍정적으로 부각됨'}],\n",
    " 'positive_stocks': ['세라젬'],\n",
    " 'summary': '세라젬이 루게릭병 환우를 위한 아이스버킷 챌린지 런 행사를 후원하며 의료가전과 건강기능식품 등을 지원했다. 캠페인은 루게릭병 환우 지원과 기부 문화 확산을 목표로 한다. 세라젬은 다양한 사회공헌활동을 지속하고 있다.'\n",
    "}\n",
    "\n",
    "### 뉴스의 내용이 특정 주식종목들에 **부정적 영향이 주는 경우**:\n",
    "{\n",
    "    \"is_stock_related\": true,\n",
    "    \"positive_stocks\": [],\n",
    "    \"positive_reasons\": [],\n",
    "    \"negative_stocks\": [\n",
    "        \"포스코\",\n",
    "        \"현대제철\"\n",
    "    ],\n",
    "    \"negative_reasons\": [\n",
    "        {\"포스코\": \"정부가 수입규제국 조사에 적극 대응하고 비관세장벽 해소를 위해 민관 협력 강화 방침을 밝혀 철강 분야에서 수출 피해 최소화 기대\"},\n",
    "        {\"현대제철\": \"철강·금속 품목에 대한 수입규제 대응 강화로 불합리한 무역제한 조치 개선 가능성이 높아져 수출 환경 개선 기대\"}\n",
    "    ],\n",
    "    \"summary\": \"산업부는 수입 규제국의 조사에 대응하고 비관세장벽 해소를 위한 협의를 진행했다. 규제 대상 국가는 26개국, 건수는 199건에 달한다.\"\n",
    "}\n",
    "\n",
    "### **뉴스기사가 주식 종목과 관련 없는 경우**:\n",
    "{\n",
    "    \"is_stock_related\": false,\n",
    "    \"positive_stocks\": [],\n",
    "    \"positive_reasons\": [],\n",
    "    \"negative_stocks\": [],\n",
    "    \"negative_reasons\": [],\n",
    "    \"summary\": \"정황근 농림축산식품부 장관이 단순가공식품 부가가치세 면제 시행 상황을 점검했다. 된장, 고추장 코너를 방문하며 현장을 살폈다.\"\n",
    "}'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946cdab-71fb-458a-a7f8-fd6f64104059",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "# 추론할 1개의 샘플 데이터 생성\n",
    "# 뉴스 기사 제목 + \"\\n\\n\" + 뉴스 기사 내용\n",
    "################################################################\n",
    "\n",
    "idx = 5\n",
    "\n",
    "sample = dataset['train'][idx]\n",
    "user_input = sample[\"title\"]+\"\\n\\n\"+sample['document']\n",
    "prompt = [\n",
    "    {\"role\":\"system\", \"content\":system_prompt},\n",
    "    {\"role\":\"user\", \"content\": user_input}\n",
    "]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720d9515-d939-456f-8da4-271ca3f83caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Pipeline을 이용해 실행\n",
    "#  - task: text-generation\n",
    "#  - pipeline은 (role base) chat format 으로 입력한다. \n",
    "#########################################################\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer )\n",
    "res = pipe(prompt,  max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab329c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3343d32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = res[0]['generated_text'][-1]\n",
    "print(response['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca101393",
   "metadata": {},
   "source": [
    "# 파인 튜닝 \n",
    "\n",
    "## 데이터셋 만들기\n",
    "\n",
    "### 프롬프트 생성\n",
    "- LLM 모델은 학습할때 사용한 프롬프트 형식이 있다.\n",
    "    - 모델 마다 형식이 다르기 때문에 확인이 필요하다.\n",
    "    - 모델이 사용한 tokenizer의 chat_template 속성을 이용해 조회할 수 있다.\n",
    "      - `tokenizer.chat_template`\n",
    "      - `jija2` 템플릿 엔진 문법으로 작성됨.\n",
    "- 학습데이터를 LLM의 프롬프트 형식으로 생성한다.\n",
    "\n",
    "### Llama 모델의 chat template 형식\n",
    "```\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "[시스템 역할 지침]<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "[유저 질문]<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "[모델의 답변]<|eot_id|>\n",
    "```\n",
    "- `<|begin_of_text|>`: 시퀀스의 시작을 나타내는 토큰.\n",
    "- `<|start_header_id|>ROLE<|end_header_id|>`: Role(발화자) 지정 - system | user | assistant\n",
    "- `메세지 내용<|eot_id|>`:  Role 메세지. 시퀀스 종료를 나타내는 토큰(`<|eot_id|>`)\n",
    "\n",
    "#### 예\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "당신은 인공지는 날씨 예보관입니다.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "오늘 서울 날씨 어때요?<|eot_id|>\n",
    "```\n",
    "> ### Gemma format chat template 형식\n",
    "> \n",
    "> ```xml\n",
    "> <bos><start_of_turn>Role\n",
    "> {사용자 입력-input}<end_of_turn>\n",
    "> <start_of_turn>model\n",
    "> {AI 답변-label}<end_of_turn>\n",
    "> ```\n",
    "> -\t`<bos>`: 시퀀스의 시작을 나타내는 토큰.\n",
    "> -\t`<start_of_turn>`: 각 대화의 시작을 나타낸다.\n",
    "> -\t`Role`: Role(발화자) 지정 - **user** | **model** \n",
    ">   - gemma는 user와 model 두가지 role을 이용해 instruction 모델을 학습함.\n",
    ">   - gemma 는 system role을 사용하지 않는다. 그래서 **system prompt는 user role의 content에 넣어준다.**\n",
    "> - `메세지`\n",
    ">   - Role 다음 줄에 이어서 메세지를 입력한다.\n",
    "> - `<end_of_turn>`: 대화의 끝을 나타냅니다.\n",
    "> - 예\n",
    "> ```xml\n",
    "> <bos><start_of_turn>user\n",
    "> AI에 대해 설명해주세요.<end_of_turn>\n",
    "> <start_of_turn>model\n",
    "> AI은 컴퓨터 시스템이 인간의 지능적 기능을 모방하여 데이터를 처리하고 의사결정을 수행하는 기술입니다.<end_of_turn>\n",
    "> ```\n",
    "\n",
    "> ### Alapaca format chat template\n",
    "> ```\n",
    "> ### Instruction:\n",
    "> [시스템 역할 지침]\n",
    "> \n",
    "> [유저 질문]\n",
    "> \n",
    "> ### Response:\n",
    "> [모델의 답변]\n",
    "> ```\n",
    "> - `### Instruction`: - 사용자 입력/질문\n",
    "> - `### Response`: - 모델의 답변\n",
    "> - 특수 토큰 대신 마크다운 스타일의 헤더 사용\n",
    "> - 보통 맨 앞에 표준 instruction 문구 포함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16af714-fc82-404b-a072-300b9a394da3",
   "metadata": {},
   "source": [
    "### InputPromptCreator(프롬프트 생성 클래스) 정의\n",
    "\n",
    "- `create_pipeline_prompt()`:\n",
    "    - 뉴스기사 제목, 뉴스기사내용을 받아서 파이프라인에 입력할 chat 형식 프롬프트를 생성한다.\n",
    "- `create_generate_prompt:()`:\n",
    "    - 뉴스기사 제목, 뉴스기사내용을 받아서 모델의 자체 chat 형식의 프롬프트를 생성한다.\n",
    "    - **system_prompt, user_input** 으로 구성된 prompt 생성\n",
    "- `create_train_prompt()`:\n",
    "    - Dataset의 개별 데이터를 입력받아서 모델 학습을 위한 chat 프롬프트 생성.\n",
    "    - **system_prompt, user_input, label** 로 구성된 prompt 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610af68-8ee5-44eb-b372-fceac8eb935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "# 학습 용 프롬프트 생성 함수\n",
    "####################################################################\n",
    "\n",
    "\n",
    "from textwrap import dedent\n",
    "class InputPromptCreator:\n",
    "    \"\"\"모델별 형식에 맞춰 입력 프롬프트를 생성한다.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer=None):\n",
    "        \"\"\"\n",
    "        모델의 chat template을 제공하는 tokenizer와 system 프롬프트를 받아서 초기화\n",
    "        Args:\n",
    "            tokenizer : 모델의 chat template을 제공하는 tokenizer\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_prompt = dedent('''\n",
    "        # Instruction\n",
    "        당신은 금융 뉴스의 핵심 내용을 요약해 설명하고, 뉴스가 특정 상장 종목에 미치는 긍정/부정 영향 여부, 이유, 근거 등을 분석하는 금융 분석 전문가입니다.\n",
    "        사용자에 의해 입력된 뉴스 기사를 분석해서 **한국에 상장된 주식 종목에 영향을 주는지 판단**하고, Output Indicator에 제시된 기준에 따라 구조화된 JSON 형식으로 결과를 출력하세요.\n",
    "        \n",
    "        ## 분석 기준\n",
    "        1. 뉴스가 **한국 주식 종목에 영향을 주는지 판단**하세요.\n",
    "        2. 영향을 준다면 다음 항목을 출력하세요.\n",
    "           - `\"is_stock_related\": true`\n",
    "           - 뉴스에 **긍정적** 영향을 받는 **회사이름들**\n",
    "           - 뉴스에 **부정적** 영향을 받는 **회사이름들**\n",
    "           - 뉴스가 각 회사에 **긍정적 또는 부정적 영향을 주는지 이유**\n",
    "             - 반드시 **뉴스기사에 언급된 내용 기반으로 작성한다.** 뉴스기사에 없는 내용을 꾸며서 임의로로 작성하지 않습니다.\n",
    "             - `None`, 유추, 추정, 일반 논평 금지합니다.\n",
    "           - 뉴스 요약 (3줄 이내)\n",
    "        3. 뉴스가 한국 주식 종목에 영향을 주지 않는다면 다음 항목을 출력하세요.\n",
    "           - `\"is_stock_related\": false`\n",
    "           - 뉴스 요약 (3줄 이내)\n",
    "        \n",
    "        # 출력 지시사항 (Output Indicator)\n",
    "        The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
    "        \n",
    "        As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
    "        the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
    "        \n",
    "        Here is the output schema:\n",
    "        ```\n",
    "        {\"properties\": {\"is_stock_related\": {\"description\": \"한국 주식과 관련있는 뉴스인지 여부\", \"title\": \"Is Stock Related\", \"type\": \"boolean\"}, \"positive_stocks\": {\"description\": \"뉴스기사에 긍정적인 영향을 받는 회사들의 이름들.\", \"items\": {\"type\": \"string\"}, \"title\": \"Positive Stocks\", \"type\": \"array\"}, \"positive_reason\": {\"description\": \"뉴스내용 중 positive_stocks에 있는 각 회사들에 긍정적 영향을 주는 내용. {\\\"회사이름\\\":\\\"긍정적인 이유\\\"}\", \"items\": {\"additionalProperties\": {\"type\": \"string\"}, \"type\": \"object\"}, \"title\": \"Positive Reason\", \"type\": \"array\"}, \"negative_stocks\": {\"description\": \"뉴스기사에 부정적인 영향을 받는 회사들의 이름들.\", \"items\": {\"type\": \"string\"}, \"title\": \"Negative Stocks\", \"type\": \"array\"}, \"negative_reason\": {\"description\": \"뉴스내용 중 negative_stocks에 있는 각 회사들에 부정적 영향을 주는 내용. {\\\"회사이름\\\":\\\"부정적인 이유\\\"}\", \"items\": {\"additionalProperties\": {\"type\": \"string\"}, \"type\": \"object\"}, \"title\": \"Negative Reason\", \"type\": \"array\"}, \"summary\": {\"description\": \"뉴스기사 요약\", \"title\": \"Summary\", \"type\": \"string\"}}, \"required\": [\"is_stock_related\", \"positive_stocks\", \"positive_reason\", \"negative_stocks\", \"negative_reason\", \"summary\"]}\n",
    "        ```\n",
    "        \n",
    "        ## 출력 조건:\n",
    "        - 뉴스에 영향을 받은 회사들은 **반드시 한국 증시에 상장된 종목** 이어야 합니다.\n",
    "        - 뉴스에 있는 내용만 출력결과에 포함시킵니다.\n",
    "        - 긍정/부정 종목은 실제 뉴스기사에 영향을 받는 회사들만 포함하세요.\n",
    "        - 모든 문자열은 큰따옴표(`\"`)로 감쌉니다.\n",
    "        - 문자열 안에 따옴표가 필요하면 작은따옴표(`'`)를 사용합니다.\n",
    "        - 모든 키(Key)는 출력 지시사항에 명시된 property들과 정확히 일치해야 합니다.\n",
    "        - `\"positive_reasons\"` 및 `\"negative_reasons\"`의 값은 `None`이 될 수 없습니다.\n",
    "        - json format을 잘 지켜 응답데이터를 만듭니다. 배열이나 object의 마지막 항목 뒤에 `,` 를 붙이지 마세요.\n",
    "        - 오직 유효한 JSON 문자열(UTF-8, RFC8259 준수)만 출력합니다.\n",
    "        - 절대 다른 텍스트, 주석, 설명, 코드 블록 표기(```), 또는 따옴표 외의 문자열을 추가하면 안 됩니다.\n",
    "        \n",
    "        ## 출력 예시 (Examples)\n",
    "        \n",
    "        ### 뉴스가 특정 주식종목들에 **긍정적 영향이 주는 경우**:\n",
    "        {'is_stock_related': True,\n",
    "         'negative_reasons': [],\n",
    "         'negative_stocks': [],\n",
    "         'positive_reasons': [{'세라젬': '루게릭병 환우 지원 캠페인 후원과 의료가전 지원 등 사회공헌활동을 통해 기업 이미지와 브랜드 가치가 긍정적으로 부각됨'}],\n",
    "         'positive_stocks': ['세라젬'],\n",
    "         'summary': '세라젬이 루게릭병 환우를 위한 아이스버킷 챌린지 런 행사를 후원하며 의료가전과 건강기능식품 등을 지원했다. 캠페인은 루게릭병 환우 지원과 기부 문화 확산을 목표로 한다. 세라젬은 다양한 사회공헌활동을 지속하고 있다.'\n",
    "        }\n",
    "        \n",
    "        ### 뉴스의 내용이 특정 주식종목들에 **부정적 영향이 주는 경우**:\n",
    "        {\n",
    "            \"is_stock_related\": true,\n",
    "            \"positive_stocks\": [],\n",
    "            \"positive_reasons\": [],\n",
    "            \"negative_stocks\": [\n",
    "                \"포스코\",\n",
    "                \"현대제철\"\n",
    "            ],\n",
    "            \"negative_reasons\": [\n",
    "                {\"포스코\": \"정부가 수입규제국 조사에 적극 대응하고 비관세장벽 해소를 위해 민관 협력 강화 방침을 밝혀 철강 분야에서 수출 피해 최소화 기대\"},\n",
    "                {\"현대제철\": \"철강·금속 품목에 대한 수입규제 대응 강화로 불합리한 무역제한 조치 개선 가능성이 높아져 수출 환경 개선 기대\"}\n",
    "            ],\n",
    "            \"summary\": \"산업부는 수입 규제국의 조사에 대응하고 비관세장벽 해소를 위한 협의를 진행했다. 규제 대상 국가는 26개국, 건수는 199건에 달한다.\"\n",
    "        }\n",
    "        \n",
    "        ### **뉴스기사가 주식 종목과 관련 없는 경우**:\n",
    "        {\n",
    "            \"is_stock_related\": false,\n",
    "            \"positive_stocks\": [],\n",
    "            \"positive_reasons\": [],\n",
    "            \"negative_stocks\": [],\n",
    "            \"negative_reasons\": [],\n",
    "            \"summary\": \"정황근 농림축산식품부 장관이 단순가공식품 부가가치세 면제 시행 상황을 점검했다. 된장, 고추장 코너를 방문하며 현장을 살폈다.\"\n",
    "        }''')\n",
    "\n",
    "    def create_pipeline_prompt(self, news_title:str, news_document:str) -> list[dict]:\n",
    "        \"\"\"파이프라인에 입력할 chat 형식 프롬프트생성한다.\n",
    "        ```\n",
    "        [\n",
    "            {\"role\":\"system\", \"content\":시스템프롬프트},\n",
    "            {\"role\":\"user\", \"content\": news_title+\"\\n\\n\"+news_document}\n",
    "        ]\n",
    "        ```\n",
    "        \"\"\"\n",
    "        news = news_title+\"\\n\\n\"+news_document\n",
    "        message = [\n",
    "            {\"role\":\"system\", \"content\":self.system_prompt},\n",
    "            {\"role\":\"user\", \"content\": news}\n",
    "        ]\n",
    "        return message\n",
    "\n",
    "    def create_generate_prompt(self, news_title:str, news_document:str) -> str:\n",
    "        \"\"\"뉴스 제목과 내용을 받아서 tokenizer를 이용해 모델의 자체 입력 chat 형식의 프롬프트를 생성한다.\n",
    "        tokenizer.apply_chat_template() 메소드의 결과를 반환\n",
    "        ```\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        {시스템 프롬프트}<|eot_id|>\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        {입력 - title+document}<|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        ```\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise Exception(\"Tokenizer가 없습니다. generate_prompt를 사용하려면 모델의 tokenizer가 필요합니다.\")\n",
    "        message = self.create_pipeline_prompt(news_title, news_document)\n",
    "        prompt =  tokenizer.apply_chat_template(\n",
    "            message,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def create_train_prompt(self, datapoint:dict) -> dict:\n",
    "        \"\"\"\n",
    "        Dataset의 개별 데이터를 입력받아서 모델 학습을 위한 chat 프롬프트생성\n",
    "        프롬프트 형식\n",
    "        ```\n",
    "        <|begin_of_text|>\n",
    "        <|start_header_id|>system<|end_header_id|>\n",
    "        {시스템 프롬프트}<|eot_id|>\n",
    "        <|start_header_id|>user<|end_header_id|>\n",
    "        {입력 - title+document}<|eot_id|>\n",
    "        <|start_header_id|>assistant<|end_header_id|>\n",
    "        {답변 - Label}<|eot_id|>                       ← create_generate_prompt() 에서 답변이 추가됨\n",
    "        ```\n",
    "        Args:\n",
    "            datapoint (dict): 변환할 데이터\n",
    "    \n",
    "        Returns:\n",
    "            str: 모델 학습을 위한 프롬프트. \n",
    "        \"\"\"\n",
    "        chat_template = dedent('''\n",
    "            <|begin_of_text|>\n",
    "            <|start_header_id|>system<|end_header_id|>\n",
    "            {system_prompt}<|eot_id|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            {input_content}<|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            {label}<|eot_id|>''')\n",
    "    \n",
    "        content = datapoint['title']+\"\\n\"+datapoint['document']\n",
    "        prompt = chat_template.format(system_prompt=self.system_prompt, \n",
    "                                      input_content=content, \n",
    "                                      label=datapoint['label'])\n",
    "\n",
    "        return {\"train_prompt\":prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6103ce1-ae09-4e36-8aee-08b29034c23b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_creator = InputPromptCreator(tokenizer=tokenizer)\n",
    "prompt_creator.create_pipeline_prompt(\">>>>>>>>>뉴스기사제목\",\"<<<<<<<뉴스내용\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b14dc12-3c25-4d9e-8537-4b4f23fc1fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_creator.create_generate_prompt(\">>>>>>>>>뉴스기사제목\",\"<<<<<<<뉴스내용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb886b27-8d58-4110-90d6-48b1cc8f123b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_creator.create_train_prompt(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5addeaa7",
   "metadata": {},
   "source": [
    "### InputPromptCreator.create_train_prompt()를 이용해 입력 프롬프트 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# Trainset\n",
    "##########################\n",
    "\n",
    "llama_input_creator = InputPromptCreator(tokenizer)\n",
    "# Dataset.map(함수) -> 개별 데이터를 함수에 전달. \n",
    "#   함수의 반환값({feature이름:값})을 Dataset에 추가. \n",
    "trainset = train_set.map(llama_input_creator.create_train_prompt, remove_columns=list(train_set.features))\n",
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09653c2a-77be-4a72-9e6f-b72ef8812069",
   "metadata": {},
   "outputs": [],
   "source": [
    "### # Testset\n",
    "##########################\n",
    "\n",
    "testset = test_set.map(llama_input_creator.create_train_prompt, remove_columns=list(test_set.features))\n",
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801263cf-ac85-4e26-9fc6-dbd057552905",
   "metadata": {},
   "source": [
    "## 파인튜닝\n",
    "\n",
    "### Data Collator\n",
    "\n",
    "- 학습 도중 입력 데이터를 받아 전처리하는 함수\n",
    "  - Dataset -> Data Collator함수 -> 모델\n",
    "    - 데이터셋에서 모델에 전달되는 batch를 받아서 모델에 입력전에 해야하는 처리를 담당하는 함수(Callable).\n",
    "\n",
    "- 구현할 내용\n",
    "  - 모델 chat 형식의 문자열을 transformers 모델에 입력하기 위한 inputs를 만든다.\n",
    "    \n",
    "  ```json\n",
    "    {\n",
    "      \"input_ids\":입력 sequence의 토큰 ID들,\n",
    "      \"attention_mask\":입력토큰과 padding구분,\n",
    "      \"labels\": input_ids에서 답변부분 masking. 답변은 토큰ID 나머지는 -100으로 채운다. \n",
    "    }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea864d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "max_seq_length = 8192\n",
    "\n",
    "def collate_fn(batch: list[dict]) -> dict[str, torch.Tensor]:\n",
    "    \"\"\"모델 학습시 batch를 입력받아 모델 입력에 맞게 처리해서 반환\n",
    "    Args:\n",
    "        batch (list[dict]): 배치 데이터, dict: {\"train_prompt\":input text}\n",
    "\n",
    "    Returns:\n",
    "        dict[str, torch.Tensor]: 모델 입력에 맞게 처리된 배치 데이터 (inputs, attention_mask, labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    new_batch = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for prompt in batch:\n",
    "        \n",
    "        text = prompt['train_prompt'].strip()\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length, # max_length이하는 자른다. truncation=True\n",
    "            padding=False,             # padding은 뒤에서 수동으로 처리할 것이기 때문에 padding처리하지 않는다.\n",
    "            return_tensors=None,       # list로 반환.\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        labels = [-100] * len(input_ids) # 답변 token값을 넣을 리스트. system/user prompt 부분은 -100으로, 답변 부분은 토큰값들로 변경.\n",
    "\n",
    "        ########################################################\n",
    "        # chat prompt에서 답변 부분을 찾아서 labels를 구성한다. \n",
    "        ########################################################\n",
    "        assistant_header = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "\n",
    "        eot_token = \"<|eot_id|>\"\n",
    "        eot_tokens = tokenizer.encode(eot_token, add_special_tokens=False)\n",
    "\n",
    "        i = 0\n",
    "        while i <= len(input_ids) - len(assistant_tokens):\n",
    "            if input_ids[i:i + len(assistant_tokens)] == assistant_tokens:\n",
    "                start = i + len(assistant_tokens)\n",
    "                end = start\n",
    "                while end <= len(input_ids) - len(eot_tokens):\n",
    "                    if input_ids[end:end + len(eot_tokens)] == eot_tokens:\n",
    "                        break\n",
    "                    end += 1\n",
    "                for j in range(start, end):\n",
    "                    labels[j] = input_ids[j]\n",
    "                for j in range(end, end + len(eot_tokens)):\n",
    "                    labels[j] = input_ids[j]\n",
    "                break\n",
    "            i += 1\n",
    "        \n",
    "        ##################################################################\n",
    "        # 생성된 input_ids, attention_mask, labels를 new_batch에 추가한다.\n",
    "        ##################################################################\n",
    "        new_batch[\"input_ids\"].append(input_ids)\n",
    "        new_batch[\"attention_mask\"].append(attention_mask)\n",
    "        new_batch[\"labels\"].append(labels)\n",
    "    \n",
    "\n",
    "    ######################################################\n",
    "    #  패딩 처리 - 동적 패딩.\n",
    "    #  -  배치내 입력중 가장 긴 sample에 길이를 맞춘다.\n",
    "    ######################################################\n",
    "    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])            \n",
    "    for i in range(len(new_batch[\"input_ids\"])):\n",
    "        pad_len = max_length - len(new_batch[\"input_ids\"][i]) \n",
    "        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * pad_len)\n",
    "        new_batch[\"attention_mask\"][i].extend([0] * pad_len)\n",
    "        new_batch[\"labels\"][i].extend([-100] * pad_len)\n",
    "\n",
    "    # list -> torch.Tensor로 변환.\n",
    "    for k in new_batch:\n",
    "        new_batch[k] = torch.tensor(new_batch[k])\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c284a5e-ca60-4df7-93d1-1539f407e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# 확인\n",
    "######################\n",
    "example = [trainset[11], trainset[2], trainset[3]]\n",
    "batch = collate_fn(example)\n",
    "\n",
    "print(\"batch:\")\n",
    "print(\"input_ids 크기:\", batch[\"input_ids\"].shape)\n",
    "print(\"attention_mask 크기:\", batch[\"attention_mask\"].shape)\n",
    "print(\"labels 크기:\", batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da2145",
   "metadata": {},
   "source": [
    "## SFTConfig 설정\n",
    "\n",
    "### LoRA 설정\n",
    "\n",
    "- LoRAConfig 주요 매개변수\n",
    "\n",
    "| 매개변수                | 의미/역할                                         | 주요 옵션·예시                                                                                                             |\n",
    "| ------------------- | --------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |\n",
    "| **r**               | LoRA 어댑터의 랭크(정보량/두께). 값이 크면 표현력은 높아지지만 메모리를 더 사용하게 된다.<br>2B ~ 7B 모델의 경우 8이 가장 효율이 좋은 것으로 알려졌다.  | 8, 16, 32 등                                                                                                          |\n",
    "| **lora\\_alpha**     | - LoRA 어댑터의 출력 크기에 곱해주는 스케일링 계수. $\\Delta W = BA$에 곱해주는 스케일링 계수. $\\Delta W = \\cfrac{\\alpha}{r}BA$<br>- `BA`는 초기에 매우 작은 값으로 시작해서 초반 업데이트가 너무느리게 진행될 수있다. 이것을 보정해 학습 안정성을 높이는 값이다.| 16, 32, 64 등                                                                                                         |\n",
    "| **lora\\_dropout**   | 어댑터에만 적용되는 드롭아웃 확률. 과적합 방지                    | 0.05, 0.1 등                                                                                                          |\n",
    "| **bias**            | 기존 모델의 bias 파라미터도 LoRA로 튜닝할지 여부               | \"none\"(권장), \"all\"                                                                                                    |\n",
    "| **target\\_modules** | LoRA를 어떤 레이어(부분)에 적용할지 지정                     | \"q\\_proj\", \"k\\_proj\", \"v\\_proj\", \"o\\_proj\"\n",
    "| **task\\_type**      | LoRA가 적용될 문제 유형(파인튜닝 목적)                      | \"CAUSAL\\_LM\"(생성)<br>\"SEQ\\_CLS\"(분류)<br>\"SEQ\\_2\\_SEQ\\_LM\"(번역/요약)<br>\"TOKEN\\_CLS\"(토큰분류)<br>\"QUESTION\\_ANSWERING\"(질의응답) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266ebe9-d3ce-4fed-956b-6a830acfe0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Peft(LoRA) 어뎁터 설정\n",
    "###############################\n",
    "\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2068ea4",
   "metadata": {},
   "source": [
    "### SFTConfig 설정\n",
    "- 전체 학습관련 설정\n",
    "\n",
    "### SFTConfig 주요 매개변수\n",
    "\n",
    "| 매개변수                                | 설명                                                  | 주요 예시 값/옵션                       |\n",
    "| ----------------------------------- | --------------------------------------------------- | ---------------------------------------       |\n",
    "| **output\\_dir**                     | 결과 모델/로그를 저장할 경로 또는 저장소 ID                          | `\"./results\"`                |\n",
    "| **num\\_train\\_epochs**              | 전체 데이터를 몇 번 반복 학습할지(에포크 수)                          | `3`, `5`                    |\n",
    "| **per\\_device\\_train\\_batch\\_size** | 각 GPU(디바이스)에서 한 번에 입력할 데이터 수(배치 크기)                 | `2`, `4`, `8`            |\n",
    "| **gradient\\_accumulation\\_steps**   | 여러 미니배치를 모아 한 번에 업데이트(실질 배치 크기 키우기)                 | `1`, `2`, `4`        |\n",
    "| **gradient\\_checkpointing**         | 메모리 절약 기능(필요할 때만 중간 계산값 저장)                         | `True`, `False`            |\n",
    "| **optim**                           | 최적화 알고리즘(학습 방법)                                     | `\"adamw_torch_fused\"`              |\n",
    "| **logging\\_steps**                  | 몇 step마다 로그를 출력할지                                   | `10`, `50`, `100`                   |\n",
    "| **save\\_strategy**                  | 모델 저장 방식(주기)                                        | `\"steps\"`, `\"epoch\"`                  |\n",
    "| **save\\_steps**                     | 몇 step마다 모델을 저장할지                                   | `50`, `100`                         |\n",
    "| **bf16**                            | bfloat16 연산 사용(GPU 메모리 절약)                          | `True`, `False`                      |\n",
    "| **learning\\_rate**                  | 파라미터 업데이트 속도(학습률)                                   | `1e-4`, `5e-5`                   |\n",
    "| **max\\_grad\\_norm**                 | 그래디언트 클리핑 임계값(학습 안정화)                               | `0.3`, `1.0`                  |\n",
    "| **warmup\\_ratio**                   | 워밍업 단계 비율(초기 학습률 천천히 증가)                            | `0.03`, `0.1`                |\n",
    "| **lr\\_scheduler\\_type**             | 학습률 조정 방식                                           | `\"constant\"`, `\"linear\"`               |\n",
    "| **push\\_to\\_hub**                   | 학습 결과를 Hugging Face Hub로 업로드할지 여부                   | `True`, `False`                  |\n",
    "| **hub\\_model\\_id**                  | 업로드할 Hugging Face Hub 저장소 ID                        |                                        |\n",
    "| **hub\\_token**                      | Hugging Face Hub 인증 토큰 사용 여부                        | `True`                                 |\n",
    "| **remove\\_unused\\_columns**         | 학습에 안 쓰는 데이터 컬럼 자동 제거 여부                            | `True`, `False`               |\n",
    "| **dataset\\_kwargs**                 | 데이터셋 추가 옵션(딕셔너리 형태)                                 | `{\"skip_prepare_dataset\": True}` |\n",
    "| **report\\_to**                      | 학습 로그를 기록할 대상(예: wandb, tensorboard, 빈 리스트면 기록 안 함) | `None`, `[\"wandb\"]`        |\n",
    "| **max\\_length**                     | 한 입력에 허용되는 최대 토큰(단어) 수 ()                             | `2048`, `4096`, `8192`          |\n",
    "| **label\\_names**                    | Trainer가 label로 인식할 컬럼명                             | `[\"labels\"]`                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae4257-f495-4a5a-a7e6-852455deb61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "# Hugginface Model-hub에 업로드할 모델 ID\n",
    "user_id = \"\" # ****본인 Huggingface ID 입력****\n",
    "model_id = \"kanana-nano-2.1b-instruct-finace_news-finetuning-100\"\n",
    "args = SFTConfig(\n",
    "    output_dir=model_id,                    # 학습 결과(체크포인트/최종 모델)를 저장할 디렉터리 경로\n",
    "    save_strategy=\"steps\",                  # 모델 저장 전략(\"steps\": 스텝 간격으로 저장)\n",
    "    save_steps=50,                          # 얼마나 자주 체크포인트를 저장할지(step 단위)\n",
    "\n",
    "    num_train_epochs=epochs,                # 전체 데이터셋을 몇 번 반복 학습할지(에포크 수)\n",
    "    per_device_train_batch_size=2,          # GPU 하나당 학습 배치 크기(micro-batch size)\n",
    "    gradient_accumulation_steps=4,          # gradient를 몇 스텝 누적해서 업데이트할지(효과적 batch 증가) :contentReference[oaicite:1]{index=1}\n",
    "    gradient_checkpointing=True,            # 활성값 저장 대신 필요할 때 재계산하여 메모리 절감 :contentReference[oaicite:2]{index=2}\n",
    "                                            # 중간 활성값을 역전파 때 저장하지 않고 필요 시 재계산하여 GPU 메모리를 크게 줄이는 기법입니다\n",
    "\n",
    "    optim=\"adamw_torch_fused\",              # 옵티마이저 종류(AdamW 변형, PyTorch fused 구현-Fused 구현은 연산이 통합되어 더 빠르고 효율적입니다.)\n",
    "    logging_steps=20,                       # 로깅(손실 등 출력)을 몇 스텝마다 할지\n",
    "    learning_rate=1e-4,                     # 학습률(learning rate)\n",
    "    warmup_ratio=0.03,                      # warmup 비율(전체 step의 몇 %를 lr warmup으로 쓸지-전체 스텝의 3%를 warmup으로 쓰며, 이 구간 동안 lr이 낮은 값에서 점점 증가합니다.)\n",
    "    max_grad_norm=0.3,                      # gradient clipping 최대 노름(norm) 값(그래디언트 폭주 방지)\n",
    "    bf16=True,                              # bfloat16 mixed precision 활성화(메모리/연산 최적화)\n",
    "    lr_scheduler_type=\"constant\",           # learning rate scheduler(조정) 유형전략(여기서는 학습률 일정 유지-\"constant\"는 warmup 후 일정한 lr을 유지합니다.)\n",
    "\n",
    "    push_to_hub=True,                       # 학습 완료 후 Hugging Face Hub에 업로드 여부\n",
    "    hub_model_id=f\"{user_id}/{model_id}\",       # Hub에 업로드할 모델 리포지토리 ID\n",
    "    hub_token=True,                         # Hub 업로드 시 사용할 토큰 정보\n",
    "\n",
    "    remove_unused_columns=False,            # 기본적으로 Hugging Face Tokenizer/Trainer는 모델에 필요 없는 컬럼을 제거합니다. False면 제거하지 않고 그대로 두며, 사전에 collate_fn에서 필요한 처리가 된 경우에 쓰입니다.\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # SFTTrainer가 자동으로 데이터 전처리/packing을 수행하는 단계를 건너뛰게 합니다. 이미 데이터셋이 원하는 형식으로 준비된 경우 True로 설정합니다.\n",
    "    max_length=max_seq_length,              # 최대 토큰 길이 제한(max_length)\n",
    "    label_names=[\"labels\"],                 # loss 계산 시 사용할 레이블 컬럼 이름\n",
    "    report_to=None                          # 로깅/모니터링 대상(off이면 wandb/tensorboard 등 비활성)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cc5ff1-ea33-4ef2-a1dc-a6efd97cbbe4",
   "metadata": {},
   "source": [
    "## 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99cff0-d149-4ed5-8e8d-b62642165aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "# Trainer 객체 생성 \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=trainset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b610a8-79e8-4dfe-ab1d-fe66c5aace1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "#  학습 시작\n",
    "#############################\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a78dac-8258-41f6-9e1a-b049ca8f0ba9",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7ab74",
   "metadata": {},
   "source": [
    "## LoRA 파인튜닝 모델 사용법\n",
    "\n",
    "- 위 과정에서 LoRA(Low-Rank Adaptation) 어댑터를 파인튜닝하여 Hugging Face Hub에 업로드한 상태\n",
    "- LoRA는 전체 모델을 학습하는 대신 작은 어댑터만 학습하는 효율적인 방법\n",
    "\n",
    "- **사용 시 필요한 구성 요소**\n",
    "    - Base Model (기본 모델)\n",
    "    - LoRA Adapter (파인튜닝된 어댑터)\n",
    "    - 이 둘을 merge해서 사용한다. \n",
    "- **사용 방법 2가지**\n",
    "    1. **개별 로드 후 병합**\n",
    "        - 기본 모델과 LoRA 어댑터를 각각 다운로드\n",
    "        - 런타임에서 두 구성 요소를 병합하여 사용\n",
    "\n",
    "    2. **병합된 모델 직접 사용**\n",
    "        - 미리 병합된 상태의 모델을 다운로드\n",
    "        - 바로 사용 가능\n",
    "    - 첫 번째 방법은 메모리 효율성과 유연성을 제공하고, 두 번째 방법은 사용 편의성을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02062bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "user_id = \"\" # ****본인 Huggingface ID 입력****\n",
    "base_model_id = \"kakaocorp/kanana-nano-2.1b-instruct\"\n",
    "lora_model_id = f\"{user_id}/kanana-nano-2.1b-instruct-finace_news-finetuning-100\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "llama_input_creator = InputPromptCreator(tokenizer)\n",
    "\n",
    "# base model 불러오기\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# base model에 LoRA adapter 추가\n",
    "model = PeftModel.from_pretrained(base_model, lora_model_id)\n",
    "\n",
    "# 파이프라인 생성\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7af557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title = \"'반도체 초호황'…삼성전자 사상 첫 분기 영업익 20조 넘나\"\n",
    "news_document = \"\"\"삼성전자가 지난해 4분기 사상 첫 분기 영업이익 20조를 달성할 수 있을지 업계와 시장의 이목이 집중되고 있다. 인공지능(AI) 수요 확대에 따른 메모리 초호황이 전례 없는 ‘역대급’ 실적을 만들고 있기 때문이다.\n",
    "4일 업계에 따르면 삼성전자(005930)는 오는 8일 오전 지난해 4분기 잠정실적을 공개한다. 금융정보업체 에프엔가이드의 집계를 보면, 삼성전자 실적 컨센서스는 매출 89조2173억원, 영업이익 16조4545억원이다. 다만 일부 증권사들은 20조원 넘는 영업이익을 점치고 있다. 메모리 초호황기였던 2018년 3분기 당시 17조5700억원을 뛰어넘는 역대 최대 실적을 달성할 수 있다는 의미다. 지난해 3분기 1년3개월 만에 ‘10조 클럽’에 복귀한 이후 불과 한 개 분기 만에 ‘20조 클럽’에 입성할 가능성이 커졌다.\n",
    "호실적을 견인하고 있는 것 D램, 낸드플래시 등 범용 메모리 가격의 급등이다. 시장조사업체 D램익스체인지에 따르면, 지난해 12월 기준 PC용 D램 범용 제품(DDR4 8Gb 1Gx8) 평균 고정거래 가격은 9.3달러로 집계됐다. 2024년 말(1.35달러)과 비교해 6.9배 급등했다. 트렌드포스는 서버용 DDR5와 5세대 고대역폭메모리(HBM3E) 간 가격 격차가 올해 말엔 4~5배에서 1~2배 수준까지 줄어들 것이라고 예상했다.\n",
    "이는 삼성전자, SK하이닉스, 마이크론 등 메모리 3사가 AI 시대 들어 수익성이 높은 HBM의 생산 비중을 높이는 와중에 범용 메모리 수요가 갑자기 폭증했기 때문이다. 특히 삼성전자는 압도적인 D램 생산능력으로 이익 규모를 늘린 것으로 추정된다. 업계에 따르면 삼성전자의 범용 D램 생산능력(월 웨이퍼 투입량 기준)은 약 50만5000장 수준이다. SK하이닉스와 마이크론은 각각 약 39만5000장, 29만5000장이다.\n",
    "HBM 호조도 긍정적인 영향을 미쳤다. 경쟁사에 비해 HBM 시장에서 뒤처졌던 삼성전자는 재설계를 통해 성능을 끌어올렸고, 지난해 말 엔비디아의 HBM3E 품질 테스트를 통과했다. ‘아픈 손가락’ 파운드리 사업의 적자 폭도 1조원 내로 줄어들었다는 관측이 나온다. 올해 파운드리, 시스템LSI 등 비메모리 사업은 흑자 전환을 위한 반등에 박차를 가한다는 방침이다.\n",
    "삼성전자는 메모리 초호황을 등에 업고 올해 내내 호실적을 낼 것으로 보인다. 산업계 한 관계자는 “삼성전자가 올해 영업이익 100조원을 돌파할 수 있다는 전망도 충분히 설득력 있게 받아들여지고 있다”고 말했다. 연간 영업이익 100조원은 전례가 없는 기록이다.\n",
    "LG전자와 LG에너지솔루션도 각각 9일과 8일 잠정실적을 발표한다. 에프엔가이드에 따르면, LG전자의 실적 컨센서스는 매출 23조5748억원, 영업손실 76억원이다. 전 사업부 차원의 희망퇴직 등 비용 효율화에 따른 것으로 읽힌다.\n",
    "LG에너지솔루션도 전기차 캐즘 충격파에 부진한 성적표를 받아들 전망이다. 다시 적자로 전환할 게 유력하다. LG에너지솔루션은 최근 들어서만 약 13조6000억원 규모의 배터리 계약을 해지했다.\"\"\"\n",
    "\n",
    "message = llama_input_creator.create_pipeline_prompt(news_title=news_title, news_document=news_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360cd3e5-1487-4990-9975-f677eb0af2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_title = \"\\\"구글 제치고 압도적 1위\\\" 네이버 큰일 했네\"\n",
    "# news_document = \"\"\"네이버 검색 점유율이 3년만에 60%를 넘겨 구글을 제치고 압도적 1위를 차지 토종 포털사이트의 자존심을 지킨 것으로 나타났다.\n",
    "# 4일 시장조사업체 ‘인터넷트렌드’에 따르면 지난해 네이버 국내 검색 점유율은 평균 62.86%로 집계됐다.\n",
    "# 전년도인 2024년 58.14%와 비교해 4.72% 증가한 수치로 네이버 점유율이 60%를 넘긴 것은 2022년 61.20%를 기록한 이후 3년 만이다.\n",
    "# 2위인 구글은 전년 동기 대비 3.45% 감소한 29.55%의 검색 점유율을 보였다.\n",
    "# 국내와 해외를 대표하는 양대 플랫폼의 검색 점유율 격차가 1년 사이 더 벌어진 것으로 국내 검색 시장에서 네이버 지배력이 더 확고해진 것으로 해석된다.\n",
    "# 3위부터는 2위와 큰 격차를 보여 영향력이 미미했다. 마이크로소프트(MS) 검색 엔진 빙(Bing)은 전년 점유율 2.91% 대비 소폭 상승한 3.12%로 3위를, 다음은 전년 3.72% 대비 소폭 감소한 2.94%로 4위를 각각 차지했다.\n",
    "# 특히 줌과 야후 등 기타 검색 사이트는 점유율 1%를 넘지 못했다.\n",
    "# 업계에서는 네이버의 반등에 대해 검색 신뢰도 향상 차원에서 네이버가 다양한 기술을 시도한 노력의 결과라고 풀이했다.\n",
    "# 특히 네이버가 작년 신규 출시한 AI 검색 ‘AI 브리핑’과 맞물려 검색 접촉 횟수가 더 늘었다는 분석이다.\"\"\"\n",
    "\n",
    "# message = llama_input_creator.create_pipeline_prompt(news_title=news_title, news_document=news_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa313bc-7cdf-4e1e-9c4f-de9e64c5e7fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266fd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipe(message, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a952ea2-e0f3-4c28-b19c-1a8e2b315c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0]['generated_text'][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3bcd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Merge된 모델 한번에 받아오기\n",
    "#####################################\n",
    "\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "fine_tuned_model = AutoPeftModelForCausalLM.from_pretrained(lora_model_id, device_map=\"auto\", dtype=torch.bfloat16)\n",
    "pipe2 = pipeline(\"text-generation\", model=fine_tuned_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bee5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = pipe2(message, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c054923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0]['generated_text'][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict:\n",
    "\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        self.prompt_creator = InputPromptCreator(pipeline.tokenizer)\n",
    "\n",
    "    def __call__(self, title: str, content: str) -> str:\n",
    "        \"\"\"뉴스 제목과 뉴스 내용을 받아서 프롬프트 생성 후 pipeline에 전달하여 응답 내용만 추출 해서 반환\n",
    "\n",
    "        Args:\n",
    "            title (str): 뉴스제목\n",
    "            content (str): 뉴스내용\n",
    "\n",
    "        Returns:\n",
    "            str: LLM 응답\n",
    "        \"\"\"\n",
    "        message = self.prompt_creator.create_pipeline_prompt(title, content)\n",
    "        response = self.pipeline(message)\n",
    "        return response[0]['generated_text'][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a7921-83f0-4e7b-bc70-6d221ce03c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Predict(pipe)\n",
    "predict(news_title, news_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7c7a7-8971-4699-a23b-93216e676be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
