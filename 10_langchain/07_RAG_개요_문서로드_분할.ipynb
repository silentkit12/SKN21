{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227316ea",
   "metadata": {},
   "source": [
    "# RAG(Retrieval Augmented Generation)\n",
    "- [RAG](https://python.langchain.com/v0.1/docs/modules/data_connection/)은 *Retrieval Augmented Generation*의 약자로, **검색 기반 생성 기법**을 의미한다. 이 기법은 LLM이 특정 문서에 기반하여 보다 정확하고 신뢰할 수 있는 답변을 생성할 수 있도록 돕는다.     \n",
    "- 사용자의 질문에 대해 자체적으로 구축한 데이터베이스(DB)나 외부 데이터베이스에서 질문과 관련된 문서를 검색하고, 이를 질문과 함께 LLM에 전달한다.\n",
    "- LLM은 같이 전달된 문서를 바탕으로 질문에 대한 답변을 생성한다. \n",
    "- 이를 통해 LLM이 학습하지 않은 내용도 다룰 수 있으며, 잘못된 정보를 생성하는 환각 현상(*hallucination*)을 줄일 수 있다.\n",
    "\n",
    "## RAG와 파인튜닝(Fine Tuning) 비교\n",
    "\n",
    "### 파인튜닝(Fine Tuning)\n",
    "\n",
    "- **정의**: 사전 학습(pre-trained)된 LLM에 특정 도메인의 데이터를 추가로 학습시켜 해당 도메인에 특화된 맞춤형 모델로 만드는 방식이다.\n",
    "- **장점**\n",
    "  - 특정 도메인에 최적화되어 높은 정확도와 성능을 낼 수 있다.\n",
    "- **단점**\n",
    "  - 모델 재학습에 많은 시간과 자원이 필요하다.\n",
    "  - 새로운 정보가 반영되지 않으며, 이를 위해서는 다시 학습해야 한다.\n",
    "\n",
    "### RAG\n",
    "\n",
    "- **정의**: 모델을 다시 학습시키지 않고, 외부 지식 기반에서 정보를 검색하여 실시간으로 답변에 활용하는 방식이다.\n",
    "- **장점**\n",
    "  - 최신 정보를 쉽게 반영할 수 있다.\n",
    "  - 모델을 수정하지 않아도 되므로 효율적이다.\n",
    "- **단점**\n",
    "  - 검색된 문서의 품질에 따라 답변의 정확성이 달라질 수 있다.\n",
    "  - 검색 시스템 구축이 필요하다.\n",
    "\n",
    "## 정리\n",
    "\n",
    "| 항목       | 파인튜닝 | RAG |\n",
    "| -------- | ---- | --- |\n",
    "| 도메인 최적화  | 가능   | 제한적 |\n",
    "| 최신 정보 반영 | 불가능  | 가능  |\n",
    "| 구현 난이도   | 높음   | 보통  |\n",
    "| 유연성      | 낮음   | 높음  |\n",
    "\n",
    "- LLM은 학습 당시의 데이터만을 기반으로 작동하므로 최신 정보나 기업 내부 자료와 같은 특정한 지식 기반에 접근할 수 없다.\n",
    "- 파인튜닝은 시간과 비용이 많이 들고 유지보수가 어렵다.\n",
    "-\t반면, RAG는 기존 LLM을 변경하지 않고도 외부 문서를 통해 그 한계를 보완할 수 있다.\n",
    "- RAG는 특히 빠르게 변화하는 정보를 다루는 분야(예: 기술 지원, 뉴스, 법률 등)에서 유용하게 활용된다. 반면, 정적인 정보에 대해 높은 정확도가 필요한 경우에는 파인튜닝이 효과적이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017ca94-32e0-4460-8937-90a8d92ca07b",
   "metadata": {},
   "source": [
    "## RAG 작동 단계\n",
    "- 크게 \"**정보 저장(인덱싱)**\", \"**검색**, **생성**\"의 단계로 나눌 수 있다.\n",
    "  \n",
    "### 1. 정보 저장(인덱싱)\n",
    "RAG는 사전에 정보를 가공하여 **벡터 데이터베이스**(Vector 저장소)에 저장해 두고, 나중에 검색할 수 있도록 준비한다. 이 단계는 다음과 같은 과정으로 이루어진다.\n",
    "\n",
    "1. **Load (불러오기)**\n",
    "   - 답변시 참조할 사전 정보를 가진 데이터들을 불러온다.\n",
    "2. **Split/Chunking (문서 분할)**\n",
    "   - 긴 텍스트를 일정한 길이의 작은 덩어리(*chunk*)로 나눈다.\n",
    "   - 이렇게 해야 검색과 생성의 정확도를 높일 수 있다.\n",
    "3. **Embedding (임베딩)**\n",
    "   - 각 텍스트 조각을 **임베딩 벡터**로 변환한다.\n",
    "   - 임베딩 벡터는 그 문서의 의미를 벡터화 한 것으로 질문과 유사한 문서를 찾을 때 인덱스로 사용된다.\n",
    "4. **Store (저장)**\n",
    "   - 임베딩된 벡터를 **벡터 데이터베이스**(벡터 저장소)에 저장한다.\n",
    "   - 벡터 데이터베이스는 유사한 질문이나 문장을 빠르게 찾을 수 있도록 특화된 데이터 저장소이다.\n",
    "   \n",
    "![rag](figures/rag1.png)\n",
    "\n",
    "### 2. 검색, 생성\n",
    "\n",
    "사용자가 질문을 하면 다음과 같은 절차로 답변이 생성된다.\n",
    "1. **Retrieve (검색)**\n",
    "   - 사용자의 질문을 임베딩한 후, 이 질문 벡터와 유사한 context 벡터를 벡터 데이터베이스에서 검색하여 찾는다.\n",
    "2. **Query (질의 생성)**\n",
    "   - 벡터 데이터베이스에서 검색된 문서 조각과 사용자의 질문을 함께 **프롬프트**(prompt)로 구성하여 LLM에 전달한다.\n",
    "3. **Generation (응답 생성)**\n",
    "   - LLM은 받은 프롬프트에 대한 응답을 생성한다.\n",
    "   \n",
    "- **RAG 흐름**\n",
    "  \n",
    "![Retrieve and Generation](figures/rag2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e3d03-2250-4c79-aa83-7faf709ba4cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21f6fd91-e3de-4f9d-9c8a-9c21de7a768c",
   "metadata": {},
   "source": [
    "# Document Loader\n",
    "- LLM에게 질의할 때 같이 제공할 Data들을 저장하기 위해 먼저 읽어들인다.(Load)\n",
    "- 데이터 Resouce는 다양하다.\n",
    "    - 데이터를 로드(load)하는 방식은 저장된 위치와 형식에 따라 다양하다. \n",
    "      - 로컬 컴퓨터(Local Computer)에 저장된 문서\n",
    "        - 예: CSV, Excel, JSON, TXT 파일 등\n",
    "      - 데이터베이스(Database)에 저장된 데이터셋\n",
    "      - 인터넷에 존재하는 데이터\n",
    "        - 예: 웹에 공개된 API, 웹 페이지에 있는 데이터, 클라우드 스토리지에 저장된 파일 등\n",
    "\n",
    "![rag_load](figures/rag_load.png)\n",
    "\n",
    "- 다양한 문서 형식(format)에 맞춰 읽어오는 다양한 **document loader** 들을 Langchain에서 지원한다.\n",
    "    - 다양한 Resource들로 부터 데이터를 읽기 위해서는 다양한 라이브러리를 이용해 서로 다른 방법으로 읽어야 한다.\n",
    "    - Langchain은 데이터를 읽는 다양한 방식의 코드를 하나의 interface로 사용 할 수 있도록 지원한다.\n",
    "        - https://python.langchain.com/docs/how_to/#document-loaders\n",
    "    - 다양한 3rd party library(ppt, github 등등 다양한 3rd party lib도 있음. )들과 연동해 다양한 Resource로 부터 데이터를 Loading 할 수 있다.\n",
    "        - https://python.langchain.com/docs/integrations/document_loaders/\n",
    "- **모든 document loader는 기본적으로 동일한 interface(사용법)로 호출할 수있다.**\n",
    "- **반환타입**\n",
    "    - **list[Document]**\n",
    "    - Load 한 문서는 Document객체에 정보들을 넣는다. 여러 문서를 읽을 수 있기 대문에 list에 묶어서 반환한다.\n",
    "        - **Document 속성**\n",
    "            - page_content: 문서의 내용\n",
    "            - metadata(option): 문서에 대한 메타데이터(정보)를 dict 형태로 저장한다. \n",
    "            - id(option): 문서의 고유 id\n",
    "     \n",
    "- **주의**\n",
    "    - Langchain을 이용해 RAG를 구현할 때 **꼭 Langchain의 DocumentLoader를 사용해야 하는 것은 아니다.**\n",
    "    - DocumentLoader는 데이터를 읽어오는 것을 도와주는 라이브러리일 뿐이다. 다른 라이브러리를 이용해서 읽어 들여도 상관없다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f25589-24a2-4f1f-9e8c-41e0594b6ce1",
   "metadata": {},
   "source": [
    "## 주요 Document Loader\n",
    "\n",
    "### Text file\n",
    "- TextLoader 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40998e95-a607-45d5-a3f6-2bb598b1aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "path = \"data/olympic.txt\"\n",
    "\n",
    "#객체생성 - 읽어들일 자원(파일)의 위치\n",
    "loader = TextLoader(path, encoding='utf-8')\n",
    "\n",
    "#Load - 읽어오기\n",
    "docs = loader.load() #메소드 호출 시 읽음. \n",
    "#loader.lazy_load() : 읽은 문서를 사용(조회) 하는 시점에 읽는다.\n",
    "\n",
    "print(type(docs), len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24a3ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/olympic.txt'}, page_content='올림픽\\n올림픽(영어: Olympic Games, 프랑스어: Jeux olympiques)은 전 세계 각 대륙 각국에서 모인 수천 명의 선수가 참가해 여름과 겨울에 스포츠 경기를 하는 국제적인 대회이다. 전 세계에서 가장 큰 지구촌 최대의 스포츠 축제인 올림픽은 세계에서 가장 인지도있는 국제 행사이다. 올림픽은 2년마다 하계 올림픽과 동계 올림픽이 번갈아 열리며, 국제 올림픽 위원회(IOC)가 감독하고 있다. 또한 오늘날의 올림픽은 기원전 8세기부터 서기 5세기에 이르기까지 고대 그리스 올림피아에서 열렸던 올림피아 제전에서 비롯되었다. 그리고 19세기 말에 피에르 드 쿠베르탱 남작이 고대 올림피아 제전에서 영감을 얻어, 근대 올림픽을 부활시켰다. 이를 위해 쿠베르탱 남작은 1894년에 IOC를 창설했으며, 2년 뒤인 1896년에 그리스 아테네에서 제 1회 올림픽이 열렸다. 이때부터 IOC는 올림픽 운동의 감독 기구가 되었으며, 조직과 활동은 올림픽 헌장을 따른다. 오늘날 전 세계 대부분의 국가에서 올림픽 메달은 매우 큰 영예이며, 특히 올림픽 금메달리스트는 국가 영웅급의 대우를 받으며 스포츠 스타가 된다. 국가별로 올림픽 메달리스트들에게 지급하는 포상금도 크다. 대부분의 인기있는 종목들이나 일상에서 쉽게 접하고 즐길 수 있는 생활스포츠 종목들이 올림픽이라는 한 대회에서 동시에 열리고, 전 세계 대부분의 국가 출신의 선수들이 참여하는 만큼 전 세계 스포츠 팬들이 가장 많이 시청하는 이벤트이다. 2008 베이징 올림픽의 모든 종목 누적 시청자 수만 47억 명에 달하며, 이는 인류 역사상 가장 많은 수의 인구가 시청한 이벤트였다.\\n또한 20세기에 올림픽 운동이 발전함에 따라, IOC는 변화하는 세계의 사회 환경에 적응해야 했다. 이러한 변화의 예로는 얼음과 눈을 이용한 경기 종목을 다루는 동계 올림픽, 장애인이 참여하는 패럴림픽, 스페셜 올림픽, 데플림픽, 10대 선수들이 참여하는 유스 올림픽 등을 들 수 있다. 그 뿐만 아니라 IOC는 20세기의 변화하는 경제, 정치, 기술 환경에도 적응해야 했다. 그리하여 올림픽은 피에르 드 쿠베르탱이 기대했던 순수한 아마추어 정신에서 벗어나서, 프로 선수도 참가할 수 있게 되었다. 올림픽은 점차 대중 매체의 중요성이 커짐에 따라 올림픽의 상업화와 기업 후원을 놓고도 논란이 생겨났다. 또한 올림픽을 치르며 발생한 보이콧, 도핑, 심판 매수, 테러와 같은 수많은 일들은 올림픽이 더욱 굳건히 성장할 수 있는 원동력이 되었다.\\n올림픽은 국제경기연맹(IF), 국가 올림픽 위원회(NOC), 각 올림픽의 위원회(예-벤쿠버동계올림픽조직위원회)로 구성된다. 의사 결정 기구인 IOC는 올림픽 개최 도시를 선정하며, 각 올림픽 대회마다 열리는 올림픽 종목도 IOC에서 결정한다. 올림픽 경기 개최 도시는 경기 축하 의식이 올림픽 헌장에 부합하도록 조직하고 기금을 마련해야 한다. 올림픽 축하 행사로는 여러 의식과 상징을 들 수 있는데 올림픽기나 성화가 그 예이다.\\n올림픽은 거의 모든 국가가 참여할 정도로 규모가 커졌다. 하계 올림픽은 33개의 종목과 약 400개의 세부종목에서 13,000명이 넘는 선수들이 겨루고 그중 각 종목별 1, 2, 3위는 각각 금/은/동을 수여받는다. 전 세계 언론에서 각각 4년마다 열리는 올림픽 경기를 중계하기 때문에 이름 없는 선수가 개인적, 국가적, 세계적으로 명성을 얻을 수 있는 기회가 된다. 이와 더불어 올림픽 경기는 개최지와 개최국에게도 전 세계에 그 이름을 널리 알리는 좋은 기회가 된다.\\n\\n고대올림픽\\n고대의 올림픽 경기(올림피아 경기)는 고대 그리스의 여러 도시 국가의 대표선수들이 모여 벌인 일련의 시합이었으며, 육상 경기가 주 종목이지만 격투기와 전차 경기도 열렸다. 그리고 패배하면 죽기도 하였다. 고대 올림픽의 유래는 수수께끼로 남아있다. 잘 알려진 신화로는 헤라클레스와 그의 아버지인 제우스가 올림픽의 창시자였다는 것이다. 전설에 따르면 이 경기를 최초로 \\'올림픽\\'이라고 부르고, 4년마다 대회를 개최하는 관례를 만든 사람이 헤라클레스라고 한다. 어떤 전설에서는 헤라클레스가 이른바 헤라클레스의 12업을 달성한 뒤에 제우스를 기리고자 올림픽 경기장을 지었다고 한다. 경기장이 완성되자 헤라클레스는 일직선으로 200 걸음을 걸었으며, 이 거리를 \"스타디온\"이라 불렀는데, 후에 이것이 길이 단위인 \\'스타디온\\'(그리스어: στάδιον → 라틴어: 영어: stadium)이 되었다. 또 다른 설로는 \\'올림픽 휴전\\'(그리스어: ἐκεχειρία 에케케이리아[*])이라는 고대 그리스의 관념이 최초의 올림피아 경기와 관련이 있다고 한다. \\'올림픽 휴전\\'이란 어느 도시 국가라도 올림피아 경기 기간 중에 다른 나라를 침범하면 그에 대한 응징을 받을 수 있다는 뜻으로, \"올림픽 기간에는 전쟁하지 말 것\"으로 요약할 수 있다.\\n고대 올림피아 경기가 처음 열린 시점은 보통 기원전 776년으로 인정되고 있는데, 이 연대는 그리스 올림피아에서 발견된 비문에 근거를 둔 것이다. 이 비문의 내용은 달리기 경주 승자 목록이며 기원전 776년부터 4년 이후 올림피아 경기 마다의 기록이 남겨져 있다. 고대 올림픽의 종목으로는 육상, 5종 경기(원반던지기, 창던지기, 달리기, 레슬링, 멀리뛰기), 복싱, 레슬링, 승마 경기가 있었다. 전설에 따르면 엘리스의 코로이보스가 최초로 올림피아 경기에서 우승한 사람이라고 한다.\\n고대 올림피아 경기는 근본적으로 종교적인 중요성을 띄고 있었는데, 스포츠 경기를 할 때는 제우스(올림피아의 제우스 신전에는 페이디아스가 만든 제우스 상이 있음)와 펠롭스를 기리기 위하여 제물 봉헌 의식을 치렀다. 펠롭스는 올림피아의 전설상의 임금이었던 피사티스의 오이노마오스 왕과 전차 경주를 겨룬 영웅으로 유명한 인물이다. 올림피아 경기의 승자는 시와 조각상으로 칭송받았다. 올림피아 경기는 4년마다 열렸으며, 이 기간을 \\'올림피아드\\'(Olympiad)라고 했는데, 그리스인들은 이를 시간 단위로 이용하였다. 올림피아 경기는 고대 그리스에서 정기적으로 열렸던 범그리스 대회의 순환 대회 가운데 하나였다.\\n올림피아 경기는 기원전 6세기~기원전 5세기에 절정에 이르렀으나, 그 후 로마가 패권을 잡은 뒤 그리스에 영향력을 행사하면서 서서히 쇠퇴하게 된다. 고대 올림픽이 공식적으로 끝난 해는 확실히 알 수 없으나, 대부분 테오도시우스 1세 황제가 모든 이단 숭배 및 예배를 금지했던 393년을 고대 올림픽의 마지막이라고 추정한다. 다른 설에 따르면 테오도시우스의 후계자인 테오도시우스 2세가 모든 그리스 신전을 파괴하라고 명령한 426년이라고도 한다. 이렇게 올림픽이 사라진 이후로 이보다 한참 뒤인 19세기에 이르러서야 비로소 다시 올림픽 경기가 열리게 된다.\\n\\n\\n근대올림픽\\n고대 올림피아 경기를 제대로 구현한 최초의 시도는 혁명 시대의 프랑스에서 1796년부터 1798년까지 3년동안 실시했던 프랑스 국내 올림픽인 \\'공화국 올림픽\\'(L\\'Olympiade de la République)이었다. 이 대회의 종목 중에는 고대 그리스 올림피아 경기 때 행한 일부 종목도 있었다. 특히 1798년 공화국 올림픽 대회는 미터법을 최초로 스포츠에 도입시킨 대회이기도 하다. 이후 52년뒤인 1850년에는 잉글랜드 슈롭셔주의 웬록에서 올림픽급의 대회가 열리기 시작하였다. 이 대회는 1859년에 아테네에서 열렸을 때 웬록 올림픽으로 명칭이 변경되었으며 지금도 열리고 있다. 브룩스 박사는 1859년에 아테네에서 열린 올림픽 경기의 내용을 이후 경기에 채택하였다. 1866년 런던의 수정궁에서는 윌리엄 페니 브룩스가 영국의 국가 올림픽 대회를 만들었다.\\n1821년 그리스에서는 오스만 제국의 지배에 반기를 들고 독립 전쟁이 일어나면서, 이때부터 올림픽 부활에 대한 관심이 생겨났다. 시인이자 신문 편집자였던 파나요티스 수초스(Παναγιώτης Σούτσος)는 1833년에 출간한 자신의 시 \\'망자(亡者)의 대화\\'에서 최초로 올림픽 부활에 대한 제안을 내놓았다. 그리스의 부유한 박애주의자였던 에방겔리스 자파스(Ευαγγέλης Ζάππας)는 1859년에 아테네 시 광장에서 열린 \"올림픽 경기(일명 자파스 올림픽)\"를 후원하였다. 이 경기에는 그리스와 오스만 제국 출신의 선수들이 참가하였다. 에방겔리스 자파스는 이후에도 올림픽 경기를 개최할 수 있도록 고대의 경기장이었던 파나티네코 경기장을 복원하는 데도 돈을 썼다. 파나티네코 경기장에서 1870년과 1875년에 자파스 올림픽을 개최했으며, 현대 올림픽인 2004년 하계 올림픽 때는 양궁 경기장으로도 쓰였다.\\n역사학자였던 쿠베르탱은 프로이센-프랑스 전쟁(1870–1871)에서 프랑스의 패배 원인을 분석하면서 군사들이 체계적인 체력 훈련을 받지 않았기 때문에 전쟁에서 패배했다고 말한 인물이다. 1890년 웬록 올림픽에 참석한 쿠베르탱은 그 이후부터 올림픽을 대규모로 부활시킬 수 있으리라 생각했다. 쿠베르탱은 웬록 올림픽과 자파스 올림픽을 토대로 하여 올림픽 경기를 국제적으로 시행하기 위해 나라별로 올림픽을 번갈아가며 개최하는 방식을 생각해냈다. 그는 이 방안을 새로 설립된 국제 올림픽 위원회(IOC)의 첫 올림픽 의회 기간 중에 언급했다. 총회는 파리의 소르본 대학교에서 1894년 6월 16일부터 6월 23일까지 7일간 지속되었으며, 총회 마지막날, 2년 후인 1896년에 아테네에서 국제적 규모의 올림픽 대회를 열기로 결정되었다. IOC는 올림픽을 조직하는 데에 모든 책임을 졌으며, 초대 위원장으로는 그리스의 작가였던 디미트리오스 비켈라스(Δημήτριος Βικέλας)가 선출되었다.\\n\\n하계올림픽\\n1859년 자파스 올림픽에 참가한 선수의 수는 250명을 넘지 못했다. 에방겔리스 자파스는 \"지난 자파스 올림픽을 포함, 1896년에 개최될 2번째 올림픽을 위해 파나티네코 경기장을 보수해야 한다.\"라는 충고를 하지만, 그리스 정부는 그의 말을 듣지 않았고 결국 1896년 아테네 올림픽 준비를 위해 파나티네코 경기장은 두 번이나 정비해야 했다. 1회 대회 정식종목으로는 9종목이 있었는데 육상, 사이클, 펜싱, 체조, 사격, 수영, 테니스, 역도, 레슬링이 있었으며, 조정도 정식종목이었으나 매우 나쁜 날씨로 인해 조정 경기는 취소되었다. 펜싱 경기는 역사적 건물인 자피온(에반젤리스 자파스의 이름을 딴 것이다)에서 열렸다. 그리스의 관리들과 국민들은 올림픽 경기 개최에 열광적이었다. 많은 선수들이 이에 동감하면서 앞으로도 올림픽 대회를 아테네에서 영구히 개최해야 한다고 요구하기까지 하였다. 그러나 국제올림픽위원회(IOC)는 근대 올림픽은 순환 개최로 열리는 세계적인 행사가 되어야 한다고 생각했다. 결국 2회 올림픽은 프랑스 파리에서 열기로 결정되었다.\\n1896년 올림픽 대회의 성공을 이어서 개최된 두 번째 올림픽인 1900년 올림픽에서는 올림픽의 존폐여부를 위협받는 지경에 이르게 되었다. 1900년에 파리와 1904년에 세인트루이스에서 열린 올림픽은 하필이면 엑스포와 시간과 장소가 겹치는 바람에 빛을 바래게 된다. 1904년 대회를 예로 들면 650명의 선수단이 참가했지만 그중 580명은 미국국적을 가진 사람이었다. 1900년과 1904년의 두 올림픽 대회는 역대 올림픽중에 최저점을 기록한다. 올림픽은 1906년 올림픽이 아테네에서 개최되었을 때 다시 일어서게 된다. 또 다른 성공적인 올림픽은 그리스 올림픽 협회가 조직했으며 세 차례나 올림픽을 치른 경기장에서 개최되었다. 이 경기는 비공식 올림픽이긴 했지만 세계적으로 상당한 참가자들을 불러 모았으며 대중들에게 큰 재미를 갖다주었다. 이 때를 시작으로 올림픽의 인기와 번영이 시작되었다.\\n\\n동계올림픽\\n동계 올림픽은 눈과 얼음을 이용하는 스포츠들을 모아 이루어졌으며 하계 올림픽 때 실행하기 불가능한 종목들로 구성되어 있다. 피겨스케이팅, 아이스하키는 각각 1908년과 1920년에 하계올림픽 종목으로 들어가 있었다. IOC는 다른 동계 스포츠로 구성된 새로운 대회를 만들고 싶어 했고, 로잔에서 열린 1921년 올림픽 의회에서 겨울판 올림픽을 열기로 합의했다. 1회 동계올림픽은 1924년, 프랑스의 샤모니에서 11일간 진행되었고, 16개 종목의 경기가 치러졌다. IOC는 동계 올림픽이 4년 주기로 하계 올림픽과 같은 년도에 열리도록 했다. 이 전통은 프랑스의 알베르빌에서 열린 1992년 올림픽 때까지 지속되었으나, 노르웨이의 릴레함메르에서 열린 1994년 올림픽부터 동계 올림픽은 하계 올림픽이 끝난지 2년후에 개최하였다.\\n\\n패럴림픽\\n패럴림픽(Paralympic)은 신체·감각 장애가 있는운동 선수가 참가하는 국제 스포츠 대회로, 장애인 올림픽으로 불린다. 1948년에 루드비히 구트만 경(Sir Ludwig Guttman)은 제2차 세계대전에 참전한 군인들의 사회 복귀를 위한 일환으로 1948년 런던 올림픽과 동시에 몇몇 병원들을 연합해서 여러 경기를 펼쳤다. 구트만의 세계 휠체어, 신체부자유자대회(World Wheelchair and Amputee Games)로 알려진 이 대회는 매년 열리는 스포츠대회가 되었다. 12년이 넘도록 구트만과 다른 사람들은 스포츠를 상처를 치료하는 방법 중 하나로써 계속 대회 개최에 노력을 기울였다. 로마에서 열린 1960년 하계 올림픽때 구트만은 400명의 선수들을 \"Parallel Olympics\"에 참가시켰으며 이것이 곧 1회 패럴림픽으로 알려지게 되었다. 그 때부터 패럴림픽은 하계 올림픽이 열린 년도에 열리게 되었다. 서울에서 열린 1988년 하계 올림픽부터는 하계 올림픽을 개최한 도시는 패럴림픽도 같이 개최하기로 한다.\\n\\n오늘날의 올림픽\\n1896년 대회때는 14개국에서 241명의 선수단이 참가했지만 2008년 하계 올림픽때는 204개국에서 10,500명의 선수가 참가하는 등 세계적인 대회로 변모했다. 동계 올림픽의 규모는 하계 올림픽 규모보다 작다. 예를 들면 2006 토리노 동계 대회때는 80개국에서 2,508명의 선수가 참가했으며 82개 세부종목이 있었고, 2008 베이징 하계 대회때는 204개국, 11,508명의 선수, 302개의 세부종목이 있었다. 올림픽이 진행되는 동안 선수와 임직원들은 올림픽 선수촌에서 지낸다. 올림픽 선수촌에는 선수들을 위한 개인실이 있으며 카페테리아, 헬스 클리닉, 종교적인 시설 등 최상의 편의를 위한 시설들이 있다.\\n올림픽에 참가하는 나라는 UN에 등록된 국가의 수 193개보다 많다. 다른 국제조직이 개최하는 대회들은 정치적 주권국으로 참가를 제한하는 반면, IOC는 그에 상관없이 올림픽에 모든 공동체들이 참가할 수 있도록 한다. 이는 연합체나 공동체에서 국가올림픽위원회(NOC)를 만드는 것을 허용한다는 의미이다. 예를 들면 푸에르토리코, 버뮤다, 홍콩과 같은 곳도 올림픽에서 다른 나라와 스포츠 경쟁을 합법적으로 할 수 있다.\\n\\n국제 올림픽 위원회\\n올림픽 활동이란 많은 수의 국가, 국제 경기 연맹과 협회 • 미디어 파트너를 맺기 • 선수, 직원, 심판, 모든 사람과 기관이 올림픽 헌장을 지키는 것을 말한다. 국제올림픽위원회(IOC)는 모든 올림픽 활동을 통솔하는 단체로서, 올림픽 개최 도시 선정, 계획 감독, 종목 변경, 스폰서 및 방송권 계약 체결 등의 권리가 있다. 올림픽 활동은 크게 세 가지로 구성된다.\\n- 국제경기연맹(IF)은 국제적인 규모의 경기를 관리, 감독하는 기구이다. 예를 들어서 국제 축구 연맹(FIFA)는 축구를 주관하며, 국제 배구 연맹(FIVB)은 배구를 주관하는 기구이다. 올림픽에는 현재 35개의 국제경기연맹이 있고 각 종목을 대표한다. (이 중에는 올림픽 종목은 아니지만 IOC의 승인을 받은 연맹도 있다.)\\n- 국가 올림픽 위원회(NOC)는 각국의 올림픽 활동을 감독하는 기구이다. 예를 들어서 대한 올림픽 위원회(KOC)는 대한민국의 국가 올림픽 위원회이다. 현재 IOC에 소속된 국가 올림픽 위원회는 205개이다.\\n- 올림픽 조직 위원회(OCOG)는 임시적인 조직으로 올림픽의 총체적인 것(개막식, 페막식 등)을 책임지기 위해 구성된 조직이다. 올림픽 조직 위원회는 올림픽이 끝나면 해산되며 최종보고서를 IOC에 제출한다.\\n올림픽의 공식언어는 프랑스어와 영어와 개최국의 공용어이다. 모든 선언(예를 들어서 개막식 때 각국 소개를 할 때)들은 세 언어가 모두 나오거나 영어나 프랑스어 중에서 한 언어로만 말하기도 한다. 개최국의 공용어가 영어나 프랑스어가 아닐 때는 당연히 그 나라의 공용어도 함께 나온다.\\n\\n국제 올림픽 위원회(이하 IOC로 지칭)는 몇몇 위원들이 한 행위에 대해서 비판을 받고 있다. 그 예로 IOC 위원장이었던 에이버리 브런디지와 후안 안토니오 사마란치가 대표적인 사람이다. 브런디지는 20년 넘게 IOC 위원장직을 맡았고 임기 중에 올림픽을 정치적으로 휘말려들지 않게 하기 위해 보호했다. 그러나 그는 남아프리카 공화국 대표단에게 아파르트헤이트와 관련된 이슈를 건드리고 반유대정책을 함으로써 비난을 받았다. 사마란치 위원장 시기 때는 족벌 정치와 부패로 비난받았다. 사마란치가 스페인에서 프랑코 정권에 협력했다는 것도 비판의 이유가 되었다.\\n1998년에 몇몇 IOC위원들이 2002년 솔트레이크 시티 동계 올림픽 유치 과정에서 미국에게 미국을 올림픽 개최지로 뽑아달라는 뇌물청탁을 받았다는 것이 폭로되었다. 이에 IOC는 사퇴한 IOC위원 4명과 강제 퇴출된 6명에 대한 조사를 했다. 이 스캔들은 이후에 개최지 선정에서 이와 같은 불미스러운 일이 일어나지 않게 하기 위해서 IOC가 개혁에 착수하도록 하는 긍정적인 역할을 하기도 했다.\\nBBC 다큐멘터리인 \\'파노라마\\'에서는 \\'매수된 올림픽\\'이란 주제로 2004년 8월에 방송을 내보내기도 했다. 이때 이 프로그램에서는 2012년 하계 올림픽의 개최지 선정과 관련된 뇌물에 대해서 조사했다. 이 다큐멘터리에서는 특정 후보 도시가 IOC 위원들에게 뇌물수수하는 것이 가능했다고 주장했으며, 특히 파리 시장이었던 베르트랑 들라노에(Bertrand Delanoë)는 영국의 총리인 토니 블레어와 런던올림픽유치위원회가 입후보 규정을 위반했다고 비난했다. 그는 당시 프랑스 대통령이었던 자크 시라크를 목격자로 내세웠지만 시라크 대통령은 이 분쟁에 휘말려드는 것을 주의했으며 인터뷰를 삼갔다. 결국 베르트랑 들라노에의 주장에 대한 조사는 체계적으로 이루어지지는 않았다. 2006년 동계 올림픽을 유치했던 토리노도 이 논쟁에서 빠져나갈 수 없었다. 이번에는 스위스 국적의 IOC위원 마크 호들러(Marc Hodler)가 이 논쟁의 중심이 되었는데, 이 위원은 스위스 시온의 경쟁 도시였던 토리노가 IOC위원들에게 뇌물수수를 했다고 말했고, 이 발언으로 광범위한 조사가 이루어졌다. 이 언행이 많은 IOC위원들이 시온에 대해 언짢게 생각하게 되고 토리노가 개최지로 선정되도록 도와주는 역할을 했을 가능성도 제기되고 있다.\\n\\n올림픽 경기 종목\\n올림픽 경기 종목은 총 33개부문 52개 종목에서 약 400개의 경기로 이루어져있다. 예를 들어서 하계 올림픽 부문인 레슬링은 자유형과 그레코로만형의 두 종목으로 나뉜다. 여기에서 10경기는 남자부, 4경기는 여자부로 열리며 분류기준은 체중이다. 하계 올림픽은 26개, 동계 올림픽은 7개 부문으로 이루어져있다. 하계 올림픽에서는 육상, 수영, 펜싱, 체조가 1회 대회때부터 한번도 빠짐없이 정식종목이었으며, 동계 올림픽에서는 크로스컨트리, 피겨 스케이팅, 아이스 하키, 노르딕 복합, 스키 점프, 스피드 스케이팅이 1924년 동계 올림픽부터 빠짐없이 정식종목이었다. 배드민턴, 농구, 배구와 같은 정식종목들은 처음에는 시범종목이었으며 그 후에 정식종목으로 승인 되었다. 야구처럼 예전에는 정식종목 이었지만 지금은 정식 종목에서 빠진 종목도 있다.\\n\\n각 올림픽 종목들은 IOC로부터 승인을 받은 국제경기연맹의 관리를 받는다. 35개의 연맹이 IOC에서 승인을 받았으며, 승인을 받았지만 현재 정식종목이 아닌 종목을 감독하는 연맹도 있다. IOC의 승인을 받았지만 올림픽 종목이 아닌 스포츠들은 올림픽 종목으로 고려되지는 않으나, 올림픽이 끝난 후 처음으로 열리는 IOC총회 때마다 정식종목이 되도록 신청을 할 수는 있다. IOC 총회 때 정식종목 선정은 총회에 참석중인 IOC위원들의 투표를 통해 이루어지며, 재적 위원 수의 과반수 이상 찬성표를 얻어야 정식종목으로 인정을 받는다. IOC의 승인을 받은 스포츠이나 찬성표를 받지 못해 정식종목이 되지 못한 스포츠로는 체스와 서핑과 같은 것이 있다.\\n\\n2004년 10월과 11월에 IOC는 \\'올림픽 프로그램 위원회\\'(Olympic Programme Commission)를 설립했다. 여기서는 올림픽 종목과 올림픽 종목이 아닌 스포츠를 모두 재검토하는 일을 한다. 이 위원회의 목표는 올림픽 종목에 더 체계적으로 다가가는 것이다. 위원회에서는 우선적으로 올림픽 종목으로 포함되기 위해서는 7개의 기준을 충족시켜야 한다고 말한다. 이 7개의 기준은 역사, 전통, 보편성, 인기도와 잠재성, 선수의 건강, 연맹의 스포츠를 관리할만한 능력, 스포츠를 여는 데에 필요한 비용이다. 예를 들면 2012년 하계 올림픽의 정식종목 후보에 7개 조건을 포함한 비(非)올림픽 스포츠가 올랐고 그 내용은, 골프, 가라테, 럭비, 인라인 스케이팅, 스쿼시였다. 이 스포츠들은 IOC 상임이사회에서 재검토되어 2005년 7월에 열린 싱가포르 총회에서 최종 결정하기로 했다. 결국 5개 중 2개(가라테와 스쿼시) 가 최종 후보로 올라왔으나 가라테와 스쿼시 둘 다 2/3의 미만의 찬성표로 정식종목이 되지는 못한다. 그 후 2016년 올림픽 정식종목에는 7개의 스포츠가 정식종목 신청을 했는데, 내용은 가라테, 골프, 스쿼시, 야구, 소프트볼, 7인제 럭비, 인라인 스케이팅이었다. 2009년 8월 13일, 신청된 7개의 스포츠 중 단 2개만 최종후보로 선정되었는데, 이는 7인제 럭비와 골프였다. 같은해인 2009년 10월에 열린 IOC 총회에서 골프와 럭비는 과반수 이상의 득표를 얻어서 2016년 하계 올림픽과 2020년 하계 올림픽의 정식종목으로 채택되었다.\\n2002년에 열린 제114차 IOC 총회에서는 하계 올림픽 종목은 최대 28부문 301개 경기에 10,500명이 참가하는 것으로 제한하기로 결정했다.그 후 3년 뒤인 제117차 IOC 총회에서는 정식종목이었던 야구와 소프트볼을 정식 종목에서 제외시킨다. 이 결과에 대한 이견이 없었으므로 2012년 올림픽 때는 26개부문에서 경기가 열린다. 2016년과 2020년 올림픽 때는 럭비와 골프가 추가되어 다시 28개부문에서 경기가 열린다.\\n프로 NHL선수들은 1998년부터 아이스 하키종목에 출전할 수 있게 되었다. (나가노 올림픽 결승전 러시아 vs 체코).\\n영국 명문 공립 학교의 이념은 쿠베르탱에게 큰 영향을 끼쳤다. 영국 공립 학교는 스포츠를 교육의 중요한 부분이라 생각해서 \\'건전한 신체에 건전한 정신을\\'이라는 의미를 가진 라틴어 mens sana in corpore sano를 표어로 삼았다. 이 이념에 의하면 신사들은 특정한 분야에서만 우수해서는 안되고 모든 분야에서 고르게 잘해야 하고, 공정한 결과에는 승복해야 하며, 연습이나 훈련은 속이는 것과 마찬가지로 여겼다. 전문적으로 스포츠를 연습한 사람은 취미로 연습한 사람에 비해 공평하지 않다고 생각한 것이다.\\n\\n현대 올림픽에서는 프로 선수의 참가 불허가 많은 분쟁을 가져왔다. 1912년 하계 올림픽의 근대 5종 경기와 10종 경기에서 우승한 짐 소프는 올림픽에 나가기 전에 준프로야구선수로 활동했다는 게 나중에 밝혀져 메달이 박탈되었다. 소프는 후에 동정적 여론의 힘을 업고 1983년에 메달을 돌려받게 된다. 1936년 동계 올림픽 때 스위스와 오스트리아 스키선수들은 돈을 벌기 위해 스포츠를 했는데 이러한 행동이 아마추어 정신에 위배된다고 결정되어 그들은 스키종목에 참가할 수 없었다.\\n20세기에 이르러서 계급구조가 붕괴되면서 이른바 귀족적인 신사라는 아마추어 선수에 대한 정의는 시대에 뒤처지는 말이 되게 된다. 일부 국가들은 \\'정식 아마추어 선수\\'를 \\'키워서\\' 순수한 아마추어 정신을 벗어나고 있었고, 자신이 내는 비용으로 연습하는 선수들의 불리함에 대한 목소리가 나오기 시작했다. 하지만 IOC는 아마추어 정신에 관한 입장을 고수했다. 1970년대 초에는 아마추어 정신이 올림픽헌장에서 폐지되어야 한다는 말이 나오기 시작했다. 결국 프로선수들의 출전은 국제경기연맹(IF)에서 결정짓도록 되었다. 2008년 기준으로 아마추어 선수만 출전하고 있는 올림픽 종목은 복싱이 유일하며 남자 축구에서는 나이가 23세 이상인 선수를 3명까지만 선발할 수 있다. 이는 아마추어 정신을 지키기 위한 일환으로 볼 수 있다.\\n\\n논란\\n올림픽에서 첫 번째 보이콧은 1956년 하계 올림픽에서 시작되었다. 네덜란드, 스페인, 스위스는 소련의 헝가리 침공에 항의해 참가를 거부했다. 캄보디아, 이집트, 이라크, 레바논은 제2차 중동 전쟁 때문에 보이콧했다. 1972년과 1976년 올림픽에는 많은 아프리카의 국가들이 남아프리카 공화국과 로디지아에서 일어나는 인종 차별정권에 대한 항의의 표시로 올림픽 참가를 거부했다. 이 보이콧에는 뉴질랜드도 관계가 되어있는데, 뉴질랜드 럭비 국가 대표팀이 당시 아파르트헤이트정책을 쓰던 남아프리카 공화국과 경기를 했음에도 불구하고 뉴질랜드의 올림픽 참가가 허용되었기 때문이었다. 국제 올림픽 위원회는 이 두 보이콧에 대해 심각하게 고민했으나 후자의 뉴질랜드의 경우는 럭비가 올림픽 종목이 아니라는 이유를 내세워 뉴질랜드의 올림픽 참가 금지 요청을 거부했다. 당시 아프리카에 속해 있던 20개국과 가이아나, 이라크는 경기를 끝낸 선수들이 있었지만 탄자니아가 이끄는 올림픽 보이콧에 가세했다. 중화민국(타이완)도 1976년 몬트리올 올림픽 참가를 보이콧했는데, 그 이유는 중화인민공화국(중국)이 몬트리올 올림픽 조직위원회에게 타이완을 \\'중화민국\\'의 이름으로 참가하지 못하도록 압박을 가했기 때문이다. 타이완은 이것에 반발해서 중화민국의 국기와 중화민국의 국가를 계속 쓸 것이라고 밝혔다. 타이완은 1984년까지 올림픽에 참가하지 않았으며 그 후 참가할 때는 중화 타이베이 올림픽기와 특별한 찬가를 사용한다. 1980년과 1984년 올림픽 때는 냉전의 당사국들이 각각 반대진영에서 개최된 올림픽에 불참했다. 1980년에 열린 모스크바 올림픽 때는 소련의 아프가니스탄 침공에 대한 항의의 표시로 미국을 비롯한 65개국이 불참해서 1956년 이후 가장 적은 국가의 수인 81개국만 참가하는 대회가 되었다. 1984년에 열린 L.A 올림픽때는 루마니아와 유고슬라비아를 제외한 소련과 동구권의 14개 국가가 자국 선수들의 안전을 보장받지 못한다는 이유로 올림픽에 불참했다. 소련의 한 관계자는 그들이 올림픽 보이콧을 한 것에 대해 다음과 같은 발언을 통해 지지했다.\\n\"미국에서 광적인 애국심과 반소련 세력이 점점 늘어나고 있다.\"\\n동구권에서 보이콧을 한 국가들은 올림픽을 대신할 대회로 프렌드십 게임을 7월과 8월에 했다.\\n2008년에는 티베트와 다르푸르에 관한 중국의 인권문제를 두고 그에 대한 항의 표시로 중국산 물품의 불매운동과 2008 올림픽 불참에 대한 요구가 컸으나 보이콧을 한 나라는 없었다. 2008년 8월, 조지아 정부는 러시아가 2008년 남오세티야 전쟁에 참전한 것과 관련하여 러시아의 소치에서 열릴 2014년 동계 올림픽을 보이콧하자고 요청했다. 이에 대해 국제 올림픽 위원회는 \"앞으로 개최될 때까지 6년이나 남았는데 시작하기도 전에 섣불리 이른 판단을 하는 것은 옳지 않다.\"라고 말했다.\\n\\n쿠베르탱이 말했던 원래 이념과는 반대로 올림픽이 정치 혹은 체제 선전의 장으로 이용되는 경우가 있었다. 1936년 하계 올림픽을 개최할 때 당시의 나치독일은 나치는 자비롭고 평화를 위한다는 것을 설명하고 싶어했다. 또 이 올림픽에서 아리안족의 우월함을 보여줄 생각이었으나 이는 흑인이었던 제시 오언스가 금메달을 4개나 따내면서 실현되지는 못했다. 소련은 헬싱키에서 열린 1952년 하계 올림픽 때 처음으로 참가했다. 그 전에는 소련이 조직한 스파르타키아다라는 대회에 1928년부터 참가했었다. 다른 공산주의 국가들은 1920년대와 1930년대의 전쟁 기간 사이에 노동자 올림픽(Socialist Workers\\' Sport International)을 조직했는데, 이는 올림픽을 자본가와 귀족들의 대회로 여기고 그에 대한 대안으로 고안된 대회였다. 그 이후 소련은 1956년 하계 올림픽부터 1988년 하계 올림픽까지 엄청난 스포츠강국의 면모를 보여주며 올림픽에서의 명성을 드높였다.\\n선수 개인이 자신의 정치적 성향에 대해 표현하기도 했다. 멕시코 시티에서 열린 1968년 하계 올림픽의 육상부문 200m 경기에서 각각 1위와 3위를 한 미국의 토미 스미스와 존 카를로스는 시상식 때 블랙 파워 설루트(Black Power salute , 흑인 차별 반대 행위)를 선보였으며 2위를 한 피터 노먼도 상황을 깨닫고 스미스와 카를로스의 행위를 지지한다는 뜻에서 급하게 인권을 위한 올림픽 프로젝트(OPHR) 배지를 달았다. 이 사건에 대해서 IOC 위원장이었던 에이버리 브런디지는 미국 올림픽 위원회에 이 두 선수를 미국으로 돌려보내거나 미국 육상팀 전부를 돌려보내는 둘 중 하나의 선택을 하게 했고, 미국 올림픽 위원회는 두 선수를 미국으로 돌려 보낸다.\\n현재 이란 정부는 이스라엘과의 어떤 경기 경쟁이든 피하고 있다. 2008년 하계 올림픽 때 이란의 수영 선수는 이스라엘 수영 선수와 같이 경기한다는 이유로 경기를 포기했으며, 2004년 하계 올림픽에서도 이란의 유도 선수는 이스라엘 선수와 경기한다는 일정이 잡혔을 때 경기를 포기했다. 이 선수는 공식적으로는 시합전에 계체량을 재서 체중이 초과되어 실격 되었으나 이란정부로부터 125,000달러나 되는 돈을 받았다고 한다.\\n\\n20세기 초반, 많은 운동 선수들은 기록향상을 위해 약물을 복용하기 시작했다. 예를 들어 1904년 하계 올림픽 마라톤에서 우승한 미국 선수 토머스 J. 힉스는 코치에게서 스트리크닌과 브랜디를 받았다. 올림픽에서 약물을 과다 복용으로 사망한 사례도 한 번 있었다. 1960년 로마 대회 때 사이클 개인도로 경기 중에 덴마크 선수인 크누드 에네마르크 옌센이 자전거에서 떨어져서 사망했다. 검시관들의 조사에 의하면 그의 죽음의 원인은 암페타민 과다 복용이라고 했다. 이에 1960년대 중반부터 각 경기 연맹은 약물 복용을 금지하기 시작했으며 1967년에는 IOC도 약물 복용 금지에 동참했다.\\n올림픽에서 약물 복용 양성 반응이 나와서 메달을 박탈당한 첫 번째 사례로는 1968년 하계 올림픽의 근대 5종 경기에 출전해 동메달을 딴 한스 군나르 리렌바르가 있다. 그는 경기 후 도핑검사 결과 알코올을 복용한 것으로 확인되어 메달을 박탈당했다. 도핑 양성 반응으로 메달을 박탈당한 것으로 가장 유명한 사람은 1988년 하계 올림픽 육상 100m 경기에서 금메달을 땄으나 도핑 검사 결과 스타노졸롤을 복용한 것으로 확인돼 금메달을 박탈당한 캐나다 선수인 벤 존슨이 있다. 이에 따라 금메달은 2위를 했던 칼 루이스가 대신 받았다.\\n1990년대 후반, 여러 뜻있는 사람들이 도핑과의 전쟁을 선포하면서 1999년에 세계반도핑기구(WADA)를 설립한다. 2000년 하계 올림픽과 2002년 동계 올림픽 때는 약물 양성 반응을 보인 선수들이 급격히 증가했고, 역도와 크로스컨트리에서는 몇몇 선수들이 도핑 테스트에 걸려서 실격되기도 했다. 2006년 동계 올림픽 때는 메달리스트 한 명이 양성반응을 보여 메달을 반납해야 했다. IOC가 만든 약물 반응 판정(현재 올림픽 도핑테스트의 기준이 됨)은 인정을 받게 되었고 이제는 다른 경기 연맹에서도 벤치마킹을 할 정도가 되었다. 2008년 베이징 올림픽 기간중에는 3,667명의 선수들이 세계반도핑기구의 검사를 받았으며 소변과 혈액 검사로 약물 복용 검사를 했다. 몇몇 선수들은 국가 올림픽 위원회(NOC)에 의해 올림픽이 시작되기 전에 출전금지 조치를 당했고, 올림픽 기간중에는 단 3명만이 도핑 검사에 걸렸다.\\n쿠베르탱의 생각과는 달리, 올림픽이 세계에 완벽한 평화를 가져다주지는 못했다. 실제로 제1차 세계대전으로 인해 독일 베를린에서 열리기로 했던 제6회 1916년 하계 올림픽이 취소되었고, 제2차 세계대전 때는 일본 도쿄에서 열리기로 했던 제12회 1940년 하계 올림픽, 삿포로에서 열리기로 했던 1940년 동계 올림픽, 영국 런던에서 열리기로 했던 제13회 1944년 하계 올림픽, 이탈리아 코르티나담페초에서 열릴 예정인 1944년 동계 올림픽이 취소되었다. 베이징에서 열린 2008년 하계 올림픽 개막식날 조지아와 러시아 간의 2008년 남오세티아 전쟁이 일어나기도 했다. 부시 대통령과 푸틴 대통령이 이 올림픽을 보러 왔으며 중국 주석인 후진타오가 주최한 오찬에 참석해서 이 현안에 대해 논의하기도 했다. 조지아 대표인 니노 살루크바체와 러시아 대표인 나탈리야 파데리나가 여자 10m 공기권총 경기에서 각각 동메달과 은메달을 땄을 때 이 일은 베이징 올림픽의 유명한 사건 중 하나로 남게 되었다. 살루크바체와 파데리나는 시상식이 끝난 뒤 서로 포옹을 하며 국적에 상관없이 기쁨을 나누었다.\\n테러도 올림픽에서 공포의 대상이었다. 뮌헨 참사로 알려진 1972년에 서독 바이에른의 뮌헨에서 열린 하계 올림픽때의 사건은 테러리스트인 검은 9월단이 일으킨 사건으로서 이스라엘 선수 11명을 인질로 붙잡았다가 전원이 사망한 사건이다. 당시 미숙한 진압으로 인해 인질 9명(선수 1명과 코치 1명은 인질로 잡기 이전에 살해), 테러범 5명, 독일 경찰관 1명이 사망했으며 이 진압 작전 이전에는 인질들은 단 한 명도 죽지 않았다. 애틀란타에서 열린 1996년 하계 올림픽 때는 센테니얼 올림픽 공원(Centennial Olympic Park)에서 폭발 사건이 일어나 2명이 죽고 111명이 다치는 사건이 발생했다. 이 사건의 주모자 에릭 로버트 루돌프는 종신형을 선고받았다. 참고로 마라톤 역시 전쟁에서 유래한 것이다.\\n\\n개최지 선정\\n올림픽 개최지는 해당 올림픽 개최 7년 전에 IOC 위원들의 투표로 결정된다. 개최지 선정에는 약 2년이 걸린다. 유치를 희망하는 도시는 우선 자국의 올림픽 위원회에 신청을 해야 한다. 만약 한 국가에서 두 도시 이상이 유치를 희망한다면, 한 국가당 한 도시만 후보가 될 수 있다는 규칙에 따라 내부적으로 후보 도시를 결정해야 한다. 후보 도시가 결정되면 후보 도시가 소속된 국가의 올림픽 위원회는 IOC에 개최 신청을 하고, 신청 후에는 올림픽 개최에 대한 질의 응답서를 보내야 한다. 이 질의응답서에서 신청한 도시는 올림픽 헌장을 준수하며 IOC 상임이사회에 의한 다른 규정들을 지킬 것이라는 확신을 주어야 한다. 이 질의응답서는 전문가들이 검토하여 신청 도시들의 잠재성과 계획을 평가한다. 이 전문적인 평가를 바탕으로 IOC 상임이사회에서는 신청도시 중에서 후보도시를 고른다.\\n후보도시로 선택되면 그 도시들은 IOC에 보내는 후보도시에 관한 문서에 그들의 계획을 더욱 상세하고 방대한 양으로 적어서 보내야 한다. 평가조사단들이 이 후보도시들을 평가한다. 평가조사단은 후보도시들을 방문해서 지역 관계자들과 회견을 갖고 경기장 시설을 세심하게 조사한 뒤 개최지 투표를 하기 한달전에 조사를 바탕으로 한 공식 보고를 한다. 회견을 하는 동안에도 후보도시들은 자신들이 올림픽을 개최하는 데 충분한 자금이 조달될 수 있는지 등을 입증할 수 있어야 한다. 평가조사단의 업무가 끝나면 후보지의 국가 위원들은 IOC 정기총회에 참석한다. 이 총회에서 IOC 위원들은 올림픽 개최지를 선정하게 되며 후보지의 국가에 소속된 위원들은 자국의 후보지가 탈락하지 않는 이상 투표를 할 수 없다. 투표가 끝난후에 개최지로 선정된 곳의 유치위원회가 IOC와 개최도시 계약서에 서명을 하면 공식적으로 올림픽 개최도시(개최국)으로 인정된다.\\n2016년까지 올림픽은 23개국 44개 도시에서 열렸으며 유럽과 북아메리카대륙 이외의 대륙에서는 고작 8번 밖에 개최하지 못했다. 1988년 하계 올림픽이 대한민국의 서울에서 열린것을 시작으로 그 후 아시아와 오세아니아 대륙에서 올림픽이 4번이나 열렸으며, 이는 그 이전의 현대 올림픽사와 비교해보면 엄청나게 늘어난 수치였다. 2016년 하계 올림픽이 개최된 브라질의 리우데자네이루는 남미에서 열리는 첫 번째 올림픽이다. 아직 아프리카에서는 올림픽이 한 번도 개최되지 않았다. 2008년 하계 올림픽 때 가장 많은 선수가 참여한 나라는 중국으로 639명이 참가했으며 그 다음은 미국과 러시아로 각각 596명과 455명이 참가했다.\\n미국은 5번의 하계 올림픽과 4번의 동계 올림픽을 개최하면서 최다 올림픽을 개최한 나라이다. 영국은 2012년에 3번째 올림픽을 개최하였다. 독일, 오스트레일리아, 그리스는 하계 올림픽을 2번 개최한 국가이다. 동계 올림픽에서는 이탈리아가 2026년 밀라노-코르티나담페초 개최지로 선정되어 3번 개최될 예정이다. 또한 프랑스가 3번을 개최했으며 2024년 하계올림픽 개최예정으로 영국에 이번 두 번째로 한 도시에서 3번 올림픽 개최하며 하계올림픽3번 개최하였다. 프랑스는 동계, 하계 올림픽 각 3번씩 총 6번 개최로 9번으로 최다개최국인 미국 다음으로 두 번째로 많이 개최한 국가가 된다. 스위스, 오스트리아, 노르웨이, 일본, 이탈리아는 2번씩 개최했다. 일본은 하계,동계 각 2번씩 총 4번으로 미국, 프랑스 다음 세번째로 많이 개최한 국가이다. 2010년에 밴쿠버에서 열린 2010년 동계 올림픽은 캐나다에서 열리는 두 번째 동계 올림픽이고, 동/하계 올림픽을 합쳐 캐나다에서 3번째로 개최되는 올림픽이다.\\n\\n우승자와 메달리스트\\n개인 혹은 팀으로 경기에 출전해서 1위, 2위, 3위를 한 선수는 메달을 받는다. 1912년까지는 우승자에게 순금으로 된 금메달을 주었으며 그 후에는 도금된 금메달을 준다. 하지만, 2010 동계 올림픽에서는 전자제품 부속품을 녹여서 넣었다. 이러한 경우처럼 순금 외에 다른 물질을 넣을 경우에는 순금이 반드시 6g 이상을 함유하고 있어야 한다. 2위를 한 선수는 은메달을, 3위를 한 선수는 동메달을 받는다. 토너먼트로 진행되는 종목의 경우에는(복싱, 태권도 등) 3위를 구분하지 않고 준결승에서 패해서 3/4위전으로 간 선수들에게 모두 동메달을 수여한다. 1896년 하계 올림픽에서는 메달이 2개만 수여됐는데 1위에게 은메달을 주었고 2위에게 동메달을 주었다. 이때 3위에게는 아무것도 없었다. 현재의 메달 수여 방식은 1904년 하계 올림픽 때부터 시작되었다. 1948년부터는 4, 5, 6위를 한 선수에게는 인증서를 수여했다. 1984년 대회부터는 7, 8위를 한 선수에게도 인증서를 수여했다. 아테네에서 열린 2004년 하계 올림픽 때는 1, 2, 3위 선수에게 메달과 함께 올리브 화환도 같이 수여했다. 국가 올림픽 위원회(NOC)와 방송사에서는 자국의 메달 현황을 실시간으로 전달하기도 한다.')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec8523a-e87c-4214-aee9-c1aa8d1745c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올림픽\n",
      "올림픽(영어: Olympic Games, 프랑스어: Jeux olympiques)은 전 세계 각 대륙 각국에서 모인 수천 명의 선수가 참가해 여름과 겨울에 스포츠 경기를 하\n"
     ]
    }
   ],
   "source": [
    "#문서내용\n",
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8afdf1c6-436e-4026-a178-7c8b48c360ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data/olympic.txt'}\n",
      "{'source': 'data/olympic.txt', 'category': '스포츠', 'teg': ['올림픽', '동계올림픽', 'IOC']}\n"
     ]
    }
   ],
   "source": [
    "# 문서관련 정보\n",
    "print(docs[0].metadata)\n",
    "docs[0].metadata['category']=\"스포츠\"\n",
    "docs[0].metadata['teg']=['올림픽','동계올림픽', 'IOC']\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bbbd08-6afc-4f2f-985b-c96da7c3b943",
   "metadata": {},
   "source": [
    "### PDF\n",
    "- PyPDF, Pymupdf 등 다양한 PDF 문서를 읽어들이는 파이썬의  3rd party library들을 이용해 pdf 문서를 Load 한다.\n",
    "    - https://python.langchain.com/docs/integrations/document_loaders/#pdfs\n",
    "- 각 PDF Loader 특징\n",
    "    -  PyMuPDFLoader\n",
    "        -   텍스트 뿐 아니라 이미지, 주석등의 정보를 추출하는데 성능이 좋다.\n",
    "        -   PyMuPDF 라이브러리 기반\n",
    "    - PyPDFLoader\n",
    "        - 텍스트를 빠르게 추출 할 수있다.\n",
    "        - PyPDF2 라이브러리 기반. 경량 라이브러리로 빠르고 큰 파일도 효율적으로 처리한다.\n",
    "    - PDFPlumberLoader\n",
    "        - 표와 같은 복잡한 구조의 데이터 처리하는데 강력한 성능을 보여준다. 텍스트, 이미지, 표 등을 모두 추출할 수 있다. \n",
    "        - PDFPlumber 라이브러리 기반\n",
    "- 설치 패키지\n",
    "    - DocumentLoader와 연동하는 라이브러리들을 설치 해야 한다.\n",
    "    - `pip install pypdf -qU`\n",
    "    - `pip install pymupdf -qU`\n",
    "    - `pip install pdfplumber -qU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9647515-7c2d-447d-a4e4-d70a18e4e025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m10 packages\u001b[0m \u001b[2min 265ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pypdfium2 \u001b[2m(3.0MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pdfminer-six \u001b[2m(5.4MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pymupdf \u001b[2m(17.6MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pypdfium2\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pymupdf\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pdfminer-six\n",
      "\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 434ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m8 packages\u001b[0m \u001b[2min 603ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcffi\u001b[0m\u001b[2m==2.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcryptography\u001b[0m\u001b[2m==46.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpdfminer-six\u001b[0m\u001b[2m==20251107\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpdfplumber\u001b[0m\u001b[2m==0.11.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycparser\u001b[0m\u001b[2m==2.23\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpymupdf\u001b[0m\u001b[2m==1.26.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpypdf\u001b[0m\u001b[2m==6.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpypdfium2\u001b[0m\u001b[2m==5.2.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install pypdf pymupdf pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdbd7f13-c4d5-4db7-b5d7-27de7984624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader, PDFPlumberLoader\n",
    "\n",
    "path = \"data/novel/동백꽃_김유정.pdf\"\n",
    "# loader = PyPDFLoader(path, mode=\"single\") \n",
    "# loader = PyMuPDFLoader(path)\n",
    "loader = PDFPlumberLoader(path)\n",
    "\n",
    "# mode: single - 한개 문서로 읽는다. ,page(defult): page별로 doc를 민든다.\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333cd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/novel/동백꽃_김유정.pdf',\n",
       " 'file_path': 'data/novel/동백꽃_김유정.pdf',\n",
       " 'page': 0,\n",
       " 'total_pages': 16,\n",
       " 'Author': 'Unknown',\n",
       " 'CreationDate': \"D:20241124070355+00'00'\",\n",
       " 'Creator': 'Wikisource',\n",
       " 'ModDate': \"D:20241124070356+00'00'\",\n",
       " 'Producer': 'Wikisource',\n",
       " 'Title': '동백꽃'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loader = PDFPlumberLoader(path)\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85df80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Wikisource',\n",
       " 'creator': 'Wikisource',\n",
       " 'creationdate': '2024-11-24T07:03:55+00:00',\n",
       " 'source': 'data/novel/동백꽃_김유정.pdf',\n",
       " 'file_path': 'data/novel/동백꽃_김유정.pdf',\n",
       " 'total_pages': 16,\n",
       " 'format': 'PDF 1.4',\n",
       " 'title': '동백꽃',\n",
       " 'author': '김유정',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2024-11-24T07:03:56+00:00',\n",
       " 'trapped': '',\n",
       " 'modDate': \"D:20241124070356+00'00'\",\n",
       " 'creationDate': \"D:20241124070355+00'00'\",\n",
       " 'page': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loader = PyMuPDFLoader(path)\n",
    "docs[0].metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7381b291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "for doc in docs:\n",
    "    doc.metadata['author']='김유정'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196a703e-dd86-4cb5-82bf-8b6726aea0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Wikisource',\n",
       " 'creator': 'Wikisource',\n",
       " 'creationdate': '2024-11-24T07:03:55+00:00',\n",
       " 'author': '김유정',\n",
       " 'moddate': '2024-11-24T07:03:56+00:00',\n",
       " 'title': '동백꽃',\n",
       " 'source': 'data/novel/동백꽃_김유정.pdf',\n",
       " 'total_pages': 16}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata #PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69cb3b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "동백꽃\n",
      "Exported from Wikisource on 2024 년  11 월  24 일\n",
      "\f2 \n",
      "오늘도  또  우리  수탉이  막  쫓기었다 . 내가  점심을  먹고  나무\n",
      "를  하러  갈  양으로  나올  때이었다 . 산으로  올라서려니까  등\n",
      "뒤에서  푸드득푸드득 , 하고  닭의  횃소리가  야단이다 . 깜짝\n",
      "놀라서  고개를  돌려보니  아니나다르랴 , 두  놈이  또  얼리었\n",
      "다 .\n",
      "점순네  수탉 ( 은  대강이가  크고  똑  오소리같이  실팍하게  생\n",
      "긴  놈 ) 이  덩저리  작은  우리  수탉을  함부로  해내는  것이다 . 그\n",
      "것도  그냥  해내는  것이  아니라  푸드득하고  면두를  쪼고  물러\n",
      "섰다가  좀  사이를  두고  푸드득하고  모가지를  쪼았다 . 이렇게\n",
      "멋을  부려  가며  여지없이  닦아  놓는다 . 그러면  이  못생긴  것\n",
      "은  쪼일  적마다  주둥이로  땅을  받으며  그  비명이  킥 , 킥 , 할\n",
      "뿐이다 . 물론  미처  아물지도  않\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772b6949",
   "metadata": {},
   "source": [
    "## CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fa792a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "path = \"data/boston_hosing.csv\"\n",
    "loder = CSVLoader(path)\n",
    "docs = loader.load() # 행 단위로 Document를 생성. # 왜 다른 걸 읽어오지 ????\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb78eee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청승맞게시리 호드기를 불고 있는 것이다. 그보다도 더 놀란\n",
      "것은 고 앞에서 또 푸드득, 푸드득, 하고 들리는 닭의 횃소리\n",
      "다. 필연코 요년이 나의 약을 올리느라고 또 닭을 집어내다\n",
      "가 내가 내려올 길목에다 쌈을 시켜 놓고 저는 그 앞에 앉아\n",
      "서 천연스레 호드기를 불고 있음에 틀림없으리라.\n",
      "나는 약이 오를 대로 올라서 두 눈에서 불과 함께 눈물이 퍽\n",
      "쏟아졌다. 나뭇지게도 벗어 놀 새 없이 그대로 내동댕이치고\n",
      "는 지게 막대기를 뻗치고 허둥허둥 달려들었다.\n",
      "가까이 와 보니 과연 나의 짐작대로 우리 수탉이 피를 흘리\n",
      "고 거의 빈사지경에 이르렀다. 닭도 닭이려니와 그러함에도\n",
      "불구하고 눈 하나 깜짝 없이 고대로 앉아서 호드기만 부는\n",
      "그 꼴에 더욱 치가 떨린다. 동네에서도 소문이 났거니와 나\n",
      "도 한때는 걱실걱실히 일 잘 하고 얼굴 예쁜 계집애인 줄 알\n",
      "았더니 시방 보니까 그 눈깔이 꼭 여우새끼 같다.\n",
      "나는 대뜸 달려들어서 나도 모르는 사이에 큰 수탉을 단매로\n",
      "때려 엎었다. 닭은 푹 엎어진 채 다리 하나 꼼짝 못 하고 그\n",
      "대로 죽어 버렸다. 그리고 나는 멍하니 섰다가 점순이가 매\n",
      "섭게 눈을 홉뜨고 닥치는 바람에 뒤로 벌렁 나자빠졌다.\n",
      "\"이놈아! 너 왜 남의 닭을 때려죽이니?\"\n",
      "\"그럼 어때?\"\n",
      "하고 일어나다가,\n",
      "\"뭐 이 자식아! 누 집 닭인데?\"\n",
      "11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "949ab0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 187ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 51ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 140ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbeautifulsoup4\u001b[0m\u001b[2m==4.14.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlxml\u001b[0m\u001b[2m==6.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msoupsieve\u001b[0m\u001b[2m==2.8.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install requests beautifulsoup4 lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507b334-e33b-435f-8a50-7dd31fa6da6b",
   "metadata": {},
   "source": [
    "### Web 문서 로드\n",
    "\n",
    "#### WebBaseLoader를 이용해 Web 문서로딩\n",
    "\n",
    "requests와 BeautifulSoup을 이용해 web 페이지의 내용을 크롤링해서 Document로 loading한다.\n",
    "\n",
    "- 주요 파라미터\n",
    "  - **web_paths***: str | list[str]\n",
    "    - 크롤링할 대상 URL\n",
    "  - **requests_kwargs**: dict\n",
    "    - requests.get() 에 전달할 파라미터를 dict로 전달. (key: parameter변수명, value: 전달할 값)\n",
    "    - headers, cookies, verify 등 설정 전달\n",
    "  - **header_template**: dict\n",
    "    - HTTP Header 에 넣을 값을 dict 로 전달.\n",
    "  - **encoding**\n",
    "    - requests의 응답 encoding을 설정 (bs_kwargs의 from_encoding 보다 상위에서 적용됨)\n",
    "  - **bs_kwargs**\n",
    "    - BeautifulSoup initializer에 전달할 파라미터를 dict로 전달. (key: parameter변수명, value: 전달할 값)\n",
    "    -  주요 옵션\n",
    "       - **parse_only**: 요청 페이지에서 특정 요소만 선택해서 가져오기. **SoupStrainer를 사용**한다.\n",
    "         - BeautifulSoup의 `SoupStrainer` 를 이용해 페이지의 일부분만 가져오기\n",
    "           - 웹 페이지를 파싱(parse, 구조 분석)할 때, 페이지 전체가 아닌 특정 부분만 필요한 경우가 많다. BeautifulSoup 라이브러리의 SoupStrainer를 사용하면, 원하는 태그나 속성이 있는 요소만 골라서 파싱할 수 있다.\n",
    "           - BeautifulSoup(\"html문서\", parse_only=Strainer객체)\n",
    "               - Strainer객체에 지정된 영역에서만 내용 찾는다.\n",
    "           - `SoupStrainer(\"태그명\")`, `SoupStrainer([\"태그명\", \"태그명\"])`\n",
    "             - 지정한 태그 만 조회\n",
    "           - `SoupStrainer(name=\"태그명\", attrs={속성명:속성값})`\n",
    "             -  지정한 태그 중 속성명=속성값인 것만 조회\n",
    "        - **from_encoding**: Encoding 설정 \n",
    "          - \"from_encoding\":\"utf-8\"\n",
    "   - **bs_get_text_kwargs**:\n",
    "     - BeautifulSoup객체.get_text() 에 전달할 파라미터 dict로 전달. (key: parameter변수명, value: 전달할 값)\n",
    "     - **RAG 구축시 `separator` 와 `strip=True` 으로 설정하는 것이 좋다.** (RAG 품질을 위해 강력히 권장되는 설정이다.)\n",
    "       -  get_text() 는 기본적으로 태그를 제거하고 텍스트만 이어 붙여 반환한다. `separator=구분자문자` 를 지정하여 추출된 텍스트 요소들 사이에 원하는 구분자를 지정할 수있다. `\\n` 을 구분자로 사용하면 텍스트 블록 사이에 줄바꿈이 들어가 **문단의 구조를 어느정도 살릴 수 있다.**\n",
    "       -  웹 문서의 줄바꿈도 포함해서 읽기 때문에 공백과 줄바꿈이 혼재된 상태로 반환된다. `strip=True`로 설정하면 추출된 문자 앞뒤의 공백 문자들을 제거할 수있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da98b0ba-97ac-445d-85c8-1e000299f926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------기본-----------\n",
      "\n",
      "\n",
      "제목내용\n",
      "다음문단\n",
      "다음 내용\n",
      "\n",
      "\n",
      "\n",
      "-----------strip=Ture-----------\n",
      "제목내용다음문단다음 내용\n",
      "-----------strip=True, separator=-----------\n",
      "제목\n",
      "\n",
      "내용\n",
      "\n",
      "다음문단\n",
      "\n",
      "다음 내용\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "html_txt = \"\"\" \n",
    "<html>\n",
    "<body>\n",
    "<p><b>제목</b><span>내용</span></p>\n",
    "<p>다음문단</p>\n",
    "<div>다음 내용</div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "soup=BeautifulSoup(html_txt)\n",
    "# 태그 제외 text만 추출 ->get_text*()\n",
    "txt1 = soup.get_text()\n",
    "\n",
    "print(\"-----------기본-----------\")\n",
    "print(txt1)\n",
    "\n",
    "txt2 = soup.get_text(strip=True) # 좌우 공백문자(공백, 엔터) 제거\n",
    "print(\"-----------strip=Ture-----------\")\n",
    "print(txt2)\n",
    "\n",
    "txt3 = soup.get_text(strip=True, separator=\"\\n\\n\") # 각 태그의 text를 지정한 구분자로 나눈다.\n",
    "print(\"-----------strip=True, separator=-----------\")\n",
    "print(txt3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "404b5826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 크롬 my user agent 로 검색\n",
    "# USER AGENT를 환경변수에 등록\n",
    "os.environ['USER_AGENT'] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)AppleWebKit/537.36\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc73b892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "urls=[\n",
    "    \"https://m.sports.naver.com/kfootball/article/001/0015808018\",\n",
    "    \"https://m.sports.naver.com/golf/article/382/0001243588\"\n",
    "]\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_path=urls,\n",
    "    default_parser=\"lxml\" #BeautifulSoup (문서, 'lxml')\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d416161b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'ko',\n",
      " 'source': 'https://m.sports.naver.com/kfootball/article/001/0015808018',\n",
      " 'title': \"손흥민, 3년 연속 'KFA 올해의 골'…볼리비아전 프리킥 득점\"}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "손흥민, 3년 연속 'KFA 올해의 골'…볼리비아전 프리킥 득점NAVER스포츠뉴스엔터메뉴홈야구해외야구축구해외축구농구배구N골프일반e스포츠아웃도어NEW뉴스영상일정순위포토홈 바로가기NAVER스포츠뉴스엔터스포츠야구해외야구축구해외축구농구배구N골프일반e스포츠아웃도어콘텐츠오늘의 경기승부예측연재이슈톡대학스포츠랭킹기타고객센터공식 블로그메뉴 닫기본문 바로가기손흥민, 3년 연속 'KFA 올해의 골'…볼리비아전 프리킥 득점입력2025.12.22. 오전 10:49수정2025.12.22. 오전 10:50기사원문공감좋아요0슬퍼요0화나요0팬이에요0후속기사 원해요0텍스트 음성 변환 서비스본문 듣기를 종료하였습니다.글자 크기 변경공유하기'2025 KFA 올해의 경기'도 2-0으로 이긴 볼리비아 평가전프리킥 선제골 넣는 손흥민(대전=연합뉴스) 서명곤 기자 = 14일 대전월드컵경기장에서 열린 남자축구 국가대표 평가전 대한민국 대 볼리비아의 경기에서 손흥민이 프리킥으로 선제골을 넣고 있다. 2025.11.14 seephoto@yna.co.kr(서울=연합뉴스) 이영호 기자 = 한국 축구대표팀의 '캡틴' 손흥민(LAFC)이 지난 11월 볼리비아와 평가전에서 터트린 기막힌 프리킥 득점이 '2025 대한축구협회(KFA) 올해의 골'로 선정됐다.    대한축구협회는 22일 팬들이 직접 참여한 '2025 KFA 올해의 골'과 '2025 KFA 올해의 경기' 투표 결과를 발표했다.     축구협회 마케팅 플랫폼 'PlayKFA'를 통해 지난 8일부터 18일까지 진행된 투표에는 1천450명이 참여했다.    투표 결과 손흥민이 지난 11월 14일 대전월드컵경기장에서 열린 볼리비아와 평가전에서 뽑아낸 프리킥 득점이 57.4%의 지지를 얻어 올해를 가장 빛낸 골로 선정됐다.    손흥민은 후반 12분 페널티아크 왼쪽 부근에서 얻은 프리킥 상황에서 오른발 슈팅으로 수비벽을 넘기고 골대 왼쪽 상단에 볼을 꽂았다.손흥민의 세리머니(대전=연합뉴스) 임화영 기자 = 14일 대전월드컵경기장에서 열린 남자축구 국가대표 A매치 평가전 대한민국과 볼리비아의 경기. 손흥민이 골을 넣은 뒤 세리머니를 하고 있다. 2025.11.14 hwayoung7@yna.co.kr    손흥민에 이어 이강인이 월드컵 3차 예선 쿠웨이트전에서 뽑아낸 득점이 24.2% 득표율로 2위에 올랐다.    이로써 손흥민은 2023년 월드컵 2차 예선 싱가포르와의 홈경기에서 나온 중거리 슈팅 득점과 2024년 아시안컵 8강 호주전에서 나온 연장전 프리킥 득점에 이어 3년 연속 팬들이 뽑은 '올해의 골'로 선정되는 기쁨을 맛봤다.    또 '2025 KFA 올해의 경기'는 손흥민과 조규성이 나란히 골 맛을 보고 2-0 승리를 따낸 볼리비아 평가전이 42.7%의 득표율로 1위를 차지했다.    지난 9월 원정으로 치른 미국전(2-0승)이 25.5%로 2위, 지난 6월 북중미 월드컵 본선 진출을 확정한 직후 치른 쿠웨이트전(4-0승)이 14.5%로 3위에 올랐다.    축구협회는 2022년부터 매년 연말 그해 열린 각급 대표팀 경기를 대상으로 '올해의 골'과 '올해의 경기'를 뽑는 팬 투표를 실시하고 있다.    horn90@yna.co.kr본문의 검색 링크는 AI 자동 인식으로 제공됩니다. 일부에 대해서는 미제공될 수 있고 동일한 명칭이 다수 존재하는 경우에는 전체 검색 결과로 연결될 수 있습니다.오분류 제보하기이영호 기자구독 0응원 0구독음바페, 올해 59호골 작렬…호날두와 '단일 연도 최다골' 동률'개최국' 모로코, 네이션스컵 개막전서 코모로에 2-0 승리연합뉴스언론사홈 바로가기Copyright ⓒ 연합뉴스. All rights reserved. 무단 전재-재배포, AI 학습 및 활용 금지.기사 섹션 분류 가이드기사 섹션 분류 안내스포츠 기사 섹션(종목) 정보는 언론사 분류와 기술 기반의 자동 분류 시스템을 따르고 있습니다. 오분류에 대한 건은 네이버스포츠로 제보 부탁드립니다.오분류 제보하기닫기[앱 다운] 빠르고 정확한 연합뉴스를 받아보세요뉴스제보는 연합뉴스 okjebo 톡!주요뉴스해당 언론사에서 선정하며 언론사 페이지(아웃링크)로 이동해 볼 수 있습니다. 첫 원피스 유니폼으로 파격 변신한 안세영 \"확실히 가벼운 듯\" 한국계 NASA 우주비행사 조니 김 \"우주서 김치·밥 그리웠다\" '사생활 논란' 정희원 박사, 서울시 건강총괄관 사의 표명 BTS \"새앨범 녹음 끝나…2026년은 방탄소년단의 해\" 논란에도 방송 복귀한 백종원·곽도원…시청자 반응 온도차좋아요0슬퍼요0화나요0팬이에요0후속기사 원해요0\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content) # 노이즈가 너무 많음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cee5a547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import SoupStrainer\n",
    "#SoupStrainer\n",
    "#(name=\"a\") # a 태그들\n",
    "# (name='a', attr={\"href\":\"........\"}) #태그 + 속성 조건.\n",
    "# (id=\"tag의 id\") # id로 조회\n",
    "\n",
    "loader2 = WebBaseLoader(\n",
    "    web_path=urls,\n",
    "    bs_kwargs ={\n",
    "        \"parse_only\":SoupStrainer(attrs={\"class\":['_article_content']})\n",
    "    },\n",
    "    bs_get_text_kwargs={\n",
    "        \"separator\":\"\\n\", \"strip\":True\n",
    "    }\n",
    ")\n",
    "\n",
    "docs2 = loader2.load()\n",
    "len(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6df1f218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://m.sports.naver.com/kfootball/article/001/0015808018'}\n"
     ]
    }
   ],
   "source": [
    "print(docs2[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db48b2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2025 KFA 올해의 경기'도 2-0으로 이긴 볼리비아 평가전\n",
      "프리킥 선제골 넣는 손흥민\n",
      "(대전=연합뉴스) 서명곤 기자 = 14일 대전월드컵경기장에서 열린 남자축구 국가대표 평가전 대한민국 대 볼리비아의 경기에서 손흥민이 프리킥으로 선제골을 넣고 있다. 2025.11.14 seephoto@yna.co.kr\n",
      "(서울=연합뉴스) 이영호 기자 = 한국 축구대표팀의 '캡틴'\n",
      "손흥민\n",
      "(LAFC)이 지난 11월 볼리비아와 평가전에서 터트린 기막힌 프리킥 득점이 '2025 대한축구협회(KFA) 올해의 골'로 선정됐다.\n",
      "대한축구협회는 22일 팬들이 직접 참여한 '2025 KFA 올해의 골'과 '2025 KFA 올해의 경기' 투표 결과를 발표했다.\n",
      "축구협회 마케팅 플랫폼 'PlayKFA'를 통해 지난 8일부터 18일까지 진행된 투표에는 1천450명이 참여했다.\n",
      "투표 결과 손흥민이 지난 11월 14일 대전월드컵경기장에서 열린 볼리비아와 평가전에서 뽑아낸 프리킥 득점이 57.4%의 지지를 얻어 올해를 가장 빛낸 골로 선정됐다.\n",
      "손흥민은 후반 12분 페널티아크 왼쪽 부근에서 얻은 프리킥 상황에서 오른발 슈팅으로 수비벽을 넘기고 골대 왼쪽 상단에 볼을 꽂았다.\n",
      "손흥민의 세리머니\n",
      "(대전=연합뉴스) 임화영 기자 = 14일 대전월드컵경기장에서 열린 남자축구 국가대표 A매치 평가전 대한민국과 볼리비아의 경기. 손흥민이 골을 넣은 뒤 세리머니를 하고 있다. 2025.11.14 hwayoung7@yna.co.kr\n",
      "손흥민에 이어\n",
      "이강인\n",
      "이 월드컵 3차 예선 쿠웨이트전에서 뽑아낸 득점이 24.2% 득표율로 2위에 올랐다.\n",
      "이로써 손흥민은 2023년 월드컵 2차 예선 싱가포르와의 홈경기에서 나온 중거리 슈팅 득점과 2024년 아시안컵 8강 호주전에서 나온 연장전 프리킥 득점에 이어 3년 연속 팬들이 뽑은 '올해의 골'로 선정되는 기쁨을 맛봤다.\n",
      "또 '2025 KFA 올해의 경기'는 손흥민과\n",
      "조규성\n",
      "이 나란히 골 맛을 보고 2-0 승리를 따낸 볼리비아 평가전이 42.7%의 득표율로 1위를 차지했다.\n",
      "지난 9월 원정으로 치른 미국전(2-0승)이 25.5%로 2위, 지난 6월 북중미 월드컵 본선 진출을 확정한 직후 치른 쿠웨이트전(4-0승)이 14.5%로 3위에 올랐다.\n",
      "축구협회는 2022년부터 매년 연말 그해 열린 각급 대표팀 경기를 대상으로 '올해의 골'과 '올해의 경기'를 뽑는 팬 투표를 실시하고 있다.\n",
      "horn90@yna.co.kr\n"
     ]
    }
   ],
   "source": [
    "print(docs2[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81342a",
   "metadata": {},
   "source": [
    "#### RecursiveUrlLoader\n",
    "\n",
    "- 주어진 URL에서 시작하여 그 페이지 안의 내부 링크를 재귀적으로 따라가며 여러 웹 문서를 자동 수집하여 로드한다.\n",
    "  - 시작 url을 요청/페이지를 파싱 한 뒤에 `<a href>` 들을 수집하고 그 페이지들을 요청/페이지 파싱을 한다. \n",
    "- WebBaseLoader가 단일 페이지(단일 URL) 단위라면 RecursiveUrlLoader는 **웹 사이트 구조 전체를 크롤링하는 전용 수집기**에 가깝다.\n",
    "```bash\n",
    "시작 URL\n",
    " ├─ 내부 링크 1\n",
    " │   ├─ 내부 링크 1-1\n",
    " │   └─ 내부 링크 1-2\n",
    " ├─ 내부 링크 2\n",
    " └─ 내부 링크 3\n",
    "```\n",
    "위 구조일때 무든 페이지를 재귀적으로 수집한다.\n",
    "- 주요 파라미터\n",
    "  - **url**: 시작 url\n",
    "  - **max_depth**\n",
    "    - 링크를 몇 단계 **깊이** 까지 따라갈지 제한\n",
    "    - 사이트 폭주를 막기 위한 안전장치\n",
    "      - **0**: 시작페이지만, **1**: 시작페이지 + 1차링크, **2**(기본값): 시작페이지 + 1차링크 + 2차링크\n",
    "  - **exclude_dirs**: list[str]\n",
    "    - 크롤링 제외 경로\n",
    "    - ex) `exclude_dirs=['/login', 'signup']`\n",
    "  - **prevent_outside**: bool\n",
    "    - True: base_url 바깥 링크는 가져오지 않고 무시한다.\n",
    "  - **base_url**: str\n",
    "    - prevent_outside=True일 때 바깥링크의 기준. 없으면 `url`(시작 url)의 host가 된다. \n",
    "  - **extractor**\n",
    "    - 문서 내용 추출 사용자 정의 함수\n",
    "    - default는 응답 받은 페이지를 `BeautifulSoup(응답페이지).get_text()` 로 텍스트를 추출한다.\n",
    "    - ````python\n",
    "        def custom_extractor(html:str) ->str:\n",
    "            # 웹 페이지 문서를 입력으로 받는다.\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "            return soup.select_one('article').get_text() # 원하는 항목을 추출해서 반환한다.\n",
    "        \n",
    "        loader = RecursiveUrlLoader(\n",
    "            url=start_url,\n",
    "            extractor=custom_extractor\n",
    "        )    \n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb3a6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "\n",
    "def extractor(html:str)->str:\n",
    "    # 전체 페이지를 받아서 원하는 부분만 parsing 한 뒤에 반환.\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    body = soup.select_one(\"div.body\")\n",
    "    return body.get_text(strip=True, separator=\"\\n\") if body else soup.get_text(strip=True, separator=\"\\n\")\n",
    "\n",
    "url = \"https://docs.python.org/3\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url,\n",
    "    extractor=extractor,\n",
    "    max_depth=2,\n",
    "    prevent_outside=True, #url 외부링크는 가져오지 않도록 함. #기본은 url의 host까지\n",
    "    base_url=url\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb1ac965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2c5d49d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content_type': 'text/html',\n",
      " 'language': 'en',\n",
      " 'source': 'https://docs.python.org/3.11/',\n",
      " 'title': '3.11.14 Documentation'}\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "pprint(docs[idx].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5f07dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.14 documentation\n",
      "Welcome! This is the official documentation for Python 3.11.14.\n",
      "Documentation sections:\n",
      "What's new in Python 3.11?\n",
      "Or\n",
      "all \"What's new\" documents since Python 2.0\n",
      "Tutorial\n",
      "Start here: a tour of Python's syntax and features\n",
      "Library reference\n",
      "Standard library and builtins\n",
      "Language reference\n",
      "Syntax and language elements\n",
      "Python setup and usage\n",
      "How to install, configure, and use Python\n",
      "Python HOWTOs\n",
      "In-depth topic manuals\n",
      "Installing Python modules\n",
      "Third-party modules and PyPI.org\n",
      "Distributing Python modules\n",
      "Publishing modules for use by other people\n",
      "Extending and embedding\n",
      "For C/C++ programmers\n",
      "Python's C API\n",
      "C API reference\n",
      "FAQs\n",
      "Frequently asked questions (with answers!)\n",
      "Indices, glossary, and search:\n",
      "Global module index\n",
      "All modules and libraries\n",
      "General index\n",
      "All functions, classes, and terms\n",
      "Glossary\n",
      "Terms explained\n",
      "Search page\n",
      "Search this documentation\n",
      "Complete table of contents\n",
      "Lists all sections and subsections\n",
      "Project information:\n",
      "Reporting issues\n",
      "Contributing to Docs\n",
      "Download the documentation\n",
      "History and license of Python\n",
      "Copyright\n",
      "About the documentation\n"
     ]
    }
   ],
   "source": [
    "print(docs[idx].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422be8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd26b68d",
   "metadata": {},
   "source": [
    "### ArxivLoader\n",
    "- https://github.com/lukasschwab/arxiv.py\n",
    "- [arXiv-아카이브](https://arxiv.org/) 는 미국 코렐대학에서 운영하는 **무료 논문 저장소**로, 물리학, 수학, 컴퓨터 과학, 생물학, 금융, 경제 등 **과학, 금융 분야의 논문**들을 공유한다.\n",
    "- `ArxivLoader` 를 사용해 원하는 주제의 논문들을 arXiv에서 가져와 load할 수 있다.\n",
    "- **arXiv API**를 사용해 논문을 가져올 수 있다.\n",
    "  - https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.arxiv.ArxivLoader.html\n",
    "- 설치\n",
    "  - `pip install langchain-community -qU`\n",
    "  - `pip install arxiv -qU`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f45cfa67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m8 packages\u001b[0m \u001b[2min 4.57s\u001b[0m\u001b[0m\n",
      "   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m\n",
      "      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m sgmllib3k\u001b[2m==1.0.0\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 1.05s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 34ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1marxiv\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfeedparser\u001b[0m\u001b[2m==6.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msgmllib3k\u001b[0m\u001b[2m==1.0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6072b39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m2 packages\u001b[0m \u001b[2min 77ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pip \u001b[2m(1.7MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pip\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 1.93s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 594ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpip\u001b[0m\u001b[2m==25.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpip-system-certs\u001b[0m\u001b[2m==5.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install pip-system-certs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb21d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip_system_certs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f210e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv lib 사용\n",
    "import arxiv\n",
    "# 검색설정\n",
    "search = arxiv.Search(\n",
    "    query=\"Advanced RAG\", # 검색어\n",
    "    max_results=5, # 검색 논문 최대 개수\n",
    "    sort_by = arxiv.SortCriterion.LastUpdatedDate # 정렬기준\n",
    ")\n",
    "\n",
    "#정렬기준 : LastUpdateDate - 논문이 마지막으로 수정된 날짜 기준.\n",
    "            #Relevance: Query와 관련성이 높은 순서\n",
    "            #SubnittedDate: 논문이 처음 제출된 날짜 기준\n",
    "# 검색 처리 Client\n",
    "client = arxiv.Client()\n",
    "results = client.results(search) # 검색 (iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1b99523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'itertools.islice'>\n"
     ]
    }
   ],
   "source": [
    "print(type(results)) # iterator: next(), for in\n",
    "# 첫번째 것만 조회\n",
    "paper = next(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64f1737f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training\n",
      "[arxiv.Result.Author('Kristoffer Wickstrøm'), arxiv.Result.Author('Teresa Dorszewski'), arxiv.Result.Author('Siyan Chen'), arxiv.Result.Author('Michael Kampffmeyer'), arxiv.Result.Author('Elisabeth Wetzer'), arxiv.Result.Author('Robert Jenssen')]\n",
      "Kristoffer Wickstrøm\n",
      "Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.\n",
      "https://arxiv.org/pdf/2512.17891v1\n",
      "2512.17891v1\n"
     ]
    }
   ],
   "source": [
    "# 논문 정보\n",
    "print(paper.title) # 제목\n",
    "print(paper.authors) # 논문 저자\n",
    "print(paper.authors[0].name) # Author.name : 이름 추출\n",
    "print(paper.summary) # 논문 요약(초록)\n",
    "print(paper.pdf_url) # arivx의 논문 URL\n",
    "print(paper.get_short_id()) #arivx 의 이 논문의 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c6663fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/papers\\\\2512.17891v1.pdf'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 논문 저장\n",
    "import os\n",
    "os.makedirs('data/papers', exist_ok=True)\n",
    "\n",
    "paper.download_pdf(dirpath=\"data/papers\", filename=f\"{paper.get_short_id()}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f34f189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 다운로드\n",
    "for paper in results:\n",
    "    paper.download_pdf(dirpath=\"data/papers\", filename=f\"{paper.get_short_id()}.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61a55b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Langchain - ArxivLoader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "    query=\"RAG\",\n",
    "    top_k_results=10\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a82289e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b89859c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2025-05-31',\n",
       " 'Title': 'RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation',\n",
       " 'Authors': 'Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, Zhiyong Lu, Aidong Zhang',\n",
       " 'Summary': 'Retrieval-augmented generation (RAG) has shown great promise for knowledge-intensive tasks and recently advanced with agentic RAG, where language agents engage in multi-round interactions with external knowledge sources for adaptive information retrieval. However, existing agentic RAG methods often depend on ad-hoc prompt engineering and lack a unified optimization framework. We introduce RAG-Gym, a comprehensive platform that systematically explores three optimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic training. For prompt engineering, we propose Re$^2$Search, a novel agent incorporating reasoning reflection that significantly outperforms standard prompts. In actor tuning, we evaluate three popular post-training algorithms with fine-grained process supervision and identify direct preference optimization as the most effective. We further demonstrate that a trained critic can enhance inference by selecting higher-quality intermediate reasoning steps. Together, these findings lead to the optimized Re$^2$Search++ agent, which surpasses most recent methods like Search-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact of different reward sources and analyze scaling properties in training and inference, offering practical insights for agentic RAG optimization. The project homepage is available at https://rag-gym.github.io.'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e5947c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2502.13957v2  [cs.CL]  31 May 2025\n",
      "RAG-Gym: Systematic Optimization of Language\n",
      "Agents for Retrieval-Augmented Generation\n",
      "Guangzhi Xiong∗1, Qiao Jin∗2, Xiao Wang3, Yin Fang2, Haolin Liu1, Yifan Yang2, Fangyuan\n",
      "Chen4, Zhixing Song5, Dengyu Wang6, Minjia Zhang3, Zhiyong Lu†2, and Aidong Zhang†1\n",
      "1University of Virginia, 2National Institutes of Health, 3University of Illinois at Urbana Champaign,\n",
      "4Dana-Farber Cancer Institute, 5University of Alabama at Birmingham, 6Yale School of Medicine\n",
      "Abstract\n",
      "Retrieval-augmented generation (RAG) has shown great promise for knowledge-\n",
      "intensive tasks and recently advanced with agentic RAG, where language agents\n",
      "engage in multi-round interactions with external knowledge sources for adaptive\n",
      "information retrieval. However, existing agentic RAG methods often depend on\n",
      "ad-hoc prompt engineering and lack a unified optimization framework. We in-\n",
      "troduce RAG-Gym, a comprehensive platform that systematically explores three\n",
      "optimization dimensions: (1) prompt engineering, (2) actor tuning, and (3) critic\n",
      "training. For prompt engineering, we propose Re2Search, a novel agent incorporat-\n",
      "ing reasoning reflection that significantly outperforms standard prompts. In actor\n",
      "tuning, we evaluate three popular post-training algorithms with fine-grained process\n",
      "supervision and identify direct preference optimization as the most effective. We\n",
      "further demonstrate that a trained critic can enhance inference by selecting higher-\n",
      "quality intermediate reasoning steps. Together, these findings lead to the optimized\n",
      "Re2Search++ agent, which surpasses most recent methods like Search-R1 by a\n",
      "relative increase of 3.2% to 11.6% in average F1. Finally, we examine the impact\n",
      "of different reward sources and analyze scaling properties in training and inference,\n",
      "offering practical insights for agentic RAG optimization. The project homepage is\n",
      "available at https://rag-gym.github.io/.\n",
      "1\n",
      "Introduction\n",
      "Large language models (LLMs) often struggle with knowledge-intensive questions when lacking\n",
      "sufficient or up-to-date domain knowledge, leading to inaccurate responses or hallucinations [97,\n",
      "59, 28]. Retrieval-augmented generation (RAG) addresses this by grounding outputs in relevant\n",
      "information from information retrieval (IR) systems, improving both accuracy and verifiability of\n",
      "answers [42, 18]. Agentic pipelines such as ReAct [91] enhances conventional RAG by allowing\n",
      "LLMs to actively generate search queries and interact with IR systems in multiple rounds, which has\n",
      "been shown to be more effective in solving complex tasks that need multi-hop reasoning [91, 4, 65].\n",
      "However, most existing agentic RAG methods focus on prompt engineering [73, 4, 31, 54], which\n",
      "demands substantial manual effort and often fails to generalize across tasks [40, 70, 2].\n",
      "Meanwhile, although various LLM post-training algorithms have been developed to enhance down-\n",
      "stream performance, they are not directly suited for agentic RAG, where the model must dynamically\n",
      "adjust its token-generation strategy in response to newly retrieved context during the reasoning\n",
      "process. Recent works have adapted reinforcement learning with outcome-based rewards for agentic\n",
      "RAG [69, 33, 8]. However, by overlooking process-level supervision, these approaches risk generat-\n",
      "ing suboptimal intermediate search actions and exhibit limited generalization on unseen data. Given\n",
      "∗Equal contribution. †Co-correspondence.\n",
      "Preprint. Under review.\n",
      "that the retrieval steps fundamentally shape the reasoning trajectory and ultimately influence the final\n",
      "answer, providing fine-grained supervision over these intermediate steps is essential for optimizing\n",
      "agentic RAG. Nevertheless, systematic analyses on how to optimize the language agent and identify\n",
      "best practices for enhancing overall agentic RAG performance are still lacking.\n",
      "In this work, we present RAG-Gym, a systematic framework that enhances agentic RAG along three\n",
      "dimensions: prompt engineering, actor tuning, and critic training. We review and compare the func-\n",
      "tional components of existing agentic RAG pipelines (see Table 1) and introduce a novel agent design\n",
      "Re2Search that leverages reasoning reflection to improve performance. Our comprehensive experi-\n",
      "ments across three widely used LLM post-training algorithms reveal that fine-grained, process-level\n",
      "supervision substantially boosts performance, particularly when both positive and negative feedback\n",
      "are integrated. Furthermore, we show that training a critic to evaluate intermediate steps yields\n",
      "additional gains across diverse LLMs. By integrating these insights, our optimized Re2Search++\n",
      "agent achieves superior performance than existing methods on challenging knowledge-intensive tasks\n",
      "(+ 3.2%∼11.6% in average F1), especially on unseen datasets (+ 8.5%∼24.7%). We also discuss\n",
      "reward sources as well as the training and inference scaling properties of agentic RAG, providing\n",
      "practical guidelines for future optimization. Our key contributions are summarized as follows:\n",
      "• We introduce RAG-Gym, a comprehensive framework that integrates advanced prompt engineering,\n",
      "actor tuning, and critic training to enhance agentic RAG.\n",
      "• Our extensive experiments uncover best practices across these dimensions and lead to the develop-\n",
      "ment of the optimized agent Re2Search++, which consistently outperforms existing methods on\n",
      "challenging knowledge-intensive tasks.\n",
      "• We provide a detailed analysis of reward sources as well as training and inference scaling properties,\n",
      "offering actionable insights for future advancements in agentic RAG.\n",
      "2\n",
      "RAG-Gym Framework\n",
      "To facilitate fine-grained process-level supervision and systematic evaluation of optimization methods\n",
      "for agentic RAG, we introduce the RAG-Gym framework. RAG-Gym formulates knowledge-intensive\n",
      "question answering as a high-level MDP with well-defined intermediate actions, and provides a\n",
      "modular approach for optimizing language agents across three key components. An overview of\n",
      "RAG-Gym is presented in Figure 1.\n",
      "2.1\n",
      "Knowledge-intensive Question Answering as Markov Decision Process\n",
      "While sequential token generation in LLMs can be modeled as an MDP [43, 49, 93], the integration\n",
      "of interactions with the IR environment introduces complex and inconsistent state transitions across\n",
      "agent architectures. To address this, we propose a hierarchical MDP formulation in RAG-Gym that\n",
      "unifies diverse agentic RAG designs. At the high level, agentic RAG is represented as a sequence of\n",
      "reasoning steps that interact with an IR system, while at the low level, each action involves sequential\n",
      "token generation by LLMs. Below, we formally define the components of the high-level MDP.\n",
      "States. For the agentic RAG process of a given question Q, we define the state st at time step\n",
      "t to be a set consisting of the original question Q and the information-seeking history Ht. The\n",
      "information-seeking history is a sequence of search queries q1, · · · , qt−1 and their corresponding sets\n",
      "of retrieved documents D1, · · · , Dt−1, and is used to augment the agent’s knowledge for answering\n",
      "the original question. The initial state is defined as s1 = (Q, H1), where H1 is an empty set.\n",
      "Actions. Although agents may employ various strategies to reason about the current state and generate\n",
      "different token sequences, RAG-Gym standardizes these outputs by defining a common macro-action\n",
      "space. At each time step t, the action at is either a search query or a predicted answer to the original\n",
      "question. While the detailed generated token sequences may differ among agent designs, they must\n",
      "always be semantically equivalent to a designated macro-action within the context of agentic RAG.\n",
      "Environment. The high-level MDP environment in RAG-Gym is powered by an IR system, which\n",
      "is central to the agentic RAG approach. At each time step t, if the agent’s action at is a search\n",
      "query qt, the IR system returns a corresponding set of documents Dt. The state is then updated\n",
      "from st = (Q, Ht) to st+1 = (Q, Ht ∪{(qt, Dt)}). Conversely, if at predicts an answer to Q, the\n",
      "2\n",
      "Process \n",
      "Reward\n",
      "Process \n",
      "Reward\n",
      "Process \n",
      "Reward\n",
      "Answer \n",
      "Generation\n",
      "Question\n",
      "Reasoning\n",
      "Retrieval \n",
      "Augmentation\n",
      "Document \n",
      "Summarization\n",
      "Reasoning \n",
      "Reflection\n",
      "Query \n",
      "Generation\n",
      "What is the date of death of\n",
      "Alberto De Martino?\n",
      "…The date of death of Alberto\n",
      "De Martino is 2 June 2015…\n",
      " \n",
      "What is the date of death of the\n",
      "director of film Holocaust 2000?\n",
      " \n",
      "Who is the director of the film\n",
      "‘Holocaust 2000’?\n",
      " \n",
      "… director of the film ‘Holocaust\n",
      "2000’ is Alberto De Martino…\n",
      "Query:\n",
      "Documents:\n",
      "Agent\n",
      "Action(s)\n",
      "What is the date of birth\n",
      "of Alberto De Martino?\n",
      "What is the date of death\n",
      "of Alberto De Martino?\n",
      "Alberto De Martino’s date\n",
      "of death is 1990. \n",
      "State\n",
      "Question: ...\n",
      "Query: ...\n",
      "Documents: ...\n",
      "Query:\n",
      "Documents:\n",
      "Query:\n",
      "Query:\n",
      "Answer:\n",
      "State\n",
      "Environment\n",
      "Environment\n",
      "Action\n",
      "Action\n",
      "Action\n",
      "Action\n",
      "Action\n",
      "State\n",
      "Action\n",
      "Critic\n",
      "Process \n",
      "Reward\n",
      "Process \n",
      "Reward\n",
      "State\n",
      "Actor\n",
      "State\n",
      "Action\n",
      "Prompt Engineering\n",
      "Actor Tuning\n",
      "Critic Training\n",
      "What is the date of death\n",
      "of Alberto De Martino?\n",
      "Query:\n",
      "Action\n",
      "Question:\n",
      "State\n",
      "Action\n",
      "Supervised Fine-tuning\n",
      "State\n",
      "Action\n",
      "Direct Preference Optimization\n",
      "Action\n",
      "State\n",
      "Action\n",
      "Proximal Policy Optimization\n",
      "Process \n",
      "Reward\n",
      "Model\n",
      "Input Question: What is the date of death of the director of film Holocaust 2000?\n",
      "HotpotQA F1\n",
      "44.91%\n",
      "55.22%\n",
      "60.19%\n",
      "41.09%\n",
      "Re Search\n",
      "Direct Preference Optimization\n",
      "Critic\n",
      "Figure 1: Overview of the RAG-Gym framework. RAG-Gym employs a modular design, comprising\n",
      "prompt engineering, actor tuning, and critic training, to systematically optimize agentic RAG perfor-\n",
      "mance. By leveraging all three components, RAG-Gym improves the F1 score of the ReAct agent on\n",
      "HotpotQA from 41.09% to 60.19%.\n",
      "episode terminates. To maintain stable and reproducible state transitions, the configuration of the IR\n",
      "system (e.g., the number of returned documents) remains constant throughout.\n",
      "Rewards. For the high-level MDP, the immediate reward for a state-action pair (st, at) is defined as\n",
      "zero when at is a search query, and as the correctness of the predicted answer when at is an answer.\n",
      "Moreover, by formulating knowledge-intensive QA as a high-level MDP, we can directly assess the\n",
      "quality of intermediate actions, with process-level rewards derived from various sources (e.g., human\n",
      "annotations, LLM evaluations, or rollouts). This enables both the evaluation of intermediate actions\n",
      "and the fine-grained supervision of language agents through process-level feedback.\n",
      "2.2\n",
      "Systematic Optimization of Agentic Retrieval-augmented Generation\n",
      "With the high-level MDP formulation, RAG-Gym optimizes the agentic RAG system through three\n",
      "key components: (1) prompt engineering, which refines the language agent’s structure and operational\n",
      "design; (2) actor tuning, which adjusts the LLM parameters to improve decision-making; and (3)\n",
      "critic training, which develops an external verifier to assess the quality of generated macro-actions.\n",
      "2.2.1\n",
      "Prompt Engineering\n",
      "The first aspect of optimizing agentic RAG is crafting effective prompts that guide the language\n",
      "model in generating the appropriate actions. The system prompt defines the agent’s functional\n",
      "capabilities when processing a given state. RAG-Gym summarizes the essential functions into six\n",
      "distinct categories:\n",
      "• Answer generation: The agent produces a final answer to the question.\n",
      "• Question reasoning: The agent outlines reasoning steps before providing the answer.\n",
      "• Retrieval augmentation: The agent incorporates retrieved content to enhance its answer.\n",
      "• Query generation: The agent formulates queries to search for relevant documents.\n",
      "• Document summarization: The agent condenses retrieved content to extract key information.\n",
      "• Reasoning reflection: The agent reviews its reasoning to identify any unverified claims.\n",
      "3\n",
      "While the first five components have already been employed in existing agent architectures, the final\n",
      "component reasoning reflection is a novel addition by RAG-Gym. Inspired by recent advancements in\n",
      "reasoning models in which the models can reflect on their own reasoning process for self-correction\n",
      "[19], the newly introduced reasoning reflection directs the agent to scrutinize its reasoning process\n",
      "and identify claims that are unsupported by the information seeking history, thereby linking search\n",
      "query generation to answer reasoning to produce more precise and relevant queries.\n",
      "Combining reasoning reflection with other existing components, we propose a new agent architecture\n",
      "called Re2Search, which stands for Reasoning, Reflection, and Search. A Re2Search agent first\n",
      "reasons about all available information to construct an answer to the original question. It then\n",
      "reflects on its reasoning process to identify unverified claims that lack sufficient justification based\n",
      "on available evidence. These unverified claims form the basis for generating the next search query\n",
      "that is designed to retrieve the missing information required for constructing the answer. Table 1\n",
      "summarizes the presence or absence of these components in several existing agent architectures,\n",
      "including Direct, CoT [81], RAG [42], ReAct [91], Search-o1 [44], and our proposed Re2Search,\n",
      "each enabling different LLM capabilities through prompting.\n",
      "Table 1: A comparative overview of agent architectures based on their functional components.\n",
      "Component\n",
      "Direct\n",
      "CoT [81]\n",
      "RAG [42]\n",
      "ReAct [91]\n",
      "Search-o1 [44]\n",
      "Re2Search\n",
      "Answer Generation\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "Question Reasoning\n",
      "✗\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "Retrieval Augmentation\n",
      "✗\n",
      "✗\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "Query Generation\n",
      "✗\n",
      "✗\n",
      "✗\n",
      "✔\n",
      "✔\n",
      "✔\n",
      "Document Summarization\n",
      "✗\n",
      "✗\n",
      "✗\n",
      "✗\n",
      "✔\n",
      "✔\n",
      "Reasoning Reflection\n",
      "✗\n",
      "✗\n",
      "✗\n",
      "✗\n",
      "✗\n",
      "✔\n",
      "2.2.2\n",
      "Actor Tuning\n",
      "The second aspect of optimizing agentic RAG is tuning LLM parameters to directly enhance reasoning\n",
      "capability. Decomposing knowledge-intensive QA into intermediate steps, the high-level MDP in\n",
      "RAG-Gym enables the targeted optimization of language agents by focusing on the generated action\n",
      "at each step, reducing the task to standard text generation. This streamlines the training process and\n",
      "facilitates the application of various LLM post-training algorithms to enhance agent performance.\n",
      "Process Reward Data Collection. As discussed in our high-level MDP definition, the process reward\n",
      "for intermediate actions can be derived from multiple sources, including human annotations, LLM\n",
      "evaluations, or rollouts. In our implementation, we focus on collecting process reward data using\n",
      "advanced LLMs such as GPT-4o [1]. Specifically, we sample trajectories from an untuned agent and\n",
      "obtain process reward annotations from GPT-4o, while filtering out trajectories that do not result\n",
      "in a correct final answer using the outcome reward. This strategy enables us to efficiently gather\n",
      "high-quality process reward data, which is subsequently used to optimize the LLMs for agentic RAG.\n",
      "Further details on alternative process reward sources can be found in Section 4.1, with additional\n",
      "information about the data collection pipeline provided in Appendix E.\n",
      "Process-based Training Algorithms.\n",
      "Let D denote the process reward dataset, which consists of tuples (s, a+, a−), where s is a state, a+\n",
      "is a preferred (high-quality) action, and a−is a less-preferred (lower-quality) action. Each action is\n",
      "annotated based on the quality of the generated query or predicted answer. We assign the preference\n",
      "label to the entire token sequence produced when reasoning about the state, thereby reducing process-\n",
      "based actor tuning to a standard text generation problem. RAG-Gym implements and compares three\n",
      "widely used LLM post-training algorithms:\n",
      "• Supervised fine-tuning (SFT) [52]: This method uses high-quality intermediate actions to train\n",
      "language agents by maximizing the log-likelihood of preferred actions (a+) conditioned on their\n",
      "respective states s.\n",
      "• Direct preference optimization (DPO) [56]: This approach employs a contrastive learning\n",
      "framework that utilizes both preferred (a+) and unpreferred (a−) actions. The DPO objective\n",
      "encourages the agent to increase the likelihood of preferred actions while decreasing that of\n",
      "unpreferred actions.\n",
      "4\n",
      "• Proximal policy optimization (PPO) [60]: This is an online reinforcement learning algorithm for\n",
      "policy optimization. The collected data D is first used to train a process reward model rϕ(s, a).\n",
      "PPO then optimizes the agent to maximize the process reward of newly generated actions, while\n",
      "constraining policy updates to ensure stability.\n",
      "2.2.3\n",
      "Critic Training\n",
      "The third aspect of optimizing agentic RAG involves training a critic, denoted as rϕ, to act as an\n",
      "external evaluator of generated actions. The critic is designed to predict process rewards for a given\n",
      "state-action pair (s, a). Its training objective employs a contrastive loss that distinguishes preferred\n",
      "actions from less-preferred ones, following the preference modeling approach widely used in LLM\n",
      "alignment and reward modeling [47, 52]:\n",
      "Lcritic(ϕ) = −E(s,a+,a−)∼D\n",
      "h\n",
      "log σ\n",
      "\u0000rϕ(s, a+) −rϕ(s, a−)\n",
      "\u0001i\n",
      ",\n",
      "(1)\n",
      "where σ is the sigmoid function and D denotes the process reward dataset containing both preferred\n",
      "(a+) and less-preferred (a−) actions.\n",
      "While process reward modeling has been studied in the context of math reasoning [62, 46], its appli-\n",
      "cation to agentic RAG for knowledge-intensive question answering remains largely underexplored.\n",
      "In RAG-Gym, our process-level critic is tailored to evaluate intermediate actions such as search\n",
      "queries, rather than only final answers. This approach enables more fine-grained and actionable\n",
      "feedback, facilitating the optimization of agentic RAG systems through process-level supervision.\n",
      "Once trained, the critic provides targeted feedback on generated actions, guiding the language agent\n",
      "to make decisions that are more likely to lead to successful outcomes.\n",
      "3\n",
      "Main Results\n",
      "3.1\n",
      "Experimental Settings\n",
      "To assess the performance of various agents on knowledge-intensive QA tasks and evaluate the\n",
      "benefits of different optimization methods in RAG-Gym, we consider four datasets that are both\n",
      "knowledge- and reasoning-intensive, spanning general and medical domains. Specifically, we use\n",
      "HotpotQA [90], 2WikiMultihopQA [21], and Bamboogle [54], which are popular multi-hop QA\n",
      "datasets constructed from Wikipedia, as well as the MedQA dataset [34], which consists of medical\n",
      "exam questions that require specialized domain knowledge and complex reasoning. Following prior\n",
      "work [61], HotpotQA, 2WikiMultihopQA, and Bamboogle are evaluated using Exact Match (EM)\n",
      "and F1 scores, while the multi-choice MedQA dataset is assessed with accuracy (Acc). We also\n",
      "compute the average EM and F1 scores across different tasks, treating accuracy as equivalent to both\n",
      "metrics in the multi-choice evaluation setting. For actor and critic training in RAG-Gym, 1k questions\n",
      "were sampled from the HotpotQA and MedQA training sets for process reward data collection. To\n",
      "test the generalizability of the tuned agents, 2WikiMultihopQA and Bamboogle were evaluated using\n",
      "LLMs trained on HotpotQA. More implementation details can be found in Appendices C, E, H.\n",
      "3.2\n",
      "Performance Improvements by Prompt Engineering and Actor Tuning\n",
      "Table 2 presents a performance comparison of various agents and their tuned versions using different\n",
      "actor tuning algorithms in RAG-Gym. The results indicate that the Re2Search agent consistently\n",
      "outperforms other agents in both zero-shot and actor-tuned settings. Furthermore, when comparing\n",
      "Table 2 with Table 1, which details the functional components of each agent, it can be observed\n",
      "that more components generally leads to improved performance. This observation validates the\n",
      "effectiveness of the summarized functions in RAG-Gym, as well as the design of the Re2Search agent,\n",
      "which incorporates all identified components, including our newly proposed reasoning reflection.\n",
      "Additional case studies of our proposed Re2Search agent are provided in Appendices G.1 and G.2.\n",
      "By comparing different process supervision approaches for actor tuning, we observe that process\n",
      "supervision consistently enhances agent performance relative to the zero-shot learning (ZSL) baseline.\n",
      "This improvement underscores the critical role of process supervision in refining agentic RAG.\n",
      "Notably, for Direct, CoT, and RAG agents, where tuning focuses solely on answer generation, SFT\n",
      "slightly outperforms both DPO and PPO. In contrast, for ReAct, Search-o1, and Re2Search agents,\n",
      "5\n",
      "Table 2: Agent performance with Llama-3.1-8B backbone. Highest scores are bolded.\n",
      "Method\n",
      "Agent\n",
      "HotpotQA\n",
      "2Wiki\n",
      "Bamboogle\n",
      "MedQA\n",
      "Average\n",
      "EM\n",
      "F1\n",
      "EM\n",
      "F1\n",
      "EM\n",
      "F1\n",
      "Acc\n",
      "EM\n",
      "F1\n",
      "Zero-shot\n",
      "Learning\n",
      "Direct\n",
      "21.10\n",
      "27.93\n",
      "24.10\n",
      "27.68\n",
      "9.60\n",
      "14.89\n",
      "61.82\n",
      "29.16\n",
      "33.08\n",
      "CoT\n",
      "27.10\n",
      "35.17\n",
      "25.70\n",
      "30.08\n",
      "37.60\n",
      "49.50\n",
      "69.60\n",
      "40.00\n",
      "46.09\n",
      "RAG\n",
      "38.30\n",
      "48.57\n",
      "32.00\n",
      "36.91\n",
      "22.40\n",
      "33.73\n",
      "66.85\n",
      "39.89\n",
      "46.51\n",
      "ReAct\n",
      "30.70\n",
      "41.09\n",
      "28.90\n",
      "35.03\n",
      "32.00\n",
      "41.35\n",
      "62.37\n",
      "38.49\n",
      "44.96\n",
      "Search-o1\n",
      "35.30\n",
      "47.33\n",
      "34.00\n",
      "41.29\n",
      "44.80\n",
      "52.50\n",
      "66.14\n",
      "45.06\n",
      "51.82\n",
      "Re2Search\n",
      "34.00\n",
      "44.91\n",
      "41.50\n",
      "49.06\n",
      "44.80\n",
      "55.33\n",
      "70.31\n",
      "47.65\n",
      "54.90\n",
      "RAG-Gym\n",
      "Supervised\n",
      "Fine-tuning\n",
      "Direct\n",
      "22.80\n",
      "31.67\n",
      "28.00\n",
      "33.17\n",
      "20.00\n",
      "27.21\n",
      "63.63\n",
      "33.61\n",
      "38.92\n",
      "CoT\n",
      "26.50\n",
      "35.60\n",
      "27.30\n",
      "32.10\n",
      "42.40\n",
      "53.89\n",
      "69.68\n",
      "41.47\n",
      "47.82\n",
      "RAG\n",
      "41.50\n",
      "52.26\n",
      "38.00\n",
      "42.74\n",
      "28.80\n",
      "40.76\n",
      "67.79\n",
      "44.02\n",
      "50.89\n",
      "ReAct\n",
      "35.50\n",
      "46.06\n",
      "31.00\n",
      "36.79\n",
      "34.40\n",
      "44.17\n",
      "66.69\n",
      "41.90\n",
      "48.43\n",
      "Search-o1\n",
      "38.20\n",
      "50.02\n",
      "39.00\n",
      "45.91\n",
      "46.40\n",
      "57.18\n",
      "67.64\n",
      "47.81\n",
      "55.19\n",
      "Re2Search\n",
      "37.60\n",
      "49.16\n",
      "44.00\n",
      "50.54\n",
      "44.80\n",
      "56.78\n",
      "69.52\n",
      "48.98\n",
      "56.50\n",
      "RAG-Gym\n",
      "Direct\n",
      "Preference\n",
      "Optimization\n",
      "Direct\n",
      "20.80\n",
      "28.79\n",
      "25.20\n",
      "29.45\n",
      "12.00\n",
      "20.67\n",
      "62.37\n",
      "30.09\n",
      "35.32\n",
      "CoT\n",
      "26.30\n",
      "35.06\n",
      "28.20\n",
      "32.84\n",
      "40.80\n",
      "51.67\n",
      "71.33\n",
      "41.66\n",
      "47.73\n",
      "RAG\n",
      "38.00\n",
      "49.38\n",
      "37.60\n",
      "42.88\n",
      "28.80\n",
      "39.57\n",
      "67.79\n",
      "43.05\n",
      "49.91\n",
      "ReAct\n",
      "33.00\n",
      "43.96\n",
      "32.20\n",
      "39.24\n",
      "44.80\n",
      "54.35\n",
      "68.89\n",
      "44.72\n",
      "51.61\n",
      "Search-o1\n",
      "42.20\n",
      "54.34\n",
      "44.10\n",
      "52.66\n",
      "42.40\n",
      "55.59\n",
      "70.23\n",
      "49.73\n",
      "58.21\n",
      "Re2Search\n",
      "42.20\n",
      "55.22\n",
      "44.30\n",
      "51.36\n",
      "48.00\n",
      "56.57\n",
      "72.11\n",
      "51.65\n",
      "58.82\n",
      "RAG-Gym\n",
      "Proximal\n",
      "Policy\n",
      "Optimization\n",
      "Direct\n",
      "19.20\n",
      "26.17\n",
      "25.60\n",
      "28.84\n",
      "7.20\n",
      "12.17\n",
      "61.12\n",
      "28.28\n",
      "32.08\n",
      "CoT\n",
      "25.50\n",
      "33.68\n",
      "24.20\n",
      "29.02\n",
      "43.20\n",
      "52.54\n",
      "68.50\n",
      "40.35\n",
      "45.94\n",
      "RAG\n",
      "37.70\n",
      "47.60\n",
      "32.00\n",
      "36.29\n",
      "28.80\n",
      "40.24\n",
      "68.03\n",
      "41.63\n",
      "41.44\n",
      "ReAct\n",
      "35.80\n",
      "47.56\n",
      "33.20\n",
      "40.06\n",
      "36.80\n",
      "46.79\n",
      "67.32\n",
      "43.28\n",
      "50.43\n",
      "Search-o1\n",
      "38.30\n",
      "50.24\n",
      "32.60\n",
      "39.34\n",
      "50.40\n",
      "59.92\n",
      "70.15\n",
      "47.86\n",
      "54.91\n",
      "Re2Search\n",
      "38.40\n",
      "50.30\n",
      "41.40\n",
      "48.06\n",
      "49.60\n",
      "62.06\n",
      "71.72\n",
      "50.28\n",
      "58.04\n",
      "where the tuning process also involves generating high-quality queries, DPO and PPO surpass\n",
      "SFT, with DPO demonstrating a slight edge over PPO on most tasks. These findings highlight the\n",
      "importance of utilizing both positive and negative samples during training, especially for agents that\n",
      "require complex, multi-step reasoning with environmental feedback. Furthermore, the tuned agents\n",
      "tend to generate more search queries during inference, as elaborated in Appendix F.\n",
      "3.3\n",
      "Performance Improvements by Critic Training\n",
      "Figure 2 illustrates the performance improvements achieved through critic training. The label “With\n",
      "Critic” indicates that an external critic evaluates 10 sampled actions at each step to select the best one.\n",
      "In our experiments, all agents except for “Direct” consistently benefit from critic training. Moreover,\n",
      "these performance gains transfer to actors using different LLMs. As shown in the figure, not only does\n",
      "the original Llama-3.1-8B benefit from the trained critic, but both the DPO-tuned Llama-3.1-8B and\n",
      "GPT-4o-mini also experience significant improvements across all datasets using the same critic. This\n",
      "highlights the potential of employing trained critics as a plug-and-play module to enhance agentic\n",
      "RAG performance, particularly for proprietary LLMs where direct fine-tuning is not feasible. A case\n",
      "study of using trained critics during inference is provided in Appendix G.3.\n",
      "3.4\n",
      "Comparisons with Outcome Supervision Methods\n",
      "Combining the findings from previous sections, we introduce Re2Search++, an optimized agent that\n",
      "integrates the best choices from each optimization direction. Built on Re2Search and tuned with DPO\n",
      "while utilizing a trained critic for action selection, Re2Search++ is evaluated against recent methods\n",
      "such as Search-R1 [33] and R1-Searcher [69], which rely on outcome supervision via reinforcement\n",
      "learning (RL) with over 8k training questions. As these methods primarily focus on general-domain\n",
      "questions, we exclude MedQA from this evaluation for a fair comparison. Table 3 shows that\n",
      "Re2Search++ achieves performance comparable to that of the RL-tuned agents on the datasets used\n",
      "for their training (HotpotQA for Search-R1; HotpotQA and 2WikiMultihopQA for R1-Searcher),\n",
      "while significantly outperforming them on unseen datasets and achieving the best performance on\n",
      "average. This result underscores the overfitting issues of RL-based outcome supervision methods\n",
      "and highlights the robustness and generalizability of Re2Search++ through its fine-grained process\n",
      "supervision on intermediate steps.\n",
      "6\n",
      "Without Critic\n",
      "With Critic\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "F1 / Accuracy\n",
      "33.08\n",
      "32.45\n",
      "Direct\n",
      "Without Critic\n",
      "With Critic\n",
      "40\n",
      "60\n",
      "80\n",
      "46.09\n",
      "49.02\n",
      "CoT\n",
      "Without Critic\n",
      "With Critic\n",
      "40\n",
      "60\n",
      "80\n",
      "46.51\n",
      "55.64\n",
      "RAG\n",
      "Without Critic\n",
      "With Critic\n",
      "40\n",
      "60\n",
      "80\n",
      "44.96\n",
      "56.47\n",
      "ReAct\n",
      "Without Critic\n",
      "With Critic\n",
      "40\n",
      "60\n",
      "80\n",
      "F1 / Accuracy\n",
      "51.81\n",
      "61.04\n",
      "Search-o1\n",
      "Without Critic\n",
      "With Critic\n",
      "40\n",
      "60\n",
      "80\n",
      "54.73\n",
      "62.41\n",
      "Re²Search\n",
      "Without Critic\n",
      "With Critic\n",
      "40\n",
      "60\n",
      "80\n",
      "58.81\n",
      "64.12\n",
      "Re²Search (Llama-3.1-8B-DPO)\n",
      "Without Critic\n",
      "With Critic\n",
      "40\n",
      "60\n",
      "80\n",
      "61.06\n",
      "65.30\n",
      "Re²Search (GPT-4o-mini)\n",
      "Avg. Without Critic\n",
      "Avg. With Critic\n",
      "HotpotQA F1\n",
      "2WikiMultihopQA F1\n",
      "Bamboogle F1\n",
      "MedQA Accuracy\n",
      "Figure 2: Performance improvements across various agents with critics.\n",
      "Table 3: Comparison of Re2Search++ and other methods.\n",
      "Shading indicates in-domain model\n",
      "performance. CEM represents the “Cover Exact Match” used in [69].\n",
      "LLM\n",
      "Method\n",
      "HotpotQA\n",
      "2WikiMultihopQA\n",
      "Bamboogle\n",
      "Average\n",
      "EM\n",
      "CEM\n",
      "F1\n",
      "EM\n",
      "CEM\n",
      "F1\n",
      "EM\n",
      "CEM\n",
      "F1\n",
      "EM\n",
      "CEM\n",
      "F1\n",
      "Llama\n",
      "-3.1-8B\n",
      "ReAct\n",
      "30.70\n",
      "38.40\n",
      "41.09\n",
      "28.90\n",
      "38.00\n",
      "35.03\n",
      "32.00\n",
      "36.80\n",
      "41.35\n",
      "30.57\n",
      "37.73\n",
      "39.16\n",
      "Search-o1\n",
      "35.30\n",
      "43.80\n",
      "47.33\n",
      "34.00\n",
      "45.80\n",
      "41.29\n",
      "44.80\n",
      "48.80\n",
      "52.50\n",
      "38.03\n",
      "46.13\n",
      "47.04\n",
      "R1-Searcher\n",
      "44.90\n",
      "50.40\n",
      "56.88\n",
      "48.70\n",
      "51.30\n",
      "54.24\n",
      "38.40\n",
      "40.80\n",
      "53.21\n",
      "44.00\n",
      "47.50\n",
      "54.78\n",
      "Re2Search++\n",
      "46.50\n",
      "57.80\n",
      "60.19\n",
      "48.90\n",
      "60.50\n",
      "56.85\n",
      "55.20\n",
      "63.20\n",
      "66.37\n",
      "50.20\n",
      "60.50\n",
      "61.14\n",
      "Qwen\n",
      "-2.5-7B\n",
      "ReAct\n",
      "36.00\n",
      "40.10\n",
      "45.84\n",
      "38.60\n",
      "44.50\n",
      "45.02\n",
      "35.20\n",
      "38.40\n",
      "44.94\n",
      "36.60\n",
      "41.00\n",
      "45.27\n",
      "Search-o1\n",
      "40.70\n",
      "46.60\n",
      "52.15\n",
      "38.90\n",
      "46.20\n",
      "45.79\n",
      "40.80\n",
      "44.80\n",
      "52.91\n",
      "40.17\n",
      "45.87\n",
      "50.28\n",
      "Search-R1\n",
      "44.90\n",
      "49.40\n",
      "57.30\n",
      "43.90\n",
      "47.80\n",
      "50.07\n",
      "40.80\n",
      "41.60\n",
      "51.69\n",
      "43.20\n",
      "46.27\n",
      "53.02\n",
      "R1-Searcher\n",
      "46.80\n",
      "53.70\n",
      "59.61\n",
      "48.80\n",
      "55.00\n",
      "55.36\n",
      "44.80\n",
      "48.00\n",
      "54.01\n",
      "46.80\n",
      "52.23\n",
      "56.33\n",
      "Re2Search++\n",
      "44.40\n",
      "50.30\n",
      "56.47\n",
      "47.00\n",
      "56.50\n",
      "54.35\n",
      "52.94\n",
      "56.30\n",
      "63.51\n",
      "48.11\n",
      "54.37\n",
      "58.11\n",
      "4\n",
      "Analysis and Discussion\n",
      "4.1\n",
      "Comparison of Different Reward Sources\n",
      "As discussed in Section 2, the process reward can be collected from different sources. This section\n",
      "focuses on the evaluation of the effectiveness of these sources in guiding the agent’s action selection\n",
      "toward correct answers, as well as their alignment with human preferences, which are often considered\n",
      "to have the highest quality for process annotation [98]. Specifically, we compare the GPT-4o\n",
      "annotations with Llama-3.1-8B, as well as the rollout-based annotations using Math-Shepherd [77].\n",
      "We collect process annotations from human experts on MedQA to examine the alignment between\n",
      "the trained reward models and human preferences.\n",
      "Table 4: Comparison of various reward sources. ORM/PRM denotes the outcome/process reward\n",
      "model. Outcome sources are labeled for PRMs due to the trajectory filtering in RAG-Gym.\n",
      "Type\n",
      "Outcome\n",
      "Source\n",
      "Process\n",
      "Source\n",
      "HotpotQA\n",
      "(EM / F1)\n",
      "2Wiki\n",
      "(EM / F1)\n",
      "Bamboogle\n",
      "(EM / F1)\n",
      "MedQA\n",
      "(Acc / Agree)\n",
      "ORM\n",
      "Truth\n",
      "–\n",
      "41.10 / 53.35\n",
      "47.70 / 55.59\n",
      "43.20 / 57.46\n",
      "66.77 /\n",
      "–\n",
      "PRM (Random)\n",
      "–\n",
      "–\n",
      "32.20 / 42.83\n",
      "35.70 / 42.00\n",
      "38.40 / 47.86\n",
      "68.26 / 50.00\n",
      "PRM (Rollout)\n",
      "Truth\n",
      "Rollout\n",
      "39.60 / 51.85\n",
      "42.94 / 49.57\n",
      "48.80 / 56.05\n",
      "68.34 / 71.03\n",
      "PRM (Llama)\n",
      "Truth\n",
      "Llama-3.1-8B\n",
      "40.30 / 51.74\n",
      "40.70 / 48.22\n",
      "44.80 / 54.36\n",
      "68.50 / 65.99\n",
      "PRM (GPT)\n",
      "Truth\n",
      "GPT-4o\n",
      "44.10 / 56.84\n",
      "50.20 / 57.94\n",
      "51.20 / 63.15\n",
      "71.96 / 85.85\n",
      "7\n",
      "The results are shown in Table 4. The reward model trained with GPT-4o annotations delivers the\n",
      "highest performance across all datasets, effectively providing accurate, fine-grained process rewards\n",
      "for agent optimization. Moreover, it exhibits the best alignment with human preferences, achieving\n",
      "an agreement rate of 85.85% with human annotators. In contrast, although rollouts and Llama-3.1-8B\n",
      "annotations improve action selection relative to a process reward model with random selections, they\n",
      "are generally less effective than GPT-4o annotations and sometimes even bring inferior outcomes on\n",
      "general-domain questions. This result underscores the limitations of current rollout-based methods,\n",
      "originally designed for math reasoning, in the context of complex reasoning and search tasks, and\n",
      "highlights the need for tailored approaches in agentic RAG.\n",
      "4.2\n",
      "Training Time Scaling\n",
      "For the evaluation of training sample size and its impacts on the performance of Re2Search agents,\n",
      "we conducted experiments using critics trained on varying numbers of instances, ranging from 250 to\n",
      "1000 questions. The results, presented in Figure 3, show how the agent’s performance scales with\n",
      "the availability of more training data across four datasets. In general, the performance of Re2Search\n",
      "improves with an increasing number of training samples, but the gains tend to converge as the sample\n",
      "size grows. Notably, there is a sharp improvement in F1 scores on HotpotQA, 2WikiMultihopQA,\n",
      "and Bamboogle when comparing the ZSL baseline to process reward models trained on 250 samples,\n",
      "showing that even a small amount of process reward data can yield significant performance gains.\n",
      "However, the improvements become less pronounced on HotpotQA and 2WikiMultihopQA when\n",
      "increasing the training samples from 500 to 1000, indicating diminishing returns as the model\n",
      "approaches a saturation point in its learning from additional data.\n",
      "250\n",
      "500\n",
      "750\n",
      "1000\n",
      "#Sample\n",
      "43\n",
      "50\n",
      "57\n",
      "F1 (%)\n",
      "ZSL\n",
      "HotpotQA\n",
      "250\n",
      "500\n",
      "750\n",
      "1000\n",
      "#Sample\n",
      "47\n",
      "53\n",
      "59\n",
      "F1 (%)\n",
      "ZSL\n",
      "2WikiMultihopQA\n",
      "250\n",
      "500\n",
      "750\n",
      "1000\n",
      "#Sample\n",
      "57\n",
      "61\n",
      "65\n",
      "F1 (%)\n",
      "ZSL\n",
      "Bamboogle\n",
      "250\n",
      "500\n",
      "750\n",
      "1000\n",
      "#Sample\n",
      "68\n",
      "70\n",
      "72\n",
      "Acc (%)\n",
      "ZSL\n",
      "MedQA\n",
      "Figure 3: Performance of Re2Search agents with critics trained on different numbers of samples.\n",
      "For MedQA, which involves complex reasoning and information-seeking tasks requiring domain-\n",
      "specific knowledge, a different trend is observed. With only 250 training samples, the performance\n",
      "slightly drops below the ZSL baseline, highlighting the challenges of capturing intricate domain-\n",
      "specific processes with limited training data. As the sample size increases, however, the performance\n",
      "gradually recovers and eventually surpasses the ZSL baseline, achieving the highest accuracy of\n",
      "71.72% with 1000 samples. This underscores the importance of sufficient training data in capturing\n",
      "the nuanced reasoning and query-generation processes required for specialized tasks.\n",
      "4.3\n",
      "Inference Time Scaling\n",
      "Since trained critics optimize action-taking by identifying high-quality actions from the generated\n",
      "candidates during inference, we explored how the agent performance changes with the increasing\n",
      "number of sampled actions at each time step. Figure 4 displays the results of our inference time\n",
      "scaling study, with Re2Search as the tested agent. We observe a consistent trend across multiple\n",
      "benchmarks, where increasing the number of sampled actions generally improves performance.\n",
      "Specifically, for HotpotQA and Bamboogle, the F1 score continues to rise as more actions are\n",
      "sampled, demonstrating the benefits of expanding the candidate set to enable better action selection at\n",
      "each step. However, performance gains gradually diminish, indicating that the agent reaches a point\n",
      "where additional sampled actions contribute less to improvement. This suggests that while action\n",
      "sampling is beneficial, there is a limit to how much additional sampling enhances decision-making.\n",
      "8\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "#Action\n",
      "43\n",
      "53\n",
      "63\n",
      "F1 (%)\n",
      "ZSL\n",
      "HotpotQA\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "#Action\n",
      "47\n",
      "53\n",
      "59\n",
      "F1 (%)\n",
      "ZSL\n",
      "2WikiMultihopQA\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "#Action\n",
      "56\n",
      "61\n",
      "66\n",
      "F1 (%)\n",
      "ZSL\n",
      "Bamboogle\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "#Action\n",
      "70\n",
      "72\n",
      "74\n",
      "Acc (%)\n",
      "ZSL\n",
      "MedQA\n",
      "Figure 4: Performance of Re2Search agents with different numbers of actions sampled per step.\n",
      "5\n",
      "Related Work\n",
      "5.1\n",
      "Retrieval-Augmented Generation\n",
      "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing large\n",
      "language models (LLMs) on knowledge-intensive tasks. A typical RAG framework comprises two\n",
      "core components: a retriever, which selects relevant documents from a large corpus, and a generator,\n",
      "which synthesizes information from these documents to produce coherent and contextually appropriate\n",
      "responses [42, 18, 9]. RAG has demonstrated strong performance across diverse domains, including\n",
      "open-domain question answering [37, 25, 7, 88, 63], fact-checking [78, 66], and summarization\n",
      "[3]. Subsequent research has focused on improving both the retriever’s ability to select relevant\n",
      "documents [95, 51, 89, 30, 32] and the generator’s capacity to effectively utilize retrieved information\n",
      "[15, 82, 80], thereby boosting overall system performance [26, 96, 36, 94]. Nevertheless, most RAG\n",
      "pipelines still rely on a single retrieval step, which can be inadequate for complex queries that require\n",
      "synthesizing information from multiple sources.\n",
      "5.2\n",
      "Multi-hop Question Answering\n",
      "Multi-hop question answering (QA) tasks require systems to synthesize information from multiple,\n",
      "diverse sources to produce accurate answers [90, 21]. These tasks highlight the limitations of\n",
      "conventional RAG architectures, where a single retrieval step often fails to capture the comprehensive\n",
      "context needed for complex queries. To address this, language agents such as ReAct [91, 73, 4, 31, 54]\n",
      "have been proposed, interleaving reasoning and retrieval to dynamically accumulate relevant evidence\n",
      "[57, 79, 38, 61], which has shown promise in improving LLM performance [76, 64, 92, 27, 29].\n",
      "However, most of these methods still rely heavily on prompt engineering, which can be fragile and\n",
      "may not effectively optimize language agents for knowledge-intensive tasks [40, 70, 2]. Recent\n",
      "studies have explored reinforcement learning (RL) to optimize language agents for multi-hop QA\n",
      "[69, 33, 8, 17, 55], but these approaches risk generating suboptimal intermediate search actions and\n",
      "show limited generalization to unseen data, as demonstrated in our experiments. Other concurrent\n",
      "work investigates process-level supervision [22, 12, 45, 71], but typically focuses on specific agent\n",
      "architectures and a narrow set of supervision methods, offering limited insight into the systematic\n",
      "optimization of language agents.\n",
      "5.3\n",
      "Post-training of Large Language Models\n",
      "Beyond the foundational knowledge acquired during pre-training, post-training methods are essential\n",
      "for refining Large Language Models (LLMs) and aligning them with specific downstream tasks and\n",
      "desired behaviors. Supervised Fine-Tuning (SFT) adapts models using curated instruction-response\n",
      "pairs to promote task-specific capabilities [52, 10]. While SFT enhances instruction-following,\n",
      "further alignment with nuanced human preferences is often achieved through Reinforcement Learning\n",
      "from Human Feedback (RLHF) [52, 6, 5], typically implemented via Proximal Policy Optimization\n",
      "(PPO) [60]. More recently, critic-free approaches such as Direct Preference Optimization (DPO)\n",
      "have emerged as streamlined alternatives [56, 50, 14, 87], directly optimizing the LLM policy based\n",
      "on reward annotations and bypassing the need for a separately trained reward model. Although\n",
      "these techniques bring strong gains on text generation benchmarks, their integration into agentic\n",
      "RAG pipelines, where models must dynamically interact with retrieval systems and adapt reasoning\n",
      "strategies to evolving contexts, remains underexplored.\n",
      "9\n",
      "6\n",
      "Conclusion\n",
      "This work presents RAG-Gym as a unified and extensible framework for systematically optimizing\n",
      "agentic RAG along the axes of prompt engineering, actor tuning, and critic training. Through\n",
      "extensive empirical analysis, we demonstrate that integrating reasoning reflection, process-level direct\n",
      "preference optimization, and critic-guided inference yields substantial improvements over existing\n",
      "approaches. We hope RAG-Gym will serve as a foundation for further advances in robust, adaptive,\n",
      "and interpretable retrieval-augmented language agents.\n",
      "References\n",
      "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\n",
      "Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\n",
      "technical report. arXiv preprint arXiv:2303.08774, 2023.\n",
      "[2] Victor Akinwande, Yiding Jiang, Dylan Sam, and J Zico Kolter.\n",
      "Understanding prompt\n",
      "engineering may not require rethinking generalization. arXiv preprint arXiv:2310.03957, 2023.\n",
      "[3] Chenxin An, Ming Zhong, Zhichao Geng, Jianqiang Yang, and Xipeng Qiu. Retrievalsum: A\n",
      "retrieval enhanced framework for abstractive summarization. arXiv preprint arXiv:2109.07943,\n",
      "2021.\n",
      "[4] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learn-\n",
      "ing to retrieve, generate, and critique through self-reflection. In The Twelfth International\n",
      "Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. Open-\n",
      "Review.net, 2024. URL https://openreview.net/forum?id=hSyW5go0v8.\n",
      "[5] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy\n",
      "Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a\n",
      "laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.\n",
      "[6] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\n",
      "Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless\n",
      "assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,\n",
      "2022.\n",
      "[7] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie\n",
      "Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,\n",
      "et al. Improving language models by retrieving from trillions of tokens. In International\n",
      "conference on machine learning, pages 2206–2240. PMLR, 2022.\n",
      "[8] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z\n",
      "Pan, Wen Zhang, Huajun Chen, Fan Yang, et al. Research: Learning to reason with search for\n",
      "llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025.\n",
      "[9] Mingyue Cheng, Yucong Luo, Jie Ouyang, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang,\n",
      "Jiawei Cao, Jie Ma, et al. A survey on knowledge-oriented retrieval-augmented generation.\n",
      "arXiv preprint arXiv:2503.10677, 2025.\n",
      "[10] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\n",
      "Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\n",
      "language models. Journal of Machine Learning Research, 25(70):1–53, 2024.\n",
      "[11] Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher. Reciprocal rank fusion outper-\n",
      "forms condorcet and individual rank learning methods. In Proceedings of the 32nd international\n",
      "ACM SIGIR conference on Research and development in information retrieval, pages 758–759,\n",
      "2009.\n",
      "[12] Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, and Ji-Rong Wen.\n",
      "Progressive multimodal reasoning via active retrieval. arXiv preprint arXiv:2412.14835, 2024.\n",
      "10\n",
      "[13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,\n",
      "Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd\n",
      "of models. arXiv preprint arXiv:2407.21783, 2024.\n",
      "[14] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto:\n",
      "Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024.\n",
      "[15] Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. Enhancing\n",
      "noise robustness of retrieval-augmented language models with adaptive adversarial training.\n",
      "arXiv preprint arXiv:2405.20978, 2024.\n",
      "[16] Jiayi Fu, Xuandong Zhao, Chengyuan Yao, Heng Wang, Qi Han, and Yanghua Xiao. Reward\n",
      "shaping to mitigate reward hacking in rlhf. arXiv preprint arXiv:2502.18770, 2025.\n",
      "[17] Jingsheng Gao, Linxu Li, Weiyuan Li, Yuzhuo Fu, and Bin Dai. Smartrag: Jointly learn\n",
      "rag-related tasks from the environment feedback. arXiv preprint arXiv:2410.18141, 2024.\n",
      "[18] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,\n",
      "and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv\n",
      "preprint arXiv:2312.10997, 2023.\n",
      "[19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\n",
      "Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in\n",
      "llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.\n",
      "[20] Binglan Han, Teo Susnjak, and Anuradha Mathrani. Automating systematic literature reviews\n",
      "with retrieval-augmented generation: A comprehensive overview. Applied Sciences, 14(19):\n",
      "9103, 2024.\n",
      "[21] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a\n",
      "multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the\n",
      "28th International Conference on Computational Linguistics, pages 6609–6625, 2020.\n",
      "[22] Sheryl Hsu, Omar Khattab, Chelsea Finn, and Archit Sharma. Grounding by trying: Llms with\n",
      "reinforcement learning-enhanced retrieval. arXiv preprint arXiv:2410.23214, 2024.\n",
      "[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\n",
      "Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\n",
      "preprint arXiv:2106.09685, 2021.\n",
      "[24] Jian Hu, Xibin Wu, Zilin Zhu, Weixun Wang, Dehao Zhang, Yu Cao, et al. Openrlhf: An\n",
      "easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143,\n",
      "2024.\n",
      "[25] Gautier Izacard and Édouard Grave. Leveraging passage retrieval with generative models for\n",
      "open domain question answering. In Proceedings of the 16th Conference of the European\n",
      "Chapter of the Association for Computational Linguistics: Main Volume, pages 874–880, 2021.\n",
      "[26] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\n",
      "Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning\n",
      "with retrieval augmented language models. Journal of Machine Learning Research, 24(251):\n",
      "1–43, 2023.\n",
      "[27] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong-Cheol Park. Adaptive-\n",
      "rag: Learning to adapt retrieval-augmented large language models through question complexity.\n",
      "In 2024 Conference of the North American Chapter of the Association for Computational\n",
      "Linguistics: Human Language Technologies, pages 7036–7050. Association for Computational\n",
      "Linguistics, 2024.\n",
      "[28] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\n",
      "Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation.\n",
      "ACM Computing Surveys, 55(12):1–38, 2023.\n",
      "11\n",
      "[29] Pengcheng Jiang, Lang Cao, Ruike Zhu, Minhao Jiang, Yunyi Zhang, Jimeng Sun, and Jiawei\n",
      "Han. Ras: Retrieval-and-structuring for knowledge-intensive llm generation. arXiv preprint\n",
      "arXiv:2502.10996, 2025.\n",
      "[30] Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng\n",
      "Sun, and Jiawei Han. Deepretrieval: Hacking real search engines and retrievers with large\n",
      "language models via reinforcement learning. arXiv preprint arXiv:2503.00223, 2025.\n",
      "[31] Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang,\n",
      "Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Proceedings of\n",
      "the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969–7992,\n",
      "2023.\n",
      "[32] Ziyan Jiang, Xueguang Ma, and Wenhu Chen. Longrag: Enhancing retrieval-augmented\n",
      "generation with long-context llms. arXiv preprint arXiv:2406.15319, 2024.\n",
      "[33] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Za-\n",
      "mani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with\n",
      "reinforcement learning. arXiv preprint arXiv:2503.09516, 2025.\n",
      "[34] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What\n",
      "disease does this patient have? a large-scale open domain question answering dataset from\n",
      "medical exams. Applied Sciences, 11(14):6421, 2021.\n",
      "[35] Qiao Jin, Won Kim, Qingyu Chen, Donald C Comeau, Lana Yeganova, W John Wilbur, and\n",
      "Zhiyong Lu. Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs\n",
      "for zero-shot biomedical information retrieval. Bioinformatics, 39(11):btad651, 2023.\n",
      "[36] Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, and Jun\n",
      "Zhao. Rag-rewardbench: Benchmarking reward models in retrieval augmented generation for\n",
      "preference alignment. arXiv preprint arXiv:2412.13746, 2024.\n",
      "[37] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,\n",
      "Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.\n",
      "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n",
      "(EMNLP), pages 6769–6781, 2020.\n",
      "[38] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and\n",
      "Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks.\n",
      "In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,\n",
      "Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?\n",
      "id=_nGgzQjzaRy.\n",
      "[39] Jakub Lála, Odhran O’Donoghue, Aleksandar Shtedritski, Sam Cox, Samuel G Rodriques, and\n",
      "Andrew D White. Paperqa: Retrieval-augmented generative agent for scientific research. arXiv\n",
      "preprint arXiv:2312.07559, 2023.\n",
      "[40] Divya Lamba. The role of prompt engineering in improving language understanding and\n",
      "generation. International Journal For Multidisciplinary Research, 2024. URL https://api.\n",
      "semanticscholar.org/CorpusID:274939741.\n",
      "[41] Guido Lang and Tan Gürpinar. Ai-powered learning support: A study of retrieval-augmented\n",
      "generation (rag) chatbot effectiveness in an online course. Information Systems Education\n",
      "Journal, 23(2), 2025.\n",
      "[42] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\n",
      "Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented\n",
      "generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing\n",
      "Systems, 33:9459–9474, 2020.\n",
      "[43] Dongheng Li, Yongchang Hao, and Lili Mou. Llmr: Knowledge distillation with a large\n",
      "language model-induced reward. In Proceedings of the 2024 Joint International Conference on\n",
      "Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages\n",
      "10657–10664, 2024.\n",
      "12\n",
      "[44] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and\n",
      "Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint\n",
      "arXiv:2501.05366, 2025.\n",
      "[45] Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, and Lidong Bing. Can we\n",
      "further elicit reasoning in llms? critic-guided planning with retrieval-augmentation for solving\n",
      "challenging tasks. arXiv preprint arXiv:2410.01428, 2024.\n",
      "[46] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy\n",
      "Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step.\n",
      "In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna,\n",
      "Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?\n",
      "id=v8L0pN6EOi.\n",
      "[47] Fei Liu et al. Learning to summarize from human feedback. In Proceedings of the 58th Annual\n",
      "Meeting of the Association for Computational Linguistics, pages 583–592, 2020.\n",
      "[48] Siru Liu, Allison B McCoy, and Adam Wright. Improving large language model applications\n",
      "in biomedicine with retrieval-augmented generation: a systematic review, meta-analysis, and\n",
      "clinical development guidelines. Journal of the American Medical Informatics Association,\n",
      "page ocaf008, 2025.\n",
      "[49] Hao Ma, Tianyi Hu, Zhiqiang Pu, Liu Boyin, Xiaolin Ai, Yanyan Liang, and Min Chen. Coevolv-\n",
      "ing with the other you: Fine-tuning llm with sequential cooperative multi-agent reinforcement\n",
      "learning. Advances in Neural Information Processing Systems, 37:15497–15525, 2024.\n",
      "[50] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a\n",
      "reference-free reward. Advances in Neural Information Processing Systems, 37:124198–124235,\n",
      "2024.\n",
      "[51] Thang Nguyen, Peter Chin, and Yu-Wing Tai. Reward-rag: Enhancing rag with reward driven\n",
      "supervision. arXiv preprint arXiv:2410.03780, 2024.\n",
      "[52] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\n",
      "Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\n",
      "follow instructions with human feedback. Advances in neural information processing systems,\n",
      "35:27730–27744, 2022.\n",
      "[53] Nicholas Pipitone and Ghita Houir Alami.\n",
      "Legalbench-rag: A benchmark for retrieval-\n",
      "augmented generation in the legal domain. arXiv preprint arXiv:2408.10343, 2024.\n",
      "[54] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.\n",
      "Measuring and narrowing the compositionality gap in language models. In Findings of the\n",
      "Association for Computational Linguistics: EMNLP 2023, pages 5687–5711, 2023.\n",
      "[55] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan\n",
      "Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958,\n",
      "2025.\n",
      "[56] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\n",
      "Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\n",
      "Advances in Neural Information Processing Systems, 36:53728–53741, 2023.\n",
      "[57] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown,\n",
      "and Yoav Shoham. In-context retrieval-augmented language models. Transactions of the\n",
      "Association for Computational Linguistics, 11:1316–1331, 2023.\n",
      "[58] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and\n",
      "beyond. Foundations and Trends® in Information Retrieval, 3(4):333–389, 2009.\n",
      "[59] Satya S Sahoo, Joseph M Plasek, Hua Xu, Özlem Uzuner, Trevor Cohen, Meliha Yetisgen,\n",
      "Hongfang Liu, Stéphane Meystre, and Yanshan Wang. Large language models for biomedicine:\n",
      "foundations, opportunities, challenges, and best practices. Journal of the American Medical\n",
      "Informatics Association, page ocae074, 2024.\n",
      "13\n",
      "[60] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\n",
      "policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n",
      "[61] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen.\n",
      "Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.\n",
      "In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248–9274,\n",
      "2023.\n",
      "[62] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\n",
      "Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical\n",
      "reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.\n",
      "[63] Yucheng Shi, Tianze Yang, Canyu Chen, Quanzheng Li, Tianming Liu, Xiang Li, and Ninghao\n",
      "Liu. Searchrag: Can search engines be helpful for llm-based medical question answering?\n",
      "arXiv preprint arXiv:2502.13233, 2025.\n",
      "[64] Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie Ren, Zhumin Chen, and Zhaochun\n",
      "Ren. Generate-then-ground in retrieval-augmented generation for multi-hop question answering.\n",
      "In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers), pages 7339–7353, 2024.\n",
      "[65] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.\n",
      "Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information\n",
      "Processing Systems, 36, 2024.\n",
      "[66] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmenta-\n",
      "tion reduces hallucination in conversation. In Findings of the Association for Computational\n",
      "Linguistics: EMNLP 2021, pages 3784–3803, 2021.\n",
      "[67] Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger.\n",
      "Defining and\n",
      "characterizing reward gaming. Advances in Neural Information Processing Systems, 35:9460–\n",
      "9471, 2022.\n",
      "[68] Michael D Skarlinski, Sam Cox, Jon M Laurent, James D Braza, Michaela Hinks, Michael J\n",
      "Hammerling, Manvitha Ponnapati, Samuel G Rodriques, and Andrew D White. Language\n",
      "agents achieve superhuman synthesis of scientific knowledge. arXiv preprint arXiv:2409.13740,\n",
      "2024.\n",
      "[69] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang,\n",
      "and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement\n",
      "learning. arXiv preprint arXiv:2503.05592, 2025.\n",
      "[70] Hari Subramonyam, Divy Thakkar, Andrew Ku, Juergen Dieber, and Anoop K Sinha. Prototyp-\n",
      "ing with prompts: Emerging approaches and challenges in generative ai design for collaborative\n",
      "software teams. In Proceedings of the 2025 CHI Conference on Human Factors in Computing\n",
      "Systems, pages 1–22, 2025.\n",
      "[71] Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang,\n",
      "Song Yang, and Han Li. Rearter: Retrieval-augmented reasoning with trustworthy process\n",
      "rewarding. arXiv preprint arXiv:2501.07861, 2025.\n",
      "[72] Jakub Swacha and Michał Gracel. Retrieval-augmented generation (rag) chatbots for education:\n",
      "A survey of applications. Applied Sciences, 15(8):4234, 2025.\n",
      "[73] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving\n",
      "retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In\n",
      "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers), pages 10014–10037, 2023.\n",
      "[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
      "Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\n",
      "processing systems, 30, 2017.\n",
      "14\n",
      "[75] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan\n",
      "Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforce-\n",
      "ment learning. https://github.com/huggingface/trl, 2020.\n",
      "[76] Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge\n",
      "Rong, and Zhang Xiong. Knowledge-driven cot: Exploring faithful reasoning in llms for\n",
      "knowledge-intensive question answering. arXiv preprint arXiv:2308.13259, 2023.\n",
      "[77] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and\n",
      "Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations.\n",
      "In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\n",
      "(Volume 1: Long Papers), pages 9426–9439, 2024.\n",
      "[78] Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi Georgiev, Rocktim\n",
      "Das, and Preslav Nakov. Factuality of large language models: A survey. In Proceedings of the\n",
      "2024 Conference on Empirical Methods in Natural Language Processing, pages 19519–19529,\n",
      "2024.\n",
      "[79] Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. Rat: Retrieval\n",
      "augmented thoughts elicit context-aware reasoning in long-horizon generation. arXiv preprint\n",
      "arXiv:2403.05313, 2024.\n",
      "[80] Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot,\n",
      "Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, et al. Speculative rag: Enhancing\n",
      "retrieval augmented generation through drafting. arXiv preprint arXiv:2407.08223, 2024.\n",
      "[81] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\n",
      "Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\n",
      "Advances in neural information processing systems, 35:24824–24837, 2022.\n",
      "[82] Zhepei Wei, Wei-Lin Chen, and Yu Meng. InstructRAG: Instructing retrieval-augmented\n",
      "generation via self-synthesized rationales. In The Thirteenth International Conference on\n",
      "Learning Representations, 2025. URL https://openreview.net/forum?id=P1qhkp8gQT.\n",
      "[83] Nirmalie Wiratunga, Ramitha Abeyratne, Lasal Jayawardena, Kyle Martin, Stewart Massie,\n",
      "Ikechukwu Nkisi-Orji, Ruvan Weerasinghe, Anne Liret, and Bruno Fleisch. Cbr-rag: case-\n",
      "based reasoning for retrieval augmented generation in llms for legal question answering. In\n",
      "International Conference on Case-Based Reasoning, pages 445–460. Springer, 2024.\n",
      "[84] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources\n",
      "to advance general chinese embedding, 2023.\n",
      "[85] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented\n",
      "generation for medicine. In Findings of the Association for Computational Linguistics ACL\n",
      "2024, pages 6233–6251, 2024.\n",
      "[86] Guangzhi Xiong, Qiao Jin, Xiao Wang, Minjia Zhang, Zhiyong Lu, and Aidong Zhang. Im-\n",
      "proving retrieval-augmented generation in medicine with iterative follow-up questions. In\n",
      "Biocomputing 2025: Proceedings of the Pacific Symposium, pages 199–214. World Scientific,\n",
      "2024.\n",
      "[87] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Ken-\n",
      "ton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries\n",
      "of llm performance in machine translation. In International Conference on Machine Learning,\n",
      "pages 55204–55224. PMLR, 2024.\n",
      "[88] Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang\n",
      "Li, Joyce C Ho, Carl Yang, et al. Simrag: Self-improving retrieval-augmented generation for\n",
      "adapting large language models to specialized domains. arXiv preprint arXiv:2410.17952,\n",
      "2024.\n",
      "15\n",
      "[89] Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May Dongmei Wang, Joyce C.\n",
      "Ho, Chao Zhang, and Carl Yang. BMRetriever: Tuning large language models as better\n",
      "biomedical text retrievers. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors,\n",
      "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing,\n",
      "pages 22234–22254, Miami, Florida, USA, November 2024. Association for Computational\n",
      "Linguistics. doi: 10.18653/v1/2024.emnlp-main.1241. URL https://aclanthology.org/\n",
      "2024.emnlp-main.1241/.\n",
      "[90] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,\n",
      "and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\n",
      "answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii, editors,\n",
      "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\n",
      "pages 2369–2380, Brussels, Belgium, October-November 2018. Association for Computational\n",
      "Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.\n",
      "[91] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan\n",
      "Cao. React: Synergizing reasoning and acting in language models. In International Conference\n",
      "on Learning Representations (ICLR), 2023.\n",
      "[92] Tian Yu, Shaolei Zhang, and Yang Feng. Auto-rag: Autonomous retrieval-augmented generation\n",
      "for large language models. 2024. URL https://arxiv.org/abs/2411.19443.\n",
      "[93] Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boullé,\n",
      "and Ievgen Redko. Large language models as markov chains. arXiv preprint arXiv:2410.02724,\n",
      "2024.\n",
      "[94] Hanning Zhang, Juntong Song, Juno Zhu, Yuanhao Wu, Tong Zhang, and Cheng Niu. Rag-\n",
      "reward: Optimizing rag with reward modeling and rlhf. arXiv preprint arXiv:2501.13264,\n",
      "2025.\n",
      "[95] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. Retrieve anything to\n",
      "augment large language models. arXiv preprint arXiv:2310.07554, 2023.\n",
      "[96] Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and\n",
      "Joseph E Gonzalez. Raft: Adapting language model to domain specific rag. arXiv preprint\n",
      "arXiv:2403.10131, 2024.\n",
      "[97] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo\n",
      "Zhao, Yu Zhang, Yulong Chen, et al. Siren’s song in the ai ocean: a survey on hallucination in\n",
      "large language models. arXiv preprint arXiv:2309.01219, 2023.\n",
      "[98] Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng\n",
      "Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in\n",
      "mathematical reasoning. arXiv preprint arXiv:2501.07301, 2025.\n",
      "16\n",
      "A\n",
      "Limitations and Future Work\n",
      "Despite the strengths of RAG-Gym, several limitations remain. First, our framework relies on\n",
      "high-quality process reward judgments to supervise intermediate agent actions. Obtaining such\n",
      "fine-grained annotations for complex reasoning or domain-specific scenarios can be challenging.\n",
      "Second, as with other reward modeling approaches, there is an inherent risk of reward hacking: agents\n",
      "may learn to exploit imperfections or biases in the reward model, optimizing for the reward signal\n",
      "rather than genuine task performance [67, 16]. Third, while our experiments focus on knowledge-\n",
      "intensive question answering, the generalizability of RAG-Gym to other task types (e.g., dialogue,\n",
      "summarization, or planning) remains to be systematically evaluated.\n",
      "While RAG-Gym serves as a pilot study of when and how process supervision works for agentic\n",
      "RAG, several promising directions remain for future work. First, developing more scalable and\n",
      "cost-effective annotation strategies for process reward modeling is essential, especially for complex or\n",
      "specialized domains. Since existing rollout-based methods such as Math-Shepherd [77] did not yield\n",
      "significant gains in our experiments (Table 4), new approaches are needed to facilitate efficient and\n",
      "high-quality process reward collection. Second, the design and training of process reward judges can\n",
      "be further refined to improve robustness and reduce susceptibility to reward hacking. Third, extending\n",
      "RAG-Gym to a broader range of agentic tasks beyond knowledge-intensive question answering such\n",
      "as dialogue will help assess its generalizability and reveal new challenges, particularly in settings\n",
      "where outcome rewards are ambiguous and process supervision is even more critical.\n",
      "B\n",
      "Broader Impacts\n",
      "RAG-Gym systematically evalutes different optimization approaches for retrieval-augmented lan-\n",
      "guage agents, which has the potential for wide-ranging societal benefits and risks. By enabling\n",
      "high-quality intermediate steps with process-level supervsion, our framework can improve the relia-\n",
      "bility of AI assistants in knowledge-intensive domains such as education [72, 41], healthcare [48, 86],\n",
      "scientific research [20, 39, 68], and legal analysis [83, 53]. Also, process-level actor tuning and\n",
      "critic-guided inference may help reduce hallucinations and increase transparency, supporting more\n",
      "trustworthy AI deployments.\n",
      "However, these advances also raise important considerations. The reliance on high-quality process\n",
      "reward annotations may introduce biases if the annotation sources are not representative or contain\n",
      "systematic errors. Reward hacking remains a risk, as agents may learn to exploit weaknesses in the\n",
      "reward model, potentially leading to unintended behaviors or misinformation.\n",
      "C\n",
      "Dataset Descriptions\n",
      "In this section, we provide detailed descriptions of the datasets used in our experiments, including\n",
      "HotpotQA [90], 2WikiMultihopQA [21], Bamboogle [54], and MedQA [34].\n",
      "HotpotQA.\n",
      "HotpotQA is a large-scale, multi-hop question-answering dataset that requires rea-\n",
      "soning across multiple documents. It consists of questions that explicitly demand retrieving and\n",
      "synthesizing information from different sources. The dataset provides both distractor and supporting\n",
      "documents, allowing evaluation of models’ ability to filter relevant information effectively. As the\n",
      "answers to the test questions in HotpotQA are not publicly available we took a subsample from its\n",
      "validation set (7,405 instances) as previous research did [91, 44]. The last 1,000 validation questions\n",
      "were selected for the agent evaluation on HotpotQA. The first 1,000 questions were used as the\n",
      "training data for process supervision.\n",
      "2WikiMultihopQA.\n",
      "2WikiMultihopQA is another multi-hop question-answering dataset con-\n",
      "structed from Wikipedia. 2WikiMultihopQA focuses on high-quality reasoning paths by selecting\n",
      "supporting documents more systematically. The dataset contains questions that require reasoning\n",
      "across different Wikipedia pages, ensuring a diverse range of factual and inferential challenges. The\n",
      "last 1000 questions in the development set (12,576 question in total) were used for agent evaluation.\n",
      "Bamboogle.\n",
      "Bamboogle is a manually constructed dataset designed to evaluate compositional\n",
      "reasoning and adversarial robustness. It consists of 2-hop questions written by researchers, where\n",
      "17\n",
      "both supporting facts exist in Wikipedia but are structured to be challenging for retrieval-based\n",
      "systems. Unlike automatically generated datasets like 2WikiMultihopQA and Musique, Bamboogle\n",
      "questions do not follow fixed templates, increasing their variability. We used the whole test set with\n",
      "125 questions for the evaluation of agents on Bamboogle.\n",
      "MedQA.\n",
      "MedQA is a medical question-answering dataset sourced from professional medical\n",
      "exams such as the USMLE (United States Medical Licensing Examination). It requires domain-\n",
      "specific knowledge and reasoning to answer multiple-choice medical questions. We focused on the\n",
      "English split of MedQA with 1,273 USMLE-style test questions. A subset of 1,000 questions was\n",
      "sampled from the training set (10,178 questions) for the optimization of various agents.\n",
      "D\n",
      "Baseline Descriptions\n",
      "Here are the detailed descriptions of various baseline agents that we implemented in the experiments.\n",
      "Direct.\n",
      "The Direct agent represents the simplest baseline, where the language model is prompted\n",
      "to output the predicted answer immediately, without any explicit intermediate reasoning or search\n",
      "steps. This approach tests the model’s ability to answer questions in a single step, relying solely on\n",
      "its internal knowledge and without leveraging external retrieval or multi-step reasoning.\n",
      "CoT [81].\n",
      "The Chain-of-Thought (CoT) agent encourages the model to generate a step-by-step\n",
      "reasoning process before producing the final answer, but still does so in a single iteration. The agent\n",
      "is prompted to articulate its reasoning explicitly, which can help with complex questions by making\n",
      "the model’s thought process transparent and potentially improving answer accuracy. However, CoT\n",
      "does not incorporate external retrieval or iterative search.\n",
      "RAG [42].\n",
      "The Retrieval-Augmented Generation (RAG) agent augments the language model with a\n",
      "retrieval step. At the first iteration, the agent issues the original question as a search query to retrieve\n",
      "relevant documents. In the subsequent step, it reasons about the updated state, which includes the\n",
      "retrieved information, and generates a predicted answer. This approach leverages external knowledge\n",
      "but does not perform multi-hop or iterative search.\n",
      "ReAct [91].\n",
      "The ReAct agent combines reasoning and acting by allowing the model to interleave\n",
      "natural language reasoning with actions, such as issuing search queries or providing answers. At each\n",
      "step, the agent reasons about the current state and decides whether to search for more information or\n",
      "to answer the question. This enables multi-step, interactive information-seeking and supports more\n",
      "complex reasoning chains.\n",
      "Search-o1 [44].\n",
      "The Search-o1 agent extends the ReAct framework by introducing a knowledge\n",
      "summarization step before reasoning. For each search query, the agent reasons about the retrieved\n",
      "documents and briefly summarize the useful information as the direct answer to the search query,\n",
      "forming query-answer pairs that are used as input for subsequent reasoning steps. This approach\n",
      "replaces the use of raw documents with structured summaries, potentially improving reasoning\n",
      "efficiency. Search-o1 can be viewed as a special case of ReAct where retrieval is performed via RAG\n",
      "and the agent operates on summarized knowledge rather than full documents.\n",
      "E\n",
      "Implementation Details\n",
      "In our experiments, we selected Llama-3.1-8B-Instruct [13] as the base LLM for the implementations\n",
      "of various language agents, due to its context length of 128k tokens and its availability of open-source\n",
      "parameters. The critic is also trained based on the same Llama-3.1-8B-Instruct, same as the actor.\n",
      "We involved GPT-4o-mini and Qwen-2.5-7B-Instruct to show the transferability of the trained critic\n",
      "(Figure 2) and the generalizability of RAG-Gym (Table 3) to other LLMs.\n",
      "E.1\n",
      "Details of Process Data Collection\n",
      "To evaluate intermediate reasoning and search steps in RAG-Gym, we design a process reward\n",
      "function that assesses queries based on three key criteria:\n",
      "18\n",
      "• If the retrieval history already contains sufficient information, answering should be the preferred\n",
      "action instead of searching further.\n",
      "• Queries should also be precise, actionable, and foundational to solving the question while avoiding\n",
      "unnecessary details.\n",
      "• Queries should introduce new, useful information rather than repeating past searches.\n",
      "These criteria ensure that queries are efficient, targeted, and contribute meaningfully to constructing\n",
      "the final answer.\n",
      "The data collection pipeline begins with trajectory sampling, where the language agent generates a se-\n",
      "quence of actions based on its current policy. At each step in the trajectory, multiple candidate actions\n",
      "are proposed, and the best action is selected according to predefined evaluation criteria. To streamline\n",
      "the annotation process and ensure consistency, we employ a ranking-based evaluation framework\n",
      "rather than assigning numerical scores. The selected action is then executed, and the trajectory\n",
      "transitions to the next state. This process is repeated iteratively until the trajectory terminates.\n",
      "To ensure quality, only sampled trajectories that result in a correct final answer are retained, as\n",
      "determined by the outcome reward. This filtering guarantees that the selected actions not only align\n",
      "with the process reward criteria but also contribute to successful task completion. To address the\n",
      "challenges of slow and costly human annotation, we leverage LLMs such as GPT-4o to annotate\n",
      "the sampled trajectories. As demonstrated in our experiments (Table 4), annotations generated by\n",
      "GPT-4o exhibit high reliability, closely aligning with domain expert judgments. This approach\n",
      "enables scalable and efficient data collection, making it feasible to gather high-quality process reward\n",
      "data at scale.\n",
      "Figure 5: Pipeline of the process data collection in RAG-Gym. Process reward data is collected by\n",
      "randomly sampling action candidates at each time step and using an external annotator (e.g., GPT-4o)\n",
      "to select the best one. The episode is terminated when the agent generates a final answer.\n",
      "For the implementation of the IR environment, we select Wikipedia as the supporting corpus for the\n",
      "retrieval of relevant information for questions from HotpotQA, 2WikiMultihopQA, and Bamboogle.\n",
      "For the environment of solving MedQA questions, we use a combination of medical textbooks and\n",
      "StatPearls which were pre-processed in MedRAG [85]. For all tasks, we used both lexical and\n",
      "semantic retrievers whose results were merged with Reciprocal Rank Fusion [11]. BM25 [58] and\n",
      "BGE-Base [84] were used for HotpotQA, 2WikiMultihopQA, and Bamboogle, while in MedQA, we\n",
      "selected BM25 and MedCPT [35]. A set of 32 documents will be retrieved for each search query.\n",
      "E.2\n",
      "Details of LLM Post-training\n",
      "For the actor tuning, we employed Low-Rank Adaptation (LoRA) [23] in the implementaion of\n",
      "supervised fine-tuning (SFT) [52] and direct preference optimization (DPO) [56] [60] with r = 256\n",
      "and alpha = 512 on all attention components in the transformers architecture [74]. SFT and DPO\n",
      "19\n",
      "were implemented using the TRL package [75]. For proximal policy optimization (PPO), we used\n",
      "the OpenRLHF package [24] with full-parameter tuning. Detailed hyperparameter settings for SFT,\n",
      "DPO, and PPO can be found in our source code. For the tuning of Search-o1 and Re2Search agents,\n",
      "only the LLM for action reasoning is trained while the one for history knowledge summarization\n",
      "remains untuned.\n",
      "E.3\n",
      "Details of LLM Inference\n",
      "All results of zero-shot learning (ZSL), supervised fine-tuning (SFT), direct preference optimization\n",
      "(DPO), and proximal policy optimization (PPO) are generated with a temperature of 0.0. For the\n",
      "evaluation of agents with a critic, we employed a temperature of 1.0 with 10 different actions sampled\n",
      "for each step in the information-seeking trajectory. Algorithm 1 presents our algorithm of using the\n",
      "trained process reward model to guide the action selection during inference. All experiments were\n",
      "conducted on NVIDIA A100 and A6000 GPUs.\n",
      "Algorithm 1 PRM-Guided Inference with Best-of-N Selection\n",
      "1. Input: Original question Q, actor πθ, critic rϕ, number of candidate actions N, maximum steps\n",
      "T, information retrieval function IR.\n",
      "2. Initialize state S ←(Q, H1 = ∅).\n",
      "3. For t = 1 to T:\n",
      "(a) Generate N candidate actions: aq, · · · , aN ∼πf(θ)(·|S).\n",
      "(b) Compute process rewards and select the best action: a∗←arg maxa∈{a1,··· ,aN} rϕ(S, a).\n",
      "(c) If a∗is a search query:\n",
      "i. Retrieve documents: D ←IR(a∗).\n",
      "ii. Update state: S ←(Q, Ht+1 = Ht ∪{(a∗, D)}).\n",
      "(d) If a∗is a final answer:\n",
      "i. Return a∗and terminate the process.\n",
      "4. End For\n",
      "F\n",
      "Study on the Number of Search Queries\n",
      "In addition to the results presented in Table 2, we further analyzed the number of search queries\n",
      "generated by Re2Search agents across different datasets. Table 5 reports the minimum, maximum,\n",
      "and mean number of search queries issued. The maximum value is capped at 10, reflecting the upper\n",
      "limit of iterations allowed per question in our experiments. The results show that tuned agents (SFT,\n",
      "DPO, and PPO) consistently generate more search queries than the zero-shot agent (ZSL), indicating\n",
      "that fine-tuning encourages more extensive information-seeking behavior, which aligns with their\n",
      "improved performance.\n",
      "Table 5: Minimum, maximum, and mean number of search queries generated by Re2Search agents\n",
      "for each dataset.\n",
      "HotpotQA\n",
      "2Wiki\n",
      "Bamboogle\n",
      "MedQA\n",
      "Min\n",
      "Max\n",
      "Mean\n",
      "Min\n",
      "Max\n",
      "Mean\n",
      "Min\n",
      "Max\n",
      "Mean\n",
      "Min\n",
      "Max\n",
      "Mean\n",
      "ZSL\n",
      "0.0\n",
      "9.0\n",
      "1.5\n",
      "0.0\n",
      "9.0\n",
      "3.4\n",
      "0.0\n",
      "9.0\n",
      "1.0\n",
      "0.0\n",
      "9.0\n",
      "0.4\n",
      "SFT\n",
      "0.0\n",
      "9.0\n",
      "2.1\n",
      "0.0\n",
      "9.0\n",
      "3.8\n",
      "0.0\n",
      "9.0\n",
      "1.9\n",
      "0.0\n",
      "9.0\n",
      "0.6\n",
      "DPO\n",
      "0.0\n",
      "9.0\n",
      "3.2\n",
      "0.0\n",
      "9.0\n",
      "4.5\n",
      "0.0\n",
      "9.0\n",
      "3.4\n",
      "0.0\n",
      "9.0\n",
      "2.2\n",
      "PPO\n",
      "0.0\n",
      "9.0\n",
      "4.6\n",
      "0.0\n",
      "9.0\n",
      "5.6\n",
      "0.0\n",
      "9.0\n",
      "2.7\n",
      "0.0\n",
      "9.0\n",
      "5.6\n",
      "20\n",
      "G\n",
      "Case Studies\n",
      "G.1\n",
      "Comparison of Agent Designs on Bamboogle\n",
      "We analyze the reasoning and search behaviors of RAG, ReAct, Search-o1, and Re2Search using an\n",
      "example from the Bamboogle dataset. As shown in Figure 7, given the question “What was the father\n",
      "of the last surviving Canadian father of Confederation?\", the three agents show distinct behaviors\n",
      "when generating the first action.\n",
      "The RAG agent directly passes the question as a search query without decomposition, relying entirely\n",
      "on retrieval to infer the answer. This often leads to ineffective searches that fail to retrieve necessary\n",
      "intermediate facts. ReAct and Search-o1 improve upon this by engaging in stepwise query reasoning,\n",
      "first identifying the need to determine the last surviving Canadian father of Confederation before\n",
      "issuing a search query. However, the generated query, “List of Canadian fathers of Confederation”,\n",
      "retrieves broad information rather than directly resolving the missing knowledge.\n",
      "In contrast, Re2Search explicitly integrates answer reasoning with search. It first constructs a potential\n",
      "answer, identifying an unverified claim that William Lyon Mackenzie King is among the last surviving\n",
      "Canadian fathers of Confederation. Recognizing the missing evidence, it formulates a targeted query,\n",
      "“Who is the last surviving Canadian father of Confederation?”, to resolve the uncertainty. This\n",
      "approach ensures that retrieval is aligned with answer construction, minimizing unnecessary queries\n",
      "and improving information efficiency. The case study illustrates how Re2Search effectively refines\n",
      "the search process by linking query generation to specific knowledge gaps.\n",
      "answer reasoning  & reflection\n",
      "(a) RAG\n",
      "(b) ReAct / Search-o1\n",
      "(c) Re Search\n",
      "1.  Need to identify the\n",
      "last surviving Canadian\n",
      "father of Confederation.\n",
      "2.  Start by searching for\n",
      "the list of Canadian\n",
      "fathers of Confederation.\n",
      "Query: List of Canadian\n",
      "fathers of Confederation\n",
      "1. William Lyon\n",
      "Mackenzie King is among\n",
      "the last Canadian father\n",
      "of Confederation\n",
      "2. Mackenzie King's\n",
      "father was James\n",
      "Mackenzie\n",
      "Query: Who is the last\n",
      "surviving Canadian father\n",
      "of Confederation?\n",
      "Question: What was the father of\n",
      "the last surviving Canadian\n",
      "father of Confederation?\n",
      "Question: What was the father of\n",
      "the last surviving Canadian\n",
      "father of Confederation?\n",
      "Question: What was the father of\n",
      "the last surviving Canadian\n",
      "father of Confederation?\n",
      "Query: What was the\n",
      "father of the last surviving\n",
      "Canadian father of\n",
      "Confederation?\n",
      "query reasoning\n",
      "step 1\n",
      "step 2\n",
      "step 1\n",
      "step 2\n",
      "query generation\n",
      "query generation\n",
      "direct pass\n",
      "Figure 6: Comparison of different agent architectures in handling a multi-hop question from Bam-\n",
      "boogle.\n",
      "G.2\n",
      "Comparison of Agent Designs on MedQA\n",
      "Similarly, when presented with a complex medical question from MedQA, the distinct approaches of\n",
      "the agents are evident. The RAG agent, as before, directly uses a truncated version of the lengthy\n",
      "input as its search query, which is unlikely to yield specific, actionable information. ReAct and\n",
      "Search-o1 engage in query reasoning, first hypothesizing that these symptoms suggest a possible\n",
      "diagnosis of serotonin syndrome and then deciding to search for information on the treatment of\n",
      "serotonin syndrome. While this is more targeted than RAG, Re2Search demonstrates a more refined\n",
      "process by engaging in answer reasoning and reasoning reflection. It posits that the symptoms are\n",
      "suggestive of a cholinergic syndrome. Recognizing the need to confirm the relationship between the\n",
      "patient’s existing conditions and the suspected syndrome, it generates a highly specific query about\n",
      "the relationship between constipation, fibromyalgia, and cholinergic syndrome. This demonstrates\n",
      "Re2Search’s ability to align its search strategy with the nuances of constructing a well-supported\n",
      "answer, thereby improving the precision of its information retrieval in a complex diagnostic scenario.\n",
      "21\n",
      "(a) RAG\n",
      "(b) ReAct / Search-o1\n",
      "these symptoms ... suggest\n",
      "a possible diagnosis of\n",
      "serotonin syndrome.\n",
      "Query: What are the appropriate\n",
      "treatments for serotonin\n",
      "syndrome?\n",
      "These symptoms ... are\n",
      "suggestive of a cholinergic\n",
      "syndrome\n",
      "Query: What is the relationship\n",
      "between constipation,\n",
      "fibromyalgia, and cholinergic\n",
      "syndrome?\n",
      "Question: A 25-year-old man with a past medical history of constipation and fibromyalgia ... generalized\n",
      "malaise and severe diarrhea ... rhinorrhea, lacrimation, and piloerection ... pupils are dilated ... scars are\n",
      "noted in the antecubital fossa ... Which could be an appropriate treatment for this patient's symptoms?\n",
      "Query: A 25-year-old man ...\n",
      "query reasoning\n",
      "step 1\n",
      "step 2\n",
      "answer reasoning  & reflection\n",
      "step 1\n",
      "step 2\n",
      "query generation\n",
      "query generation\n",
      "direct pass\n",
      "we need to search for\n",
      "information on the\n",
      "treatment of serotonin\n",
      "syndrome\n",
      "the most appropriate\n",
      "treatment ... an\n",
      "anticholinergic medication\n",
      "(c) Re Search\n",
      "Figure 7: Comparison of different agent architectures in handling a multi-hop question from MedQA.\n",
      "G.3\n",
      "Critic-Guided Action Selection in MedQA\n",
      "To illustrate how the trained critic improves decision-making, we examine a case from the MedQA\n",
      "dataset (Table 6). The model is tasked with identifying the mechanism of action of the most likely\n",
      "anti-diabetic medication for a 60-year-old patient presenting with symptoms indicative of fluid\n",
      "overload.\n",
      "For the generation of the first action, the agent initially generates a broad and less actionable query\n",
      "about heart failure, which, while relevant, does not directly contribute to identifying the correct\n",
      "medication. Meanwhile, it also provides another candidate action with the query “What are common\n",
      "classes of oral anti-diabetic medications?\", which leads to retrieving a structured list of relevant drug\n",
      "categories. The process reward model estimates the quality of these two candidates and identifies the\n",
      "second candidate as the better action.\n",
      "As the reasoning progresses, the process reward model continues to refine action selection, identifying\n",
      "the best queries to resolve missing information efficiently. The rewarded queries ultimately guide\n",
      "the agent toward correctly inferring that the medication is most likely to be Biguanides (metformin),\n",
      "which acts by increasing gene expression of GLUT-4. This case demonstrates how process reward\n",
      "models enhance decision quality by selecting queries that effectively bridge knowledge gaps, leading\n",
      "to more precise reasoning and higher answer accuracy.\n",
      "H\n",
      "Prompt Templates\n",
      "We provide structured prompt templates for history knowledge summarization and action generation\n",
      "in our proposed Re2Search agent. The template in Figure 8 ensures that retrieved documents are\n",
      "summarized concisely and factually for follow-up queries. Figure 9 shows the prompt template used\n",
      "by Re2Search to align the answer construction with query formulation. The prompt of using GPT-4o\n",
      "for process reward data annotation is presented in Figure 10.\n",
      "22\n",
      "Table 6: An example of how PRM helps identify high-quality actions that lead to correct answers.\n",
      "The unverified claims in the presented example are highlighted.\n",
      "MedQA Question\n",
      "Question: A 60-year-old woman presents to a physician for worsening shortness of breath and increasing\n",
      "abdominal distention over the last 3 months. She says that the shortness of breath is worse on exertion and\n",
      "improves with rest. While she could previously walk to the nearby store for her groceries, she now has to\n",
      "drive because she gets ”winded” on the way. The patient was diagnosed with diabetes 5 years ago and is\n",
      "compliant with her medications. The medical history is otherwise unremarkable. The physical examination\n",
      "reveals gross ascites and visibly engorged periumbilical veins. Bilateral pitting edema is noted around\n",
      "the ankles. The finger-prick blood glucose level is 100 mg/dL. What is the mechanism of action of the\n",
      "anti-diabetic medication this patient is most likely taking? A. Binding to the alpha subunit of the insulin\n",
      "receptor, B. Closure of ATP-sensitive K-channels in the pancreatic beta-cell, C. Increased gene expression of\n",
      "GLUT-4, D. Glucagon-like peptide-1 receptor agonist\n",
      "Answer (invisible): C\n",
      "Action 1\n",
      "[Candidate 1 – Reasoning] ...\n",
      "[Candidate 1 – Action]\n",
      "Query: What is the primary concern with the patient’s condition, specifically whether heart failure is\n",
      "confirmed based on symptoms and physical examination findings? l\n",
      "[Candidate 2 – Reasoning]\n",
      "1. The patient presents with worsening shortness of breath, increasing abdominal distention, and gross\n",
      "ascites. These symptoms, along with visibly engorged periumbilical veins and bilateral pitting edema, are\n",
      "indicative of fluid overload and congestive heart failure.\n",
      "2. The patient’s history of diabetes, which is compliant with medication, suggests that her anti-diabetic\n",
      "medication is likely a type of oral hypoglycemic agent rather than insulin.\n",
      "3. Considering the patient’s age (60 years) and symptoms, it is unlikely that she is taking a glucagon-like\n",
      "peptide-1 (GLP-1) receptor agonist, which is typically used in younger patients with type 2 diabetes and has\n",
      "a different side-effect profile.\n",
      "4. With the remaining options, ’Binding to the alpha subunit of the insulin receptor’ and ’Closure of\n",
      "ATP-sensitive K-channels in the pancreatic beta-cell’, we can narrow down the possibilities. ’Binding to the\n",
      "alpha subunit of the insulin receptor’ is more characteristic of insulin receptor agonists or insulin itself, which\n",
      "is unlikely given the patient’s age and the oral nature of her medication. Therefore, ’Closure of ATP-sensitive\n",
      "K-channels in the pancreatic beta-cell’ is the most plausible mechanism of action.\n",
      "5. This mechanism is consistent with the sulfonylureas class of medications, which are commonly used in\n",
      "patients with type 2 diabetes.\n",
      "[Candidate 2 – Action]\n",
      "Query: What are common classes of oral anti-diabetic medications? m\n",
      "Environment Feedback 1\n",
      "[+ Documents]\n",
      "Common classes of oral anti-diabetic medications include: 1. Sulfonylureas, 2. Dipeptidyl Peptidase IV\n",
      "(DPP IV) inhibitors (gliptins), 3. Thiazolidinediones, 4. Meglitinides, 5. Sodium-glucose co-transporter 2\n",
      "(SGLT 2) inhibitors, 6. Alpha-glucosidase inhibitors, 7. Biguanides (metformin).\n",
      "Action 2\n",
      "[Candidate 1 – Reasoning] ...\n",
      "[Candidate 1 – Action]\n",
      "Query: Is metformin commonly prescribed for type 2 diabetes and known to cause lactic acidosis as a side\n",
      "effect? l\n",
      "[Candidate 2 – Reasoning] ...\n",
      "[Candidate 2 – Action]\n",
      "Query: What is the mechanism of action of Biguanides (metformin)? m\n",
      "Environment Feedback 2\n",
      "[+ Documents]\n",
      "The mechanism of action of Biguanides (metformin) is to activate the enzyme AMP-activated protein kinase\n",
      "(AMPK) and reduce hepatic glucose production.\n",
      "Action 3\n",
      "[Candidate 1 – Reasoning] ... [Candidate 1 – Action] Answer: B l\n",
      "[Candidate 2 – Reasoning] ... [Candidate 2 – Action] Answer: C m\n",
      "23\n",
      "Prompt template for history knowledge summarization in Search-o1 and Re2Search\n",
      "You are a helpful assistant tasked with answering a follow-up query using the relevant\n",
      "documents provided.\n",
      "### Relevant Documents\n",
      "{{documents}}\n",
      "### Context\n",
      "Original question: {{question}}\n",
      "### Follow-up Query\n",
      "{{query}}\n",
      "Answer the follow-up query succinctly, using only the information from the documents. When\n",
      "the documents do not provide sufficient information, explicitly point this out instead of making\n",
      "up facts. Do not include unrelated or excessive details in the response.\n",
      "Figure 8: Template used for history knowledge summarization in Search-o1 and Re2Search.\n",
      "Prompt template for generating actions using the Re2Search agent\n",
      "You are a helpful assistant. Your task is to answer a given question following user instructions.’\n",
      "### Information-seeking History\n",
      "{{history}}\n",
      "### Original Question\n",
      "{{question}}\n",
      "Your output must include three sections:\n",
      "1. **### Step-by-step Reasoning**:\n",
      "- Think step-by-step and then answer the question.\n",
      "2. **### Unverified Claim Identification**:\n",
      "- Identify if there are claims in the step-by-step reasoning section that are not grounded in the\n",
      "information-seeking history section.\n",
      "- If yes, summarize the first piece of missing information as an atomic query to search in an\n",
      "external knowledge base.\n",
      "- If no, clearly state that no further query is needed.\n",
      "3. **### Structured Output**:\n",
      "- Present your predicted answer and generated query (if applicable) in the following JSON\n",
      "format:\n",
      "“‘json\n",
      "{\n",
      "“predicted_answer\": “Provide a single letter (for multiple-choice questions), digit, word, or\n",
      "short phrase here.\",\n",
      "“generated_query\": “Provide an entity, question, or statement to be searched in an external\n",
      "knowledge base. Output \\“None\\\" if no query is generated.\",\n",
      "}\n",
      "“‘\n",
      "Figure 9: Template used to generate actions for the Re2Search agent.\n",
      "24\n",
      "Prompt template for ranking candidate actions with GPT-4o\n",
      "You are a decision-evaluation assistant. Your task is to rank the proposed actions from the\n",
      "most appropriate to the least appropriate as the next step in a sequential decision-making\n",
      "process aimed at solving a given question.\n",
      "### Original Question:\n",
      "{{question}}\n",
      "### Information-Seeking History:\n",
      "{{curr_history}}\n",
      "### Proposed Next Actions:\n",
      "{{actions_text}}\n",
      "### Important Assumption\n",
      "The agent has no prior knowledge about the subject matter. It must rely solely on the\n",
      "information-seeking history provided to evaluate and answer the original question. Assump-\n",
      "tions not explicitly supported by the history must not influence the ranking of proposed actions.\n",
      "### Evaluation Criteria for Appropriateness\n",
      "1. **Sufficiency Check**:\n",
      "- Determine whether the available information is sufficient to directly answer the original\n",
      "question. If not, the proposed action to “Answer” is inappropriate.\n",
      "- Prioritize queries that gather specific, missing information essential to solving the question.\n",
      "- If the history already contains all necessary information, then “Answer” is the most\n",
      "appropriate action, and the correct answer should be ranked highest.\n",
      "2. **Utility Check**:\n",
      "- Queries must be precise, actionable, and directly relevant to solving the question.\n",
      "- Prioritize foundational queries that establish critical context or general knowledge necessary\n",
      "for more specific follow-ups.\n",
      "- Rank overly narrow or prematurely specific queries lower if they presume knowledge not yet\n",
      "available.\n",
      "- Avoid irrelevant queries that do not contribute to solving the original question.\n",
      "3. **Redundancy Check**:\n",
      "- Queries that duplicate information already covered in the history or repeat previous queries\n",
      "should be ranked lower.\n",
      "- Proposed actions must add new value to the decision-making process by seeking new or\n",
      "clarifying missing information.\n",
      "### Expected Output Format\n",
      "- Output the indices of the ranked actions in JSON format: “‘json{“ranked_indices”: [list of\n",
      "indices]}”’.\n",
      "- Rank actions from most appropriate to least appropriate based on the evaluation criteria\n",
      "above.\n",
      "- Do not provide additional explanations or reasoning.”’\n",
      "Figure 10: Template used by GPT-4o to rank action candidates given the state.\n",
      "25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content) # 논문 내용\n",
    "# 다운로드 기능은 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984953e",
   "metadata": {},
   "source": [
    "### Docling\n",
    "- IBM Research에서 개발한 오픈소스 문서처리 도구로 다양한 종류의 문서를 구조화된 데이터로 변환해 생성형 AI에서 활용할 수있도록 지원한다.\n",
    "- **주요기능**\n",
    "  - PDF, DOCX, PPTX, XLSX, HTML, 이미지 등 여러 형식을 지원\n",
    "  - PDF의 **페이지 레이아웃, 읽기 순서, 표 구조, 코드, 수식** 등을 분석하여 정확하게 읽어들인다.\n",
    "  - OCR을 지원하여 스캔된 PDF나 이미지에서 텍스트를 추출할 수있다.\n",
    "  - 읽어들인 내용을 markdown, html, json등 다양한 형식으로 출력해준다.\n",
    "- 설치 : `pip install langchain-docling ipywidgets -qU` \n",
    "- 참조\n",
    "  - docling 사이트: https://github.com/docling-project/docling\n",
    "  - 랭체인-docling https://python.langchain.com/docs/integrations/document_loaders/docling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c0672-a932-4b55-bc39-1863e00ef3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !uv pip install langchain-docling transformers ipywidgets\n",
    "# !uv pip install accelerate\n",
    "# ## 딥러닝 모델 사용. gpu가 있을 경우 torch cuda 버전을 먼저 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e423ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface 로그인 - 모델을 받기 위해서\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "hf_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "login(hf_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a639f1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 14:43:09,555 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-22 14:43:09,558 - INFO - Going to convert document batch...\n",
      "2025-12-22 14:43:09,559 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e15bc6f248154cc62f8db15ef18a8ab7\n",
      "2025-12-22 14:43:09,560 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-12-22 14:43:09,562 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-12-22 14:43:09,563 - INFO - Accelerator device: 'cpu'\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,574 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,575 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,587 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,588 [RapidOCR] main.py:50: Using C:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,700 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,700 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,702 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,702 [RapidOCR] main.py:50: Using C:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,747 [RapidOCR] base.py:22: Using engine_name: torch\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,748 [RapidOCR] device_config.py:50: Using CPU device\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,769 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "\u001b[32m[INFO] 2025-12-22 14:43:09,769 [RapidOCR] main.py:50: Using C:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001b[0m\n",
      "2025-12-22 14:43:09,915 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-12-22 14:43:09,916 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-22 14:43:10,798 - INFO - Accelerator device: 'cpu'\n",
      "2025-12-22 14:43:11,346 - INFO - Processing document 2412.15184v2.pdf\n",
      "2025-12-22 14:44:14,926 - INFO - Finished converting document 2412.15184v2.pdf in 65.39 sec.\n"
     ]
    }
   ],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "\n",
    "path = \"data/papers/2412.15184v2.pdf\"\n",
    "loader = DoclingLoader(\n",
    "    file_path=path,\n",
    "    export_type=ExportType.MARKDOWN\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c6cd786-947d-49ae-933c-627ca714c06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "849e4ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning\n",
      "\n",
      "Simon Frieder ∗,1 , Jonas Bayer 2 , Sam Looi 4 , Jacob Loader 2 , Julius Berner 4 , Katherine M. Collins 2 , András Juhász 1 , Fabian Ruehle 9 , Sean Welleck 7 , Gabriel Poesia 10 , Ryan-Rhys Griffiths 11 , Adrian Weller 2 , Anirudh Goyal 12 , Thomas Lukasiewicz 3,1 , Cameron Freer 13 , Kevin Buzzard 8 , and Timothy Gowers 2,6\n",
      "\n",
      "1 University of Oxford\n",
      "\n",
      "2 University of Cambridge\n",
      "\n",
      "3 Vienna University of Technology 4 Caltech\n",
      "\n",
      "6 Collège de France\n",
      "\n",
      "7 Carnegie Mellon University\n",
      "\n",
      "8 Imperial College London\n",
      "\n",
      "9 Northeastern University\n",
      "\n",
      "10 Stanford University\n",
      "\n",
      "11 FutureHouse Inc.\n",
      "\n",
      "12 Meta\n",
      "\n",
      "13 MIT\n",
      "\n",
      "## Abstract\n",
      "\n",
      "The datasets and benchmarks commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings and misdirections. These range from a restricted scope of mathematical complexity to limited fidelity in capturing aspects beyond the final, written proof (e.g. motivating the proof, or representing the thought processes leading to a proof). These issues are compounded by a dynamic reminiscent of Goodhart's law: as benchmark performance becomes the primary target for model development, the benchmarks themselves become less reliable indicators of genuine mathematical capability. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or 'thought partners'), necessitates a course correction both in the design of mathematical datasets and the evaluation criteria of the models' mathematical ability. In particular, it is necessary for benchmarks to move beyond the existing result-based datasets that map theorem statements directly to proofs, and instead focus on datasets that translate the richer facets of mathematical research practice into data that LLMs can learn from. This includes benchmarks that supervise the proving process and the proof discovery process itself, and we advocate for mathematical dataset developers to consider the concept of 'motivated proof' , introduced by G. Pólya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations.\n",
      "\n",
      "∗ Corresponding author: simon.frieder@cs.ox.ac.uk .\n",
      "\n",
      "## 1 AI Systems for Mathematicians - Present and Future\n",
      "\n",
      "There has been a rapid surge in research around AI for mathematics. For example, AlphaGeometry was shown to solve mathematical geometry problems on the level of the International Math Olympiad (IMO) [Trinh et al., 2024], Numina won the first AIMO Progress Prize, 1 by providing a model capable of turning mathematical questions into executable Python code, and previously GPT-4 [OpenAI, 2023] was shown to attain, in some cases, the performance of an undergraduate university student [Frieder et al., 2023a]. High accuracies on standard mathematics benchmarks [Reid et al., 2024] suggest that many benchmarks may be close to being solved.\n",
      "\n",
      "Update since the December 2024 version. This manuscript was initially circulated in December 2024. Since then, reported performance on boxed-answer benchmarks has continued to increase, and some reasoning-focused systems report near-saturation on MATH -style evaluations [DeepSeek-AI, 2025, Qwen Team, 2024]. At the same time, many of the issues discussed in this paper have seen fewer changes than model scores: public math benchmarks remain largely static, scoring often still collapses rich outputs to a single correctness bit, and contamination risk persists as benchmarks diffuse into training corpora. The revisions in this version therefore focus on incorporating recent benchmark results while keeping the central claim unchanged: progress toward mathematical copilots depends on data and evaluation that represent intermediate reasoning and workflow competence, not only final answers.\n",
      "\n",
      "The rapid progress and proliferation of ideas in this space - while exciting and productive in many ways has led to a series of problems due to a lack of alignment across research directions and a lack of coherence around clear goals. Problems include:\n",
      "\n",
      "- (1) some datasets being overstudied (e.g., the many versions of the GSM8K dataset [Cobbe et al., 2021]), whereas other data-related aspects (few datasets comprising advanced mathematics or reflecting tool use in mathematics) are neglected;\n",
      "- (2) various workflows and reasoning modes of how professional mathematicians conduct their research are not represented in datasets;\n",
      "- (3) different ways to express the same mathematical content (in formal language, vs. in natural language) have not been unified;\n",
      "- (4) scalability of evaluation that aligns with the goals of the tools being developed.\n",
      "\n",
      "These problems are not unique to AI for mathematics, but the broader machine learning community has begun systematic self-assessment efforts that the mathematical AI community has yet to undertake. For instance, Bhardwaj et al. [2024] present an evaluation framework grounded in data curation principles from librarianship and archives, and use it to assess 60 datasets from the NeurIPS Datasets and Benchmarks track. Their analysis shows gaps in documentation around ethical considerations, data management, and environmental footprint, and they provide concrete recommendations for improving peer-review standards. No comparable audit exists for mathematical datasets. Such an effort would be valuable: mathematical datasets come with domain-specific concerns (e.g., difficulty calibration, formal-informal alignment, contamination risks) that generic ML dataset assessments do not address.\n",
      "\n",
      "Despite these problems, the landscape of machine learning models that have been devised (and embraced by parts of the mathematical community) consists primarily of models that lie between the two categories below , based on the amount of interaction with humans they are designed for:\n",
      "\n",
      "1. Highly specialized, 'narrow' models that fulfill all of the criteria of 1) being designed for a precisely defined mathematical domain or mathematical problem, 2) requiring domain knowledge to be used (such as formal languages as input), 3) that cannot explain themselves, and 4) do not allow free-form interaction. Examples of such specialized models vary widely in terms of how their foundation is set up\n",
      "\n",
      "1 https://aimoprize.com/updates/2024-07-20-progress-prize-results\n",
      "\n",
      "(model architecture, training data), and in what they assist mathematicians. Specialized models could rely on symbolic reasoners, such as the recent solver for elementary geometry (Newclid [Sicca et al., 2024], which supersedes AlphaGeometry [Trinh et al., 2024]) or for inequality problems ([Wei et al., 2024]). Olympiad-level mathematics problems are within reach of both, and these assist mathematicians solely in solving closed-ended questions. Alternatively, such specialized models could simply be regression models, relying on raw mathematical data to infer new relationships between mathematical objects, such as for knot theory or representation theory [Davies et al., 2021] or group theory [He et al., 2023], assisting mathematicians in finding conjectures (which then need to be proved by hand). Many further approaches exist [Romera-Paredes et al., 2024, Fawzi et al., 2022, Hashemi et al., 2024], see Table 1 for more information. These systems typically require a significant amount of specialization by a user, such as mastering a formal input language (e.g., AlphaGeometry and Newclid, which both require mastery of the JGEX formal language) or understanding the machine learning model itself (e.g., [Davies et al., 2021]) beyond the necessary mathematical domain knowledge. The data on which they operate also varies strongly: Symbolic solvers rely on a symbolic language; models like [Davies et al., 2021] rely on tabular data, while models like [Hashemi et al., 2024] combine different data types.\n",
      "\n",
      "2. General purpose, 'broad' models that can interact with humans via natural language, understand input images, and use tools (being able to call, in particular, specialized models, which can be viewed as a particular kind of 'tool', as we argue below), can provide general assistance to a mathematician (both with routine tasks, such as literature search, and domain-dependent tasks, such as counterexample search) and are not tied to a specific area of mathematics. These are what we refer to as 'Mathematical Copilots' , as their goal is not simply to provide correct proofs but also to enhance the user's understanding of the results and guide them towards their own discoveries [Collins et al., 2024a]. Currently, the best candidates for these systems are large language models (LLMs), which we take to include multi-modal models that also admit images as components of the prompt. We contend that LLMs represent early predecessors of future general-purpose models (mathematical copilots), ultimately acting as mathematicians' 'thought partners' [Collins et al., 2024b]. Anecdotal evidence of such usefulness of current LLMs has been presented in various discussion forums, 2 while noting the failures that LLMs often succumb to. However, there are concrete ways in which LLMs can be improved to become mathematical copilots: As indicated by the tool-integrated-reasoning approach, championed by models such as ToRA [Gou et al., 2024] or Numina, 3 general LLMs can incorporate other symbolic systems to which they can delegate symbolic tasks to. The mixture-of-experts paradigm [Cai et al., 2024], on the other hand, highlights different ways in which LLMs can incorporate other LLMs as specially trained 'modules' to which they can delegate mathematical tasks that are not symbolic in nature but still require particular mathematical skills. Such a hierarchical system can help mathematicians both on routine academic tasks (see [Frieder et al., 2023b] for a short, high-level overview) and specialized ones. We envision that such models will require significantly less effort from users to use them, compared to specialized models, being self-explanatory, where natural language is the main mode of interaction. (While the outlined architectural approaches provide a viable step forward, in some cases, no data exists on which to train these models; see below.)\n",
      "\n",
      "In addition, a third class of models will likely be developed:\n",
      "\n",
      "3. Fully automated, 'universal' models that can generate mathematical theorems and theories autonomously, merely by being directed to a mathematical result. We envision that these systems will be the true successors of automated theorem provers (ATPs) such as the Vampire [Riazanov and Voronkov, 2002, Kovács and Voronkov, 2013] or E [Schulz, 2002] ATPs. These systems have not been widely adopted by the mathematical community. While there have been attempts to imbue these systems with (non-LLM) machine learning techniques [Holden, 2021], these have not yet dramatically increased their performance. Nonetheless, once general-purpose models have been established, it is conceivable one could put these in automation loops to obtain 'AI mathematicians'. For the domain of\n",
      "\n",
      "2 https://www.reddit.com/r/math/comments/14p6j5c/terence\\_tao\\_on\\_using\\_gpt4\\_to\\_help\\_with\\_math/\n",
      "\n",
      "3 https://huggingface.co/AI-MO/NuminaMath-7B-TIR\n",
      "\n",
      "AI, this has already been recently investigated in the form of an 'AI Scientist' [Lu et al., 2024a] that autonomously generated machine learning research articles. These are systems that conceivably will require minimal human input, merely guiding them towards certain theorems that one wants to see analyzed (proved, refuted, with comments on whether slightly different variations of a theorem, with slightly altered hypotheses, admit a proof).\n",
      "\n",
      "We focus in this article on general purpose models , as outlined above, which are mathematical copilots . LLMs are currently the systems that hold the most promise to become, given a better data foundation, general purpose models. Fully automated, universal models are currently out of reach, as they are contingent on the former category of models. These are the systems that are ultimately sought when one speaks of 'AI Mathematicians' [Bengio and Malkin, 2024]. However, without the proper datasets, it is unlikely that these will come into existence. Hence, our focus is on the data that is needed to arrive at general-purpose models, the mathematical copilots . We have deliberately avoided drawing a hard boundary among the properties a model must fulfill to be placed in one of these three because the boundaries will be fluid, as the levels of automation and required human interaction are continuous scales.\n",
      "\n",
      "Currently, the existing datasets only support highly specialized models and do not put LLMs on a trajectory to achieve general-purpose models. In this article, we highlight the limitations of current datasets (Section 3, which is retrospective) and necessary changes in terms of data that will be needed to train the next generation of LLMs that approximate what we outlined general-purpose models to be (Section 4, which is prospective).\n",
      "\n",
      "Several general principles have emerged from decades of machine learning research that shape expectations about AI progress. Perhaps the most influential is Sutton's 'bitter lesson' [Sutton, 2019], which observes that general methods leveraging computation (particularly search and learning) consistently outperform approaches that encode human domain knowledge. More recently, researchers have argued that openendedness - the capacity for systems to continually generate novel, learnable artifacts - is essential for artificial superhuman intelligence [Hughes et al., 2024]. However, there are reasons to believe these principles may not straightforwardly transfer to mathematical AI. Mathematics demands logically rigorous proofs that must be verifiable, not merely plausible; pattern matching and statistical regularities, which suffice in domains like vision or game-playing, are insufficient for mathematical reasoning. Moreover, the 'bitter lesson' presupposes abundant data that scales with computation, yet high-quality mathematical data, particularly at research level, is scarce, and much mathematical knowledge is implicit in workflows and heuristics that textbooks do not record. Open-endedness in mathematics requires more than generating novel outputs: it requires outputs that are correct and meaningful within a precise logical framework. These considerations suggest that progress in AI for mathematics may require domain-specific innovations in data, evaluation, and methodology that go beyond what has succeeded elsewhere in machine learning.\n",
      "\n",
      "The limits of end-to-end generation for mathematics. A recurring theme in machine learning is that end-to-end approaches - where models learn directly from inputs to outputs without explicit intermediate structure - often outperform systems with hand-designed pipelines [Huang et al., 2024]. However, we argue that for mathematical reasoning, pure end-to-end generation must be abandoned. Mathematics demands proofs that are not merely plausible but logically valid, and errors in intermediate steps propagate to invalidate entire arguments. Recent reasoning models address this by incorporating explicit chain-of-thought processes and, in formal settings, by interacting with proof assistants that verify each step [First et al., 2023, Song et al., 2024a]. The success of these hybrid approaches, which combine neural generation with symbolic verification, suggests that mathematical AI requires structured reasoning modes rather than monolithic sequence-to-sequence generation.\n",
      "\n",
      "Our main reader audience consists of machine learning researchers, as we intend this article to raise awareness of these issues. Automated theorem provers (ATPs) and interactive theorem provers (ITPs), which have a decades-old history [Harrison et al., 2014], also had the ambition of realizing the goal of implementing mathematical copilots. Unfortunately, history has shown that if the computer science community and the mathematical community do not operate in lockstep, there is a risk of obtaining systems that will not be of\n",
      "\n",
      "interest to practicing research mathematicians; maybe controversially, Blanchette et al. [2012] mentions that certain efforts to make these systems more user-friendly have perhaps reached a 'plateau'. (Nonetheless, ATPs and ITPs have, in other domains, contributed significantly to driving the field of software automation forward.)\n",
      "\n",
      "One of the goals of this paper is to make sure that the renewed effort by computer scientists and machine learning researchers to provide mathematicians with tools that make their trade easier this time is on a track to converge to the needs of mathematicians. We hope that one of the first manifestations of this will be in the form of datasets that better capture the various rich facets of mathematical research so LLM creators can provide scores on such benchmarks that are more informative for mathematicians regarding how useful the LLM is for actual daily mathematical practice. While the most-used datasets, GSM8K and MATH, are indicative of LLMs' reasoning abilities, they are largely irrelevant to mathematical practice.\n",
      "\n",
      "## 2 An Overview of Data-Related Issues\n",
      "\n",
      "Without using tools specialized for mathematics, the current general models, LLMs, simply by well-designed training methodologies and trained on mathematical data, have demonstrated unprecedented capabilities in generating humanlike mathematical text, solving complex problems, and even engaging in creative problemsolving. For example, GPT-4 has performed promisingly on undergraduate-level mathematics formulated in natural language, as pointed out by Frieder et al. [2023a]. Math-Specialized Gemini 1.5 Pro [Reid et al., 2024], a commercial model by Google not available to the public, has been reported to have an accuracy of over 90% on the MATH dataset [Hendrycks et al., 2021], one of the most widely-used datasets for testing mathematical ability. This score has recently been replicated by an open-weight model, QwQ. 4 Attaining such a high score is equivalent to achieving the ability of an IMO gold medallist (according to [Hendrycks et al., 2021]). However, to date, no reports have been made where the mathematical benchmarks of QwQ are tested for contamination; not undertaking tests may lead to this score not being reproducible on a different dataset from the same difficulty and problem type distribution [Yang et al., 2023, Xu et al., 2024].\n",
      "\n",
      "Benchmark saturation on MATH. Recent reasoning models approach or exceed 90% accuracy on the MATH benchmark [Hendrycks et al., 2021], which was considered extremely challenging at introduction (models scored below 7%). On MATH -500, a commonly used 500-problem subset, DeepSeek-R1 reports 97.3% DeepSeek-AI [2025] and QwQ-32B-Preview reports 90.6% Qwen Team [2024]. Earlier models scored lower: Gemini 1.5 Pro achieved 77.9% on full MATH with majority voting Reid et al. [2024]. This rapid progression leaves limited headroom under boxed-answer scoring and motivates harder benchmarks like FrontierMath [Glazer et al., 2024].\n",
      "\n",
      "DeepTheorem reports a large corpus of informal IMO-style theorems paired with proofs, and uses entailed and contradictory variants to provide a binary reward signal for reinforcement learning in informal theorem proving [Zhang et al., 2025a]. Notably, this is a step beyond standard boxed-answer scoring: the supervision and evaluation are tied to logical consistency across theorem variants rather than only matching a numeric final answer.\n",
      "\n",
      "These benchmark improvements do not, by themselves, resolve the evaluation and data problems that matter for mathematical copilots. First, high accuracy under boxed-answer scoring (still common in public benchmarks) leaves ambiguity about what changed, since multiple failure modes in the reasoning trace can be invisible once only the final answer is checked. Second, evidence from out-of-distribution tests suggests that strong benchmark scores can coexist with brittle reasoning on simple variants of problems [Nezhurina et al., 2024]. Third, as scores rise, contamination and memorization become harder to rule out with simple string-based decontamination, especially when mathematically equivalent problems can be phrased in many surface forms [Xu et al., 2024]. These points motivate evaluation sets with stronger verification and a shift toward datasets that expose intermediate structure.\n",
      "\n",
      "4 https://qwenlm.github.io/blog/qwq-32b-preview/\n",
      "\n",
      "Further notable general-purpose LLMs are open-weight models with strong reported performance on baseline datasets, such as MATH and GSM8K [Cobbe et al., 2021], the DeepSeek family of models [Shao et al., 2024, Liu et al., 2024, Zhu et al., 2024, Xin et al., 2024], and the Qwen family of models [Bai et al., 2023, Yang et al., 2024a, Hui et al., 2024, Yang et al., 2024b]. Other LLMs focus more on specific abilities such as MathPrompter [Imani et al., 2023], which associates a confidence value to arithmetic problem solutions, MathVista [Lu et al., 2024b], which solves geometric reasoning problems, WizardMath [Luo et al., 2023], or Llemma [Azerbayev et al., 2024], which was specialized through further pretraining on a math corpus, and is both open-weight and open-source. A special use case where LLMs have found success is in interactive theorem provers (ITPs) such as Isabelle [Nipkow et al., 2002] or Lean [de Moura and Ullrich, 2021], where they are used to prove theorems most prominently in the context of generating proofs of given formal statements [First et al., 2023, Zheng et al., 2023, Wang et al., 2024a], autoformalization [Szegedy, 2020, Jiang et al., 2023], or providing code snippets of formal mathematics [Song et al., 2024a]. For a survey of deep learning approaches on both formal and informal (natural language) mathematics, we refer to Lu et al. [2023], and for more recent information on language models, including their performance on mainstream datasets, we refer to Zhao et al. [2023].\n",
      "\n",
      "This paper argues that the current dataset landscape does not support the advancement of such generalpurpose AI systems, in particular LLMs, towards a level of mathematical performance that makes them usable as daily mathematical thought partners [Collins et al., 2024b] that help mathematicians push the boundaries of what is known, and capture the richness of mathematical reasoning and invention that human mathematical minds are capable of [Zhang et al., 2023a, Dehaene, 2011, Feigenson et al., 2004]. Our focus thus includes, in particular, research-level mathematics, which involves many steps beyond deriving or formalizing proofs - the aspects of mathematical practice that are currently most strongly represented in data.\n",
      "\n",
      "The spectrum of tasks a copilot can assist with is not solely related to mathematics: programmers presently carry out programming tasks, such as fixing bugs or other GitHub issues [Jimenez et al., 2024], using various code-generating copilots, such as the GitHub Copilot [Chen et al., 2021]; the emerging Lean Copilot ecosystem [Yang et al., 2024c, Song et al., 2024b] assists users in the task of formalizing a mathematical theorem; Tutor CoPilot [Wang et al., 2024b] provides real-time guidance to human tutors that teach school students mathematics at the level of math word problems.\n",
      "\n",
      "Limits of relying on a single formal system. Interactive theorem provers such as Lean provide machinecheckable correctness, and stronger automation inside these systems is valuable. However, there are limits to what is natural to express and maintain inside a single formal foundation and library, even when the underlying mathematics is standard. The following non-exhaustive examples illustrate why a mathematical copilot cannot be reduced to a Lean-only workflow.\n",
      "\n",
      "First, formal developments can incur large overhead when a concept has many surface variants. Patrick Massot has pointed out this issue using the elementary calculus notion of a limit: a naive approach that treats each variant of 'limit at a point' as a distinct definition leads to a combinatorial explosion in the number of definitions and routine plumbing lemmas (his talk gives concrete counts and examples) [Massot, 2022]. In practice, mathematicians avoid learning and manipulating hundreds of separate limit definitions by teaching an abstract generator behind them, or by moving to a more uniform formalism (for example, ultrafilter-based formulations). Both approaches can be implemented formally, but they change what is convenient to teach, state, and reuse.\n",
      "\n",
      "Second, some common mathematical activities are more naturally carried out in informal language. Concise proof exposition, proof summarization, and 'zooming out' to communicate the core idea of a long argument are straightforward in natural language, while formal proofs typically expose low-level structure. Producing readable summaries from formal developments is a separate task that requires its own representations and data.\n",
      "\n",
      "Third, conjecturing and exploration tend to happen before the final formal statement is stable. Early-stage work often involves proposing definitions, testing variants, and revising hypotheses. Doing all of this directly\n",
      "\n",
      "in Lean can add technical overhead at a point where the main goal is to move quickly through candidate formulations.\n",
      "\n",
      "Fourth, the choice of foundations affects what counts as 'the same' object. For example, work in homotopy type theory was driven in part by a refined analysis of equality and equivalence. Informal mathematics routinely treats objects as equal 'at the level that matters' in a given theory, even when they are not definitional equalities in a fixed foundational encoding. Formal developments must commit to a specific notion of equality and transport, and that commitment can make some styles of reasoning less direct.\n",
      "\n",
      "In the course of proving a theorem, one may need to convert between more abstract mathematics and less abstract mathematics. Flexible conversion might prove to be more difficult in Lean.\n",
      "\n",
      "These limitations do not argue against formalization. They suggest that Lean is one tool in a broader toolbox, and that mathematical copilots will likely need a federation of tools and representations, including automation over informal mathematics, to support rapid iteration in research workflows.\n",
      "\n",
      "Mathematics, with its rich set of domain-dependent workflows, tools, and modes of thinking, requires copilots with exceptionally broad capabilities. A mathematical copilot that is useful to a mathematician must significantly exceed the capabilities of the three exemplary copilots mentioned above, which are systems that are designed to help with a narrow technical set of tasks.\n",
      "\n",
      "Our paper is split into three main sections : In the ensuing Section 3, we review past datasets that may have limitations of various kinds. We split these into two further categories, relating to natural language and formal language datasets.\n",
      "\n",
      "Training corpora vs benchmarks. In this paper, the term dataset covers two use cases: (i) corpora used for pretraining, fine-tuning, or reinforcement learning, and (ii) held-out benchmarks used to report model performance. Some collections are used in both ways. Benchmarks are also sometimes converted into training data, for example by creating instruction-style variants of benchmark problems. As a result, many of the issues discussed below apply to both training and benchmarking, but they affect different parts of the pipeline. Difficulty calibration and interaction-mode coverage affect the supervision signal during training and the amount of headroom in a benchmark. Contamination affects both use cases: benchmark items can appear in training corpora, and benchmark scores can be inflated when test problems, or close variants, leak into training data. Other issues depend more strongly on the use case. Binary scoring and the evaluation protocol determine what a benchmark score measures. For formal mathematics, placeholders ( sorry , Admitted ), duplication across alternative formalizations, and toolchain provenance affect both the quality of training data and the interpretation of evaluation results. Table 2 summarizes these distinctions.\n",
      "\n",
      "To give an overview of our contributions in that section, current natural language datasets typically model a question-response interaction, such as posing a problem and writing a solution. We identify dimensions where such datasets frequently fall short of being able to assess advanced mathematics:\n",
      "\n",
      "- Difficulty : Most datasets are situated on a level of difficulty at or below the level of an undergraduate degree in mathematics, or impossibly hard, with few datasets at an appropriate level of difficulty, from which model creators can better learn about the strengths and weaknesses of their system.\n",
      "- Binary evaluation : Benchmarks typically only support assessing if the output was correct or incorrect, but in case of errors, do not provide information about how the output failed to be correct, which is crucial to support.\n",
      "- Lack of trusted automation evaluation : Benchmarks rely on checking a final answer (generally, by keyword matching), imperfect model-based evaluation, or provide no automatic evaluation at all.\n",
      "- Standardized Interaction Modes : Often, natural language datasets only contain question-answer mathematical problems. The daily experience of a mathematician is much richer than that [Zhang et al., 2023a], since to prove/search for a theorem, all kinds of 'lateral' modes of thinking have to be applied.\n",
      "\n",
      "Examples are intermediate conjecture formulation, (counter)example search, and finding analogies with other theorems. These are typically not found in textbooks, so there is no representation of these in terms of data. We point to Collins et al. [2024a] for an exposition of the problems of static evaluation for natural-language mathematics problems in practice. In short, static evaluation relies on fixed input-output pairs that fail to capture iterative or adaptive interactions, such as clarifying questions and corrections, that are an integral part of effective problem solving.\n",
      "\n",
      "- Contamination : Many datasets released solely for evaluation and benchmarking purposes have a short half-life, as, once released, they are easily leaked into datasets that will be used as training data for future models.\n",
      "- (Binned) Sankey Diagrams : Most benchmarks are snapshots of current models, and don't record clearly how performance is changing. This makes it hard to assess the broader direction of evolution of language models w.r.t. mathematics. (Binned) Sankey Diagram offer an easy remedy, but have not been embraced by the community.\n",
      "\n",
      "Several of these issues are also present for formal mathematics. But benchmarks around formal mathematics frequently suffer from distinct issues than those listed above; for instance, while datasets for formal mathematics come at a range of difficulties, from competition-level problems [Zheng et al., 2022] to undergraduate textbooks [Azerbayev et al., 2023], to some results that are at the mathematical state-of-the-art [Scholze, 2022, Bordg et al., 2022], formalized mathematics evaluation can be easily carried out in a safe and automated manner since proofs can be checked by the ITP, rendering the issue of trusted automatic evaluation irrelevant. On the other hand, the following dimensions are problematic for datasets related to formal mathematics:\n",
      "\n",
      "- Tool misalignment : Various automation tools exist that aid in proving small, intermediate lemmas arising in the formalization of more comprehensive results. This creates incentives to change a proof's structure to maximize the use of these tools, potentially resulting in less readable and 'human looking' proofs. This misalignment often manifests as proof-script scaffolding, such as broad tactics applied repeatedly, large automation argument lists, or redundant steps that make proofs brittle and non-idiomatic. Since such redundancy is frequently detectable by linters (e.g. unused simp arguments), it can be treated as dataset metadata in corpora that include a raw view for realism along with a linter-cleaned view that better reflects canonical proof structure.\n",
      "- Non-trivial data duplication : In mathematics, it is common that one theoretical concept has several formal representations. For example, the concept of a graph can be described formally in multiple ways. Formal libraries typically feature a variety of representations for the same mathematical concept, some being almost exact copies of each other. Such near-duplication can lead to downstream issues when the library is both used as training and test data in machine learning approaches. Duplication also arises at the level of lemma interfaces: when a library lacks a lemma in the form needed, developers introduce thin wrapper lemmas or local variants to patch an API gap. These near-duplicates are often engineering artifacts rather than conceptual novelty, and they can inflate the overlap between training and evaluation in ways that are invisible to string-based de-duplication.\n",
      "- Standardized interaction modes : (Analogous to natural-language, but with different characteristics) Current formal benchmarks focus on proving theorems in isolation or translating individual statements into formal code. These tasks do not capture all aspects of advanced mathematical practice. In practice, formalization is dominated by interactive patterns that are mostly absent from current benchmarks, including iterative repair (compile → inspect diagnostics → patch), systematic search for lemmas or imports, and refactoring for readability and performance. For example, Lean supports interactive lemma search via exact? , apply? , and rw? , which reflect a workflow that interleaves searching and proving, rather than proof synthesis from scratch.\n",
      "- Scope of formalization and handling of incomplete proofs : Formal-math corpora vary substantially in scope and granularity . Many benchmarks emphasize short, isolated theorems or textbook-style exercises [Zheng et al., 2022, Azerbayev et al., 2023], whereas large-scale formalization projects capture\n",
      "\n",
      "long-form arguments that unfold across hundreds of intermediate lemmas before reaching a major theorem [Scholze, 2022, Bordg et al., 2022]. This matters for dataset construction and evaluation: models trained or tested only on short lemmas may appear strong while still failing to manage the dependency structure, planning, and bookkeeping required for long developments.\n",
      "\n",
      "A second, closely related issue is incompleteness during development . In Lean, unfinished results are often recorded using sorry , and in Coq using Admitted ; these placeholders accelerate library growth but may render a corpus logically incomplete (and, in the limit, inconsistent). If such items are included unmarked in training data, a model observes theorems treated as 'proven' without ever seeing a proof term, which can distort learning and confound evaluation. This phenomenon is common in actively developed libraries; one example is SciLean [Skrivan, 2025]. Consequently, formal-math datasets should explicitly document how placeholders are handled (e.g., excluded, retained with flags, or separated into a distinct split), and should report basic completeness statistics (e.g., number or fraction of theorems containing placeholders, and where they occur in dependency graphs).\n",
      "\n",
      "Finally, to better reflect real mathematical practice, there is value in long-form curated corpora where an entire paper or chapter is represented as a structured development: (1) statements, (2) complete machinechecked proofs when available, (3) dependency metadata, and (optionally) (4) high-level commentary about proof strategy and lemma roles. Even when full completion is infeasible, clearly distinguishing finished proofs from partial ones enables more meaningful training objectives (e.g., proving the remaining sorry lemmas) and more reliable benchmarks.\n",
      "\n",
      "- Version fragility and tactic stability : Formal proofs are executable artifacts whose replay can be brittle under prover or library drift. A common failure mode is tactic instability. For instance, in Lean a 'flexible' tactic such as simp changes the goal in a context-dependent way, and downstream steps silently assume a particular postsimp shape (e.g. after simplification one must start a calc chain from the actual goal state, not the expected form).\n",
      "- Hidden context dependence (imports, local instances, and attribute state) : Even with a fixed statement, the behavior of a proof script can depend on surrounding environment, including imports, local instances, and attribute state (e.g. simp lemmas). In Lean, instance 'pollution' can occur when local bindings introduce alternate instances (such as multiple MeasurableSpace structures), causing later lines to elaborate against an unintended instance unless the intended instance is explicitly pinned.\n",
      "- Resource sensitivity (heartbeats and elaboration/typeclass search) : Correctness is not cleanly separable from performance: proofs can fail due to heartbeat limits or timeouts caused by elaboration or typeclass synthesis rather than mathematical content. This is especially salient in large developments where small refactors or additional instances can cause elaboration to take much longer, possibly more than the previous heartbeat limit.\n",
      "- Redundant scaffolding and non-canonical proof artifacts : Formal corpora often contain redundant tactic steps and linter-detectable redundancy (such as unused automation arguments, no-op steps) that are harmless for one-off development but distort the signal for training and evaluation. For instance, a tactic may already close the goal, making subsequent steps ill-typed (e.g. leading to a Lean error 'no goals to be solved'). A useful dataset practice is to include both a raw corpus and a lint-cleaned canonicalized view.\n",
      "- Toolchain and automation : Many formal corpora are now partially produced by automated or semiautomated methods-ranging from autoformalization tools that attempt to translate informal text into formal proofs [Jiang et al., 2023], to specialized rewriting tactics, to code-generation scripts that unify definitions across libraries. Current datasets may not have enough detail about which automated methods were used and how their outputs were curated. A related form of provenance is linter metadata: Mathlibstyle linters flag redundant arguments, unstable patterns, and stylistic violations, and recording which lints were applied (or waived) can help downstream users understand how canonical the resulting proof scripts are. Similarly, explicit set\\_option declarations (e.g. heartbeat or recursion-depth overrides) should be treated as dataset metadata, since proofs that require non-default resource limits may behave differently\n",
      "\n",
      "under standard evaluation harnesses.\n",
      "\n",
      "- Relation of dataset to existing formal libraries : Many formal datasets build upon large existing libraries (e.g. mathlib in Lean). If a new dataset reuses or extends such a library, overlaps or version discrepancies can arise. Two projects may formalize the same theorem in slightly different ways, or rely on different versions of the library's foundational definitions. Version fragility is particularly acute for automation-heavy proofs, as changes to typeclass instance priorities, simp-lemma sets, or default tactic behavior can cause a proof that worked in one commit to fail silently or timeout in the next, even when the underlying mathematics is unchanged.\n",
      "- Unclear benchmarking protocols : Formal mathematics datasets are frequently used as benchmarks for automated theorem proving and related model-based formal reasoning. However, papers often report a single success rate without specifying the task definition and evaluation protocol in enough detail to make results comparable.\n",
      "\n",
      "At minimum, a formal-math benchmark should state which task is being evaluated. Common variants include: (i) theorem proving from scratch (given a statement and imports, produce a proof that checks), (ii) tactic prediction (given a proof state, predict the next tactic or a short tactic script), (iii) proof repair or hole filling (complete a partially written proof term or script), (iv) premise selection (select a set of lemmas from a library that suffices to close a goal), and (v) autoformalization (translate an informal statement into a formal one, optionally followed by proving it).\n",
      "\n",
      "Even within a single task label, the protocol can change difficulty by orders of magnitude. For reproducibility, benchmark authors should specify:\n",
      "\n",
      "- Input specification : what the model receives (the statement only, or also local context such as imported modules, available lemmas, and current proof state), what implicit environment is assumed (active typeclass instances, scoped attributes, and simp-lemma sets), and any context limits (number of retrieved premises, maximum file size, or maximum tokens).\n",
      "- Output specification : what counts as a valid submission (a full proof script, a proof term, a sequence of tactics, or a patch to an existing file).\n",
      "- Success criterion : whether the artifact must be accepted by the checker without placeholders (e.g., no sorry / admit ), and what counts as failure (type error, timeout, unsolved goals).\n",
      "- Tooling and automation : which tactics and automation are permitted (e.g., rewriting and simplification tactics, SMT calls, hammer-style tools), and whether tool use is counted as part of the model or treated as an external oracle. If multiple settings are relevant, separate tracks (for example, 'with automation' and 'restricted automation') avoid conflating results.\n",
      "- Resource constraints : wall-clock time per goal, memory limits, maximum number of proof attempts, and any search limits (tactic budget, backtracking depth). Limits that are internal to the prover matter as well: heartbeat budgets, typeclass-synthesis depth, and recursion limits can all cause a correct proof script to fail silently or timeout when replayed under different settings. These choices interact strongly with inference-time compute and must be reported.\n",
      "- Environment pinning : the exact proof assistant version and library commit (for example, Lean + mathlib revision, or Isabelle + AFP snapshot), since minor version changes can make proofs succeed or fail. An evaluation harness that replays proofs in a fixed environment reduces ambiguity.\n",
      "- Metrics : beyond a single success rate, report pass@ k when multiple attempts are allowed, average time to solution, and optional secondary measures such as proof length or number of tactic steps, since these affect usability.\n",
      "\n",
      "Without these details, two papers can report the same score while solving different tasks under different constraints. Explicit protocols and pinned environments enable reliable comparison across models and\n",
      "\n",
      "make it possible to interpret gains as improvements in formal reasoning rather than changes in tooling or evaluation setup.\n",
      "\n",
      "## 2.0.1 Mixed-mode datasets for tool use\n",
      "\n",
      "Many candidate mathematical copilots are not text-in, text-out systems. They are trained and evaluated in settings where a model is expected to call external tools, such as a computer algebra system, a numerical solver, a proof assistant, a retrieval system, or a specialized symbolic model. Tool-integrated systems already appear in work on tool-integrated reasoning for math (for example, models that generate and execute Python code as part of the solution process [Gou et al., 2024]).\n",
      "\n",
      "A mixed-mode dataset records these interactions explicitly. Rather than storing only a problem statement and a final response, each datapoint is a short trajectory consisting of interleaved representations, for example: (i) natural-language problem context, (ii) intermediate formal objects (expressions, goals, proof states, or formal statements), (iii) tool calls with arguments, (iv) tool outputs (for example, the result of executing code, a CAS simplification, or a proof assistant error message), and (v) a final response that integrates the tool outputs into a mathematically coherent explanation. This structure supports training objectives beyond next-token prediction, including learning when to call a tool, how to translate between representations, and how to recover from tool failures (timeouts, type errors, missing imports, or non-terminating proof searches).\n",
      "\n",
      "A special case of mixed-mode data consists of datasets pairing informal and formal mathematics. These datasets support autoformalization and naturalization, as well as theorem proving conditioned on an informal statement. Examples include miniF2F [Zheng et al., 2022] and ProofNet [Azerbayev et al., 2023]. More recent efforts scale paired informal-formal corpora to substantially larger sizes [Ying et al., 2024], which matters for training rather than only evaluation. In these settings, the tool is the proof assistant itself: correctness can be checked by verifying the generated formal artifact, while intermediate feedback takes the form of proof-state information and type errors, which can be recorded in the dataset.\n",
      "\n",
      "Mixed-mode datasets introduce evaluation and reproducibility issues that do not arise in static text-only benchmarks. Tool outputs depend on the toolchain (version, library state, and sometimes randomness). A dataset intended for benchmarking should therefore specify the tool interface and pin versions (for example, by commit hashes for proof assistant libraries), and provide an evaluation harness that can replay tool calls in a controlled environment. Without this, differences in tool versions can change whether a proof checks or whether a symbolic computation succeeds, making scores hard to compare across papers.\n",
      "\n",
      "Recommendations. For mixed-mode datasets used for training or benchmarking, we recommend (1) representing each datapoint as an explicit tool-interaction trace, including tool inputs and outputs; (2) recording the toolchain in sufficient detail to replay the trace (versions, dependencies, and execution settings); and (3) separating tasks where tool use is optional from tasks where tool use is required, so that evaluation can distinguish mathematical reasoning from tool invocation competence. Mixed-mode datasets also interact with the contamination problem: if tool outputs encode the final answer too directly, a model can learn to pattern-match on tool outputs rather than learning to select and use tools effectively. This motivates evaluation protocols that check both the final answer and the correctness of the tool trace itself (for example, by re-executing code or re-checking formal proofs).\n",
      "\n",
      "These limitations have various consequences: For example, evaluating progress of the mathematical capabilities of AI systems, particularly for advanced mathematics, is difficult. For instance, improved accuracy on a standard problem-solving dataset may not provide a meaningful signal about what has improved, while performing well on an existing formal benchmark may not accurately indicate where models currently struggle.\n",
      "\n",
      "In the next section, Section 4, we discuss how certain aspects of the proof creation can be mapped to datasets. We argue that such datasets that describe intermediate stages that mathematicians go through in the long process of devising proofs are potentially highly valuable to support mathematical copilots that assist humans in these tasks. The overt focus of current datasets on result (the final proof), rather than the process\n",
      "\n",
      "that led to that proof, severely diminishes their usefulness for training LLMs to be used as mathematical copilots. While from a perspective of pure proof search, results-based datasets are justified, it is unlikely that mathematics will soon be reduced purely to proof search, as there are several mathematical activities (such as finding the 'right' set of definitions, that makes a mathematical theory clear and easy to follow) that are not solely about proof search. Such datasets that describe these intermediate stages do not exist at all currently! The GHOSTS dataset [Frieder et al., 2023a], to our knowledge, is the only one that preliminarily investigates how well LLMs can engage in literature search. Thus, in Section 4.1, we take a more holistic level to describe how mathematical workflow might be described by datasets. This problem is essential, as a lot of mathematical activity is based on chaining together complex workflows. There is currently no clear way of measuring the advances of AI systems in tackling the full spectrum of mathematical workflows. Such data about intermediate stages in the proof discovery process is often not explicitly contained in textbooks and articles - the data sources on which current LLMs are trained. In Section 4.3, novel ways of capturing data that may contain this information are described. Finally, in Section 4.4, we discuss a stricter proof structure called a 'motivated proof', requiring the author to explain the origin behind each step of a proof. We then argue for an evaluation benchmark enforcing this proof structure from the perspectives of end-user utility and evidence of LLM reasoning rather than LLM memorization (coming up with a motivated proof, given an unmotivated one, requires reasoning and deeper proof understanding) and speculate on paths toward designing such a benchmark.\n",
      "\n",
      "We note that in this article we do not focus on datasets that are designed primarily to be consumed by the current LLM technology stack, for example, various instruction tuning datasets, such as the OpenMathInstruct dataset [Toshniwal et al., 2024]. These are often based on existing datasets (GSM8K and MATH in the case of OpenMathInstruct) and thus could be described as derived datasets. Rather, our focus is on assessing how much and how well primary datasets describe various aspects of doing mathematics .\n",
      "\n",
      "## 3 The Past - Common Pitfalls in Current Datasets\n",
      "\n",
      "In this section, we discuss several issues with current datasets. Note that our limitations are with respect to the landscape of AI for mathematics at the time of writing (Dec 2024); it is possible that some of these limitations are addressed by the time the reader reads this article. Indeed, we hope that they are addressed swiftly.\n",
      "\n",
      "Underlying several of these problems is a dynamic described by Goodhart's law: 'When a measure becomes a target, it ceases to be a good measure.' A related formulation, Campbell's law [Campbell, 1979], warns that quantitative indicators used for decision-making become increasingly subject to corruption pressures. In the context of mathematical AI, when LLM developers optimize for specific benchmark scores (such as accuracy on MATH or GSM8K), models may learn to exploit superficial patterns or benefit from data contamination rather than developing robust mathematical reasoning. High benchmark scores then no longer reliably indicate the capabilities that matter for mathematical copilots. This motivates our call for richer evaluation frameworks that are harder to 'game' and that measure diverse facets of mathematical competence. Notably, the NLP community identified these same issues years ago and proposed solutions such as dynamic, adversarial benchmark creation [Kiela et al., 2021], yet the math-AI community has largely continued to rely on static benchmarks susceptible to the same failure modes.\n",
      "\n",
      "Recent near-saturation results on boxed-answer benchmarks [DeepSeek-AI, 2025, Qwen Team, 2024] are consistent with this dynamic: as optimization concentrates on a small set of public targets, benchmark scores become less informative about broader mathematical competence.\n",
      "\n",
      "We first list issues that are common to both natural-language datasets and datasets comprising formalized mathematics. Then, we list issues that are specific to either natural-language datasets or formal datasets.\n",
      "\n",
      "## 3.1 Common Issues\n",
      "\n",
      "## 3.1.1 Difficulty\n",
      "\n",
      "The difficulty of mathematical problems typically varies along at least two salient dimensions: level of abstraction and proof (or solution) sophistication. Abstraction varies from simple 'one-layer' definitions, such as those that involve properties of topological spaces, to those in areas such as algebraic geometry, where definitions of highly intricate mathematical objects involve several layers.\n",
      "\n",
      "Proof sophistication is related to problem-solving ability and is often tested (for humans and AI systems alike) in mathematical competitions, such as the International Mathematical Olympiad (IMO). Typical for these is that the statements to be proved are elementary, but highly ingenious proof techniques need to be used to arrive at the solution.\n",
      "\n",
      "Several datasets have been proposed which explore proof sophistication, such as the OlympiadBench [He et al., 2024], the IMO Small Challenge, or the IMO-AG-30 dataset [Trinh et al., 2024], but very few existing datasets reach a level of difficulty that is more advanced than that of an undergraduate degree in mathematics in terms of level of abstraction. For a (non-exhaustive) overview of datasets, ordered by their difficulty, see Table 3.\n",
      "\n",
      "For example, MATH focuses on high-school competition problems, while the recent TheoremQA [Chen et al., 2023] and Advanced Reasoning Benchmark (ARB) [Sawada et al., 2023] are upper-undergraduate level. Many commonly used datasets test mathematics below the undergraduate level, such as GSM8k and the 23 tasks in the Lila benchmark [Mishra et al., 2022b].\n",
      "\n",
      "Two datasets that go beyond upper-undergraduate level are the GHOSTS dataset [Frieder et al., 2023c], as well as the NaturalProofs(-Gen) [Welleck et al., 2021, 2022] datasets, which contain problems, theorems, and proofs on a wide range of more advanced mathematical topics. However, these suffer from a lack of automated evaluation and contain many problems that could, in principle, be in the training corpora of modern LLMs, as we discuss later. FrontierMath [Glazer et al., 2024] is also a more advanced dataset, but is currently not publicly accessible. STORM-BORN is a recent derivation-style benchmark curated from recent research papers; it reports that strong models solve only a small fraction of the selected problems without task-specific training [Liu et al., 2025].\n",
      "\n",
      "Most current LLMS (e.g., the Qwen [Yang et al., 2024a] family of models, or commercial models released by OpenAI and Google) are evaluated on the MATH dataset [Hendrycks et al., 2021] and the GSM8K dataset [Cobbe et al., 2021], with a recent trend of some LLMs using held-out datasets such as the Hungarian National Finals high-school exam as an additional test of generalization [Paster, 2023]. The MATH dataset uses five categories of difficulty and is strictly more difficult than GSM8K. Nonetheless, state-of-the-art LLMs have been found to reach an acceptable performance on it [Frieder et al., 2023a]. For instance, Gemini reaches a performance of about 50%. 5 , while solving almost all problems on GSM8K. Progress in LLMs is proceeding at a rapid pace, and the 'Math-Specialized' version of Gemini 1.5 reaches (at best) a score of 91% on MATH, which is representative of the current state of the art (although, due to the closed nature of this line of research, it is difficult to place this achievement into context, such as by analyzing how similar training data has been to the MATH dataset, and how much was needed).\n",
      "\n",
      "While this performance may appear impressive, neither of these datasets covers a high degree of difficulty in terms of abstractness and proof sophistication. Hence, the current performance of LLMs on these datasets is not yet indicative of advanced mathematical understanding, which is relevant for professional mathematicians.\n",
      "\n",
      "This is particularly striking in the case of AlphaGeometry, which can solve 25 geometry problems from the IMO out of a test set of 30 problems [Trinh et al., 2024] (and 213 out of 231 on a larger benchmark). We argue that the accompanying dataset, while indeed difficult in terms of proof sophistication, is not sufficiently large to be able to accurately compare AlphaGeometry with other models.\n",
      "\n",
      "5 https://blog.google/technology/ai/google-gemini-ai/performance\n",
      "\n",
      "Moreover, recent work casts doubt on whether high benchmark scores reflect genuine mathematical reasoning. Nezhurina et al. [2024] demonstrate that GPT-4, Claude 3 Opus, and other state-of-the-art LLMs at the time, exhibit dramatic reasoning failures on simple common-sense math problems (the 'Alice in Wonderland' or AIW problems) that are trivially solvable by humans. These failures persist even with chain-of-thought prompting and multi-step re-evaluation, and models express strong overconfidence in their incorrect answers. If LLMs struggle with such elementary problems, it is difficult to credit their high performance on substantially harder benchmarks like MATH to genuine reasoning rather than to memorization or pattern-matching on familiar problem types. Proof or Bluff? reports expert grading of model solutions on the 2025 USAMO and finds low proof scores even for models that score well on short-answer benchmarks [Petrov et al., 2025]. This tension motivates two directions: first, the development of out-of-distribution evaluation sets - for instance, via procedural generation of novel problems - that are less susceptible to memorization; and second, decontamination methods tailored specifically to mathematics, which must account for the fact that mathematically equivalent problems can appear syntactically different.\n",
      "\n",
      "Recommendations. We believe that it is crucial to introduce more difficult datasets into the evaluation repertoire of math-AI. And importantly, the skill level of these datasets should be graded , such that we can form a nuanced characterization of model capabilities - this can be achieved, in part, by interpolating difficulty level between existing datasets and those representative of advanced mathematics. This includes datasets on graduate-level mathematical domains, such as functional analysis (to measure handling of arguments involving high levels of abstraction), partial differential equations (to measure handling of intricate computational arguments), as well as other domains that come with their own 'mathematical flavor'. At the same time, more comprehensive datasets illustrating and measuring advanced problem-solving skills are needed. The miniF2F [Zheng et al., 2022] dataset and the GHOSTS dataset are just the first steps in this direction. For natural language datasets, we note that simply scraping many competition-level problem sets, coupled with an evaluation principle that only verifies a 'final answer', is not sufficient to ensure that systems can progress to becoming useable mathematical copilots. Such problem sets, even if some of them ultimately are only math word problems, require expert, specialized knowledge and training for humans to be proficient. Without a detailed evaluation protocol (see Section 3.1.2), not much insight will be gained into why poor performance was obtained - and, conversely, human evaluation will be very costly (see Section 3.2.1).\n",
      "\n",
      "## 3.1.2 Binary Benchmarks\n",
      "\n",
      "Most datasets tailored for machine learning, except GHOSTS and NaturalProofs-Gen, including the dataset for AlphaGeometry, use a simple correct-incorrect rating scheme. The given feedback indicates if a system fails to solve a problem correctly, but without any indication of how it fails. Binary evaluation thus provides limited information on how to improve or interpret performance.\n",
      "\n",
      "Benchmark diagnosticity and 'training on the test task.' A further limitation of many mathematics benchmarks is that they offer little insight into why a model succeeded or failed, beyond a single correctness bit. In practice, this can make it hard to improve a system other than by training on large quantities of data that is closely matched to the benchmark task distribution. Dominguez-Olmedo et al. [Dominguez-Olmedo et al., 2024] describe this as training on the test task : unlike leakage or contamination, it is a legitimate use of knowledge about benchmark tasks at training time, but it can confound both relative model comparisons and claims about emergent abilities. In mathematics, the problem is amplified by the concentration of benchmarks on a small set of task formats; Frieder et al. [Frieder et al., 2023a] describe a 'vast unpopulated landscape of benchmarks' beyond the few overstudied datasets. One response is to make benchmarks more diagnostic by attaching structured feedback to failures, such as error codes (as in GHOSTS) that distinguish common mathematical failure modes, which can guide targeted data curation and model improvements rather than generic task-matching fine-tuning.\n",
      "\n",
      "GHOSTS and NaturalProofs-Gen address this problem by using labels that are attached to language model outputs, which provide a classification of (mathematical) errors. For instance, GHOSTS distinguishes two\n",
      "\n",
      "types of labels, called 'error codes' and 'warning codes', which make up, in total, 15 error or warning labels, while NaturalProofs-Gen distinguishes between 11 types of errors. A small number of datasets use non-binary error rating schemes, to our knowledge these are only TheoremQA (3-point scheme) and ARB (5-point).\n",
      "\n",
      "Finally, formalized mathematics offers feedback from the interactive theorem prover (ITP) when a system fails to produce a correct proof (one case being timeouts). However, this feedback is more akin to a compiler error than high-level feedback on the mathematical reasoning process. We speculate that this feedback may be useful but still limited in its ability to diagnose AI systems.\n",
      "\n",
      "Recommendations. We believe that it is crucial to develop datasets that support multi-aspect feedback. Relying on a single numerical representation, in the face of the diversity of mathematical failure modes, risks providing a signal that is too weak to be suitable for learning or evaluation - which is especially important to guard against when we consider deploying systems alongside people, who may care not only that a model is correct but also that the response is appropriately helpful [Collins et al., 2024a].\n",
      "\n",
      "One of the reasons that AI researchers may have avoided multi-aspect feedback, particularly subjective feedback, is for fear of the difficulty of obtaining consistent human annotations. While annotator disagreement is a challenge, we suggest that the math-AI community look to the ways in which computer vision researchers have worked on developing new theoretical and empirical tools which lean in to the diversity of human responses [Uma et al., 2021, Collins et al., 2022, Sucholutsky et al., 2023, Gordon et al., 2022]. Appropriately interleaving formality and subjectivity - especially for advanced mathematics, where there is a limited pool of possible human annotators - is a challenge, but one which demands cross-disciplinary collaboration and is ripe for future work. As a middle-ground, we also recommend considering evaluation methods that do not assign numerical ratings at all - but rather annotate the outputs of AI systems with error codes and warning codes, then derive ratings from the codes directly (where no error codes mean a perfect rating).\n",
      "\n",
      "## 3.1.3 Standardized Interaction Modes\n",
      "\n",
      "Almost all datasets to date that are designed to evaluate LLMs capture only textbook-like questions. Moreover, owing to issues of automatic evaluation described in Section 3.2.1, the majority of datasets focus on a smaller subset of questions with a unique, typically numerical, answer. While such questions might span a range of different topics, from numerical (e.g., GSM8K, problems of level 1-2 from MATH, MultArith) and symbolic evaluations (the dataset associated with the work of [Lample and Charton, 2020]) to somewhat complex problem-solving tasks (problems of level 4-5 from MATH), they do not come close to covering all aspects of the mathematical tasks a mathematician might encounter in their daily practice. We believe that any future mathematical assistant should provide support for these tasks.\n",
      "\n",
      "In the following list, we describe different dimensions of interaction modes for natural-language mathematics (both question types as well as session types), across which we believe that mathematical assistants should be evaluated.\n",
      "\n",
      "## Question Types.\n",
      "\n",
      "1. School/University Curriculum-Like Questions that are encountered in educational settings and have well-defined answers. This includes questions with arithmetic, symbolic content, math word problems, as well as problem-solving and proofs.\n",
      "2. Proof-Specific Questions as the concept of a mathematical proof is varied and gives rise to a host of specific questions about proofs, which go beyond the type of questions related to proofs from the previous point. These are advanced questions concerning matters such as: Establishing whether a proof is effective or not, in the sense of proving a general statement about a mathematical object merely by proving its existence as opposed to producing a witness or an algorithm that computes; investigating what distinct proof strategies might be used to prove a certain statement; exploring the distinction between whether an 'elementary' or an 'advanced' proof is given. For example, the prime number\n",
      "\n",
      "theorem has classical complex-analytic proofs (via properties of the Riemann zeta function) [Hadamard, 1896] as well as later 'elementary' proofs [Selberg, 1949, Erdős, 1949].\n",
      "\n",
      "3. Pictorial representation. Many mathematical objects and arguments have standard diagrammatic encodings that support reasoning and communication. Examples include plotting functions to inspect qualitative behavior (monotonicity, convexity, asymptotics), diagram-based proofs and explanations (for example, 'proofs without words' collections [Nelsen, 1993]), and commutative or string diagrams used in areas such as algebra, topology, and category theory [Selinger, 2011]. A mathematical copilot that supports research practice should therefore be evaluated on tasks that translate between symbolic or textual descriptions and graphical representations, and on tasks that interpret diagrams to extract relevant properties or propose next steps in an argument.\n",
      "4. Mathematical explanations and intuition , e.g., by providing (counter-)examples. Such questions are open-ended and have potentially (infinitely) many correct answers. This freeform exploration is crucial in the process of establishing the truth value of a given (advanced) mathematical statement, where one swings between attempting a direct proof of the statement and finding a counterexample to the statement. Terence Tao has recounted encountering a counterintuitive statement in compressed sensing, first attempting to construct a counterexample, and then redirecting that effort into a proof [Bernstein, 2019].\n",
      "\n",
      "One recent counterexample-focused benchmark is CounterMATH, which asks models to refute undergraduatelevel statements by producing explicit counterexamples (1,216 statement-rationale pairs) [Li et al., 2025a].\n",
      "\n",
      "5. Proficiency in using mathematical tools , where the assistant is given access to external tools such as computer algebra systems (SageMath, Mathematica, Maple), numerical solvers, and domain-specific packages (for example, Singular, GAP, SnapPy), and is evaluated on four steps: selecting an appropriate tool, translating the mathematical intent into the tool's input language, validating and interpreting the tool's output, and composing multiple tool calls into a coherent mathematical argument. Tool-integrated reasoning approaches already use this pattern in restricted form (for example, delegating substeps to executable code) [Gou et al., 2024]. This also clarifies why Lean alone does not cover the full workflow of research mathematics: Lean can verify formal proofs, but many routine sub-tasks (symbolic simplification, numeric exploration, combinatorial search, heuristic experimentation) are more naturally handled by specialized systems. A practical copilot therefore needs a federation of tools, each with its own interface and notion of correctness, and datasets should record tool traces so that evaluation can replay the same sequence of calls.\n",
      "6. Retrieval Tasks for definitions and mathematical facts, either from name to statement or reverse ('mathematical search engine'). MIRB provides a benchmark suite for mathematical retrieval, including premise retrieval and formula retrieval, which can serve as components of literature-search workflows [Ju and Dong, 2025]. Retrieval tasks can range from a well-defined answer (e.g., ' Define this mathematical object! ') to open-ended questions (' what are the most important theorems in a certain field?; which results are relevant to prove a certain statement?; has a form of this statement already been proved? '). This tests the capabilities of mathematical assistants to provide high-level explanations similar to those of a human expert in a certain mathematical field.\n",
      "7. Informal Proof Completion , where a proof (or generally a mathematical statement) needs to be checked or completed. In the best case, the mathematical assistant should be able to provide a correction and fill in gaps. In such functionality, mathematical assistants can be running in the background to act as 'copilots' and 'mathematical grammar-checkers' - as the current generation of copilots 6 does successfully for programmers.\n",
      "8. Mixed-mode interactions , where translation between formal and informal mathematics needs to be\n",
      "\n",
      "6 Such as GitHub's Copilot: https://github.com/features/copilot\n",
      "\n",
      "performed. These are particularly important for the combination of ITPs and general-purpose LLMs, which has recently picked up interest (e.g., [Jiang et al., 2023, First et al., 2023, Frieder et al., 2023c]).\n",
      "\n",
      "To our knowledge, GHOSTS is the only natural-language dataset that focuses on all of the first four items from above. Accompanying this variety of questions ( what is asked) are session types that describe how the questions are posed.\n",
      "\n",
      "## Session Types.\n",
      "\n",
      "1. Zero-Shot , where the model is prompted without further information.\n",
      "2. In-Context Learning , where additional background information is provided, ranging from clarifications of the notation to context (e.g., the chapter of the book or paper), to similar questions with corresponding proof.\n",
      "3. Interactive , where the prompter (whose mathematical abilities can span a wide spectrum) solves mathematical questions together with the language model in an interactive way as in Collins et al. [2024a].\n",
      "\n",
      "In practice, typical mathematical workflows require a mixture of different question and session types. Even if a question is not open-ended, the amount of mathematical detail and rigor the answer should contain will depend on the user's background. In part, this should be deduced from the way the question is phrased. Currently, no datasets offer support for this. We note that with the exception of Collins et al. [2024a] and the collection of CheckMate, almost all datasets are zero-shot. Interactive training modes are extensively employed in a formal language setup. IMProofBench evaluates research-level problems in an agentic setting with tools (web search and computer algebra), combining automated scoring on subproblems with expert grading of full proofs [Schmitt et al., 2025].\n",
      "\n",
      "For formal mathematics, different considerations apply, as outlined below. While modern LLMs can, to a degree, perform the tasks below, they are not specialized for these tasks:\n",
      "\n",
      "- Refactoring formal mathematics to yield nicer-looking proof is a significant undertaking. A tool that takes as input a formal proof and refactors it along desired criteria would be a welcome addition to the toolbox of researchers working in formalizing mathematics. To our knowledge, the only work to date that attempts this is the ImProver [Ahuja et al., 2024] for Lean, which allows formal proof creation of specific length.\n",
      "- Interoperability between different libraries of formal proofs (such as Lean's Mathlib library and Isabelle's AFP) is a long-term (but still largely unachieved 7 ) goal. Currently, it takes intimate knowledge of the various formal libraries in order to assess which library, and ensuing ITP, would best be used to formalize a given mathematical theorem. A tool that can assess the suitability of a certain ITP and its library before starting the formalization process could make formalization more accessible.\n",
      "\n",
      "Recommendations. We advise exploring more diverse mathematical interaction modes - across a variety of question and session types - as outlined here, both for formal as well as informal datasets. Our list is not exhaustive and can also be broken down to a more detailed level. In particular, oral, mathematical communication is not well mapped to data (see Section 4.3). Mathematical collaboration often involves an exchange of very high-level ideas at a rapid pace. Specific, wide-ranging recommendations can be found in Section 4. We believe capturing this in data will help to lead to more widespread mathematical assistants. Finally, we believe that input from research in mathematical education must be taken into account to derive curated datasets that exhibit richer interaction modes.\n",
      "\n",
      "7 A score of overlap by assessing how many theorems out of 100 theorems have been formalized in different ITPs is provided here: https://www.cs.ru.nl/~freek/100/\n",
      "\n",
      "## 3.1.4 Contamination\n",
      "\n",
      "When releasing a dataset, control is lost over whether the dataset ends up as training data for machine learning models. For several of the state-of-the-art models, such as GPT-4 [OpenAI, 2023], that perform well on benchmarks, no information is available in the ensuing on whether efforts were made to ensure that training datasets were decontaminated and no datapoints from the evaluation benchmark were included in the training set. For Gemini 1.5 [Reid et al., 2024], Qwen2 [Yang et al., 2024a] and DeepSeekMath [Shao et al., 2024], only n -gram decontamination approaches have been tested. For complex datapoints, such as math, n -grams are unfortunately not always sufficient to ensure the training dataset is clean. There is evidence that suggests that several such modern models are already contaminated [Xu et al., 2024]. While mitigating approaches have been proposed, such as generating new data, either from scratch [Mishra et al., 2024] or by using existing datasets as seeds [Zhou et al., 2024], it is not clear whether these can scale to more sophisticated mathematics, as the methods were tested using mathematics on the level of grade-school, and it is unclear how well they would scale to much higher levels of mathematical abstractions and problem-solving difficulty (the two main metrics of difficulty, as outlined in 3.1.1.\n",
      "\n",
      "Recommendations. If the dataset is sufficiently large, it is advisable to keep a part of the dataset hidden from the public so that the dataset creators can compare how well newly-released, publicly accessible openweight models score in the public versus the hidden part of the dataset. Diverging scores on two splits of the dataset can indicate contamination. Care must be taken that datapoints from the hidden datasets are from the same distribution compared. For mathematics, this means at minimum: Same domain, difficulty range, and proof technique. This can be challenging: Given a specific problem P , it takes effort to source a new problem P /star , that on all relevant metrics (including the mentioned ones) is similar to P , but at the same time is not completely analogous to P - as it would be by, e.g., merely changing items in the problem statement that have a negligible on the proof (this is an approach followed by GSM8K-Symbolic dataset [Mirzadeh et al., 2024], which nonetheless turned out to be challenging for LLMs, highlighting their current limited reasoning capabilities). The approach advocated here has limitations for models released as-a-service [La Malfa et al., 2024], via APIs or GUIs, as running the hidden dataset through risks it automatically being used at a later stage as training data for those models. RealMath proposes a continuously refreshed benchmark mined from arXiv papers and forum discussions, which reduces the half-life of static public test sets [Zhang et al., 2025b].\n",
      "\n",
      "## 3.1.5 Binned Sankey diagrams\n",
      "\n",
      "Math benchmarks are often reported with aggregate statistics such as accuracy, pass@ k , or majority-vote accuracy. These statistics compress item-level outcomes into a small number of scalars. On mixed benchmarks, an average gain can coincide with regressions on specific slices (topic, modality, or answer format), which the average does not show. When assessing model improvement over time, this information conveys both too little detail and is also hard to read. This issue is not solely applicable to mathematics, but is very prevalent in this case.\n",
      "\n",
      "Recommendation. To support better visualizations, we recommend the use of binned Sankey diagrams. These are item-level transition plots between two evaluations. Each benchmark item is assigned to an outcome bin under a baseline evaluation (for example, model M t ) and to an outcome bin under a second evaluation (model M t +1 ). The diagram draws flows from baseline bins to new bins, with band widths proportional to the number of items making each transition. Sankey-style migration plots have been used in other evaluation contexts; see Fig. 2 in Frieder et al. [2023a].\n",
      "\n",
      "For mathematics, the visualization works best in the following way: Choose two evaluation points and define a small set of outcome bins. The minimal binning is binary ( correct / incorrect ). If annotations exist, bins can reflect error types [Frieder et al., 2023a] (forsexample, algebra error, invalid step, wrong final answer) or partial credit (for example, solved , nearly solved , off-track ). If the evaluation involves tools, bins can separate solved without tools , solved with tools , and failed . The resulting diagram makes\n",
      "\n",
      "regressions explicit. A flow from solved to failed remains visible even when net accuracy increases. For heterogeneous benchmarks, the same transition plot can be computed within slices such as topic, difficulty level, or representation (symbolic, word problem, diagram-based, or formal proof assistant). Slice-level transitions show whether gains are concentrated in a subset of items and whether any slice exhibits systematic regressions.\n",
      "\n",
      "Benchmark reports can include a binned Sankey diagram alongside aggregate metrics when comparing model versions or training regimes. This requires storing per-item outcomes (and, when applicable, slice labels) so that transitions can be reproduced. Using the same bins across releases supports longitudinal comparisons.\n",
      "\n",
      "## 3.2 Distinct Issues\n",
      "\n",
      "This section will detail issues that specifically appear with either natural language or formal language datasets, or mixed datasets. The most prominent difficulty with natural language datasets is the lack of trusted automatic evaluation as will be explained in Section 3.2.1.\n",
      "\n",
      "A natural way to circumvent the difficulty of evaluating natural language proofs is by making use of formal language datasets. Instead of having an LLM generate natural language mathematical proofs, one can train it to generate proofs in a verifiable formal language such as Lean or Isabelle. Therefore, existing libraries of formal proofs can play a crucial role in the development of machine learning-based automatic theorem provers. Sections 3.2.2 and 3.2.3 will provide details on how some of the largest existing collections of formal proofs have been and are being created. This is followed by an analysis of how this process affects the use of formal proof libraries as evaluation datasets; with some remarks relevant to using formal libraries for training. More concretely, Isabelle's Archive of Formal Proofs (AFP) and Lean's Mathlib will be investigated. This choice is due to the authors' familiarity with these libraries but many of the points mentioned below will also be valid (at least to some extent) for other systems. One particular issue relevant to all datasets is their scale. The lack of a large-scale dataset consisting of pairs of formal and informal mathematics is a significant bottleneck for autoformalization. Current datasets, such as miniF2F are on the order of a few hundred datapoints, which are enough for assessing autoformalization, but not for training models to support autoformalization. A recent effort [Ying et al., 2024] scales this to 57k datapoints of pairs. Nonetheless, this scale is not yet comparable to the scale at which LLMs can be trained on informal mathematics.\n",
      "\n",
      "## 3.2.1 Lack of Trusted Automatic Evaluation\n",
      "\n",
      "The easiest way to automate the evaluation of mathematical prompts is to formulate them in a way such that the answer can be represented as a single token (e.g., a number or a mathematical term). Automatic evaluation can then be performed by keyword-matching the token to the gold-truth answer. A slightly more advanced variant is pursued by the MATH dataset, which allows a reasoning section, but encloses the final answer in a \\boxed environment (and only this is keyword-matched).\n",
      "\n",
      "No large machine learning dataset on mathematics currently exists that allows an arbitrary proof of an AI system to be checked for correctness against the gold-standard proof in the evaluation dataset. The Open Proof Corpus collects expert judgments on model-generated natural-language proofs and reports large gaps between boxed-answer accuracy and proof validity [Dekoninck et al., 2025]. A proof-of-concept, highlighted by the IMO Small Challenge 8 , shows how partial automation might be achieved, where a necessary test for correctness is carried out using detailed proof annotation. This builds on the idea that a system like an LLM, combined with a deterministic form of matching, can at least exclude incorrect proof candidates. If the ground truth is sufficiently rich, the annotations help the LLM break down a proof and understand its main features (accepting that subtler points of proofs may still be currently out of reach of LLMs' understanding).\n",
      "\n",
      "Some success has been attained in using an LLM to grade (or teach) another LLM [Eldan and Li, 2023, Mukherjee et al., 2023]. Yet, for mathematics, we believe that this is not necessarily the right approach\n",
      "\n",
      "8 www.imo-small-challenge.io\n",
      "\n",
      "forward. The current generation of models does not have a sufficiently high performance on mathematics to be used as graders: for tasks that would often be deemed simpler than mathematics, this approach can fail [Wang et al., 2023], although for solving programming puzzles this approach was successful [Haluptzok et al., 2023]. Anecdotal evidence suggests that adapting the approach to proofs is challenging because of the multitude of potential proof variations for a single statement, which gives rise to a diverse set of textual expressions. At present, grading can, at best, be automated by the use of detailed human annotation for each problem in combination with LLM assistance based on such annotation.\n",
      "\n",
      "Recommendations. In general, we recommend developing evaluation methodologies that allow for natural language proofs to be checked for correctness. This is difficult in general. A middle ground is to include as much metadata as possible when annotating datasets with manual effort, with a view towards supporting LLMs in using this metadata to assess proof candidates. Second, we recommend more research that quantifies the degree to which LLMs can evaluate mathematics. This will help in making objective statements about how good or how poor LLMs are at evaluating different kinds of mathematics, and help in identifying areas for improvement.\n",
      "\n",
      "## 3.2.2 Tool Misalignment\n",
      "\n",
      "As writing formal proofs imposes slightly different challenges to writing informal proofs, it is important to distinguish mathematical proficiency from proficiency in using tools provided by the formal environment.\n",
      "\n",
      "One cause of this discrepancy is that side conditions deemed trivial in conventional mathematical literature have to be formalized as diligently as the rest of the mathematical text. On the other hand, proving many statements considered routine by mathematicians can be mechanized. For example, proving that a function is continuous often corresponds to choosing the correct subset of a few relevant lemmas and this can be achieved with a simple search algorithm, such as Lean's continuity tactic. Next to this domain-specific formalization aid, there are also general-purpose automation tools that can be used to formalize certain simple statements. For example, Isabelle's Sledgehammer tool [Paulson and Susanto, 2007, Meng and Paulson, 2008] translates the given statement so that it can be understood by a number of external automatic theorem provers (ATPs). The ATPs, in turn, attempt to prove the statement and send the proof back to Isabelle.\n",
      "\n",
      "Automation and Proofs Naturally, the existence of automation affects how the task of formalization is approached: Without automation, formalization would consist of translating a mathematical proof step-bystep into the ITP's logic. But having access to advanced tactics and general-purpose ATPs, one only needs to repeatedly break down the proof into smaller pieces until these can be tackled by automation.\n",
      "\n",
      "Eventually, this can lead to proofs being formalized in a way that is not very close to the original natural language proof. For example, elementary homework-style problems often consist of long calculations that might correspond to one single tactic invocation in the formal system. Also, in more advanced topics, the automation of a theorem prover can make the proof less readable or skew its focus. For example, natural language proofs typically make it very clear which definitions need to be unfolded and when. In Lean, such unfolding does not have to be made explicit, which can widen the gap between formal and informal.\n",
      "\n",
      "The impact of these mechanisms needs to be kept in mind when formal datasets such as Lean's Mathlib or Isabelle's AFP are used for LLM evaluation. In particular, it is likely that current datasets overly reward models for their ability to deploy the language-specific automation, rather than complete 'understanding\" of the underlying mathematics. As some evidence of this, Hu et al. [2024] show that LLM performance drops significantly when common automation tactics are turned off, even for theorems that have human-written proofs that do not rely on the automated tactics.\n",
      "\n",
      "Strong automation and the Archive of Formal Proofs (AFP) These matters are especially significant with the Isabelle AFP because of the powerful general-purpose automation that has been widely used in its development over the last decade. It can therefore be expected that a large subset of theorems in today's\n",
      "\n",
      "AFP have been formalized by repeatedly breaking them down into smaller subproblems until Sledgehammer could find their proof.\n",
      "\n",
      "A natural way to turn the AFP into a proof dataset is by splitting up all formal proofs into their individual steps [Jiang et al., 2021]. However, by the above characterization, such a dataset might be skewed towards being tractable for automation because of the particular way in which AFP theorems have come into existence. This can make it hard to assess the level of difficulty in the problem set.\n",
      "\n",
      "Especially when equipping LLMs with the ability to access existing automation, careful evaluation is essential. It is important to determine whether the model genuinely 'understands' complex mathematical concepts and can scale this understanding up to more difficult problems; the alternative being that the model merely excels at finding 'low-hanging fruits', which, when combined with non-ML automation, might appear deceptively impressive.\n",
      "\n",
      "We would like to stress that we believe that the use of formal mathematics datasets such as the AFP is a valid evaluation tool. The results of recent publications [First et al., 2023, Jiang et al., 2022, Mikuła et al., 2024] are especially promising, and we are looking forward to their continuation. Nevertheless, we believe that future evaluation should carefully consider the impact of existing automation in ITPs on their libraries.\n",
      "\n",
      "Recommendations. The available formal mathematical libraries that include deep, research-level results have generally been created using a significant amount of automation. Researchers could create small, specific additional test sets of proofs that are formalized without automation and evaluate mathematical assistants on these.\n",
      "\n",
      "## 3.2.3 Non-Trivial Data Duplication\n",
      "\n",
      "Next to these aspects related to the nature of theorem provers, there are also challenges intrinsic to mathematics itself. In particular, mathematics has a self-similar structure: there are profound connections between areas that are seemingly disconnected, and many mathematical statements have several related versions, depending on the perspective and level of generality chosen. This section will detail some of the inherent challenges this creates for the construction of formal mathematical libraries and then consider the impact of using them as datasets.\n",
      "\n",
      "Ideally, a formal library should have as little duplication as possible to allow for a smoother user experience and to reduce the amount of code maintenance required. At the same time, this ideal can seldom be upheld fully since formalizing theorems only in their most general form is intractable. Some level of non-trivial duplication is therefore accepted and common in formalization.\n",
      "\n",
      "As an example, Lean's Mathlib often provides several versions of a lemma, 9 which makes it more convenient to use the library. Instead of having to 'import' a lemma and then transform it into the right format, one can directly refer to commonly used variants. For example, the statement that\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "in a commutative semigroup is explicitly formalized even though it could easily be derived from associativity and commutativity whenever needed. Furthermore, there is support for automatically generating lemmas, e.g., to generate the additive version of an abstract algebra statement given in terms of multiplication.\n",
      "\n",
      "Next to this, the rules and customs of how a collection of formal proofs is built and maintained have a significant impact on its properties as a machine-learning dataset. Compared with Mathlib, the Isabelle AFP is more static and aims at being archival, i.e., providing long-lasting support for its content and not removing previously defined concepts. This makes duplication inevitable as soon as parts of the library go through a larger restructuring or even redevelopment. For example, there are two separate algebra libraries for Isabelle,\n",
      "\n",
      "9 We note that this facility is not exclusive to Lean; e.g., Isabelle provides several versions of a lemma.\n",
      "\n",
      "one in the AFP and a second one using a different formalization approach in the Isabelle HOL-Library, which often gets used in conjunction with the AFP.\n",
      "\n",
      "As a further example, consider the AFP section on category theory. Currently, there are at least five separate formalizations of category theory at various stages of development with all of them taking different approaches 10 . This is likely to be for historical reasons, but also because exploring different representations of category theory is an interesting topic in mathematical foundations.\n",
      "\n",
      "Naturally, if duplication is present in the formal library, a lot of care becomes necessary when designing machine-learning datasets from it. If data is just randomly split into a training and validation set, then leakage can occur on one hand. Of course, the duplication will typically be on the level of mathematical concepts and not exist as a verbatim doubling of formal language code. Nevertheless, disregarding the issue of duplication makes it hard to gauge to what extent a model manages to produce formal proofs from the ground up. Possibly, parts of the model's performance simply stem from learning how to translate between different representations of the same mathematical concept. On the other hand, duplication brings to the forefront the issue of data representation, as machine learning models will only perform well on that representation on which they have been trained, which is in contrast to human mathematical reasoning, which, to a degree, is robust against different formal representations of the same mathematical objects.\n",
      "\n",
      "Translation of formal representations constitutes a highly interesting and relevant skill in its own right. Nevertheless, good test design should enable researchers to judge to which degree the respective skills are achieved.\n",
      "\n",
      "Recommendations. Following the discussion above, we recommend developing methodologies that ensure that train/test splits adequately measure a model's ability to generalize. One direction is to test on new formalizations that occur after a model has been trained, as was explored in Hu et al. [2024]. Nevertheless, care should be taken to ensure that the mathematics being formalized does not occur in previous projects. Hence, we particularly recommend evaluating on domains that have not previously been formalized.\n",
      "\n",
      "More broadly, the ideal evaluation of digital mathematical assistants should include case studies in which previously unformalized proofs get formalized using the assistant. There are various ways to realize this. On a small scale, researchers can qualitatively evaluate the LLM by formalizing some reasonably difficult, previously unformalized proof. On a larger scale, the assistant can be provided as a plugin to theorem provers, which a large number of people working in formalization can then use in real-world scenarios. If there are enough users, such a setup could even use A/B testing for multiple architectures of mathematical assistants.\n",
      "\n",
      "## 3.2.4 Version Fragility and Tactic Stability\n",
      "\n",
      "Formal proof corpora are often treated as static datasets consisting of a theorem statement plus a proof script. In practice, they are tightly coupled to a moving toolchain (involving a specific version of the prover, a particular commit of the library, and details of automation implied by this combination). This can make them fairly brittle as datasets: even small library changes can invalidate proofs or silently change what intermediate tactics do.\n",
      "\n",
      "A common failure mode is 'tactic instability', where a proof uses a flexible tactic (in Lean, most notably simp ) and then applies a subsequent tactic whose success depends on the exact transformed goal. Because the set of parameters used by the automated tactic (e.g., lemmas used by simp ) evolves, the transformed goal may drift over time, making the downstream step brittle even if the overall theorem statement is unchanged.\n",
      "\n",
      "10 cf. https://www.isa-afp.org/topics/mathematics/category-theory/ not counting generalizations of category theory and continuations of previous developments.\n",
      "\n",
      "Recommendations. Recent work in Mathlib 11 introduces a stability linter ('FlexibleLinter') that flags precisely these patterns, encouraging authors to make tactic behavior more explicit (e.g., by specifying a minimal set of lemmas used in the simplification). As a dataset design principle, proofs that pass such stability linting are better candidates for long-lived benchmarks, since their execution is less sensitive to future library drift.\n",
      "\n",
      "## 3.2.5 Hidden Context Dependence: Local Instances and Attribute State\n",
      "\n",
      "Even in a fully formal setting, a dataset item is seldom determined by the goal alone. In Lean, imports, local instances, and attribute state can change the meaning of a line of proof text. This matters for dataset construction because 'the same' proof script can behave differently when replayed under a different environment.\n",
      "\n",
      "Example (instance 'pollution'). In measure-theoretic developments in Lean, the proposition MeasurableSet s is parameterized by an implicit measurable-space instance. If a proof introduces a local measurable space m0 but an ambient instance mW remains in scope, MeasurableSet may silently refer to mW unless the instance is pinned:\n",
      "\n",
      "```\n",
      "--Fails: 'MeasurableSet' uses the ambient instance, not the local one have h_bad : MeasurableSet s := by simpa using hs --Works: pin the intended instance explicitly have h_ok : @MeasurableSet alpha m0 s := by simpa [m0] using hs\n",
      "```\n",
      "\n",
      "This illustrate that storing only the final proof term (or only the local goal) in a dataset can be insufficient to replay a proof deterministically, because replay depends on the surrounding instance environment and imported modules. For more examples from various areas of math, see the reference patterns contained in the Claude Code skill for Lean 4 [Freer, 2025].\n",
      "\n",
      "Example (context-dependent simplification). Automation can also be context-sensitive in a more mundane way. Automated tactics such as an instance of simp in Lean can succeed or fail depending on hypotheses introduced earlier in the proof:\n",
      "\n",
      "```\n",
      "--Works with extra context in scope filter_upwards [hw, hu_eps, hv_eps] with w hu hv simp [Real.norm_eq_abs] --May fail without that context, even if the syntactic goal looks similar simp [Real.norm_eq_abs]\n",
      "```\n",
      "\n",
      "This suggests that proof snippets that are extracted without their surrounding context (imports, local hypotheses, attributes) can be underspecified as dataset items for training or evaluation.\n",
      "\n",
      "## 3.2.6 Resource Sensitivity: Heartbeats and Elaboration as Part of the Task\n",
      "\n",
      "In interactive theorem provers, 'correctness' is not clearly delineated from performance. A proof that compiles only under extremely permissive resource limits (such as 'heartbeats', recursion depth, or wall-clock time) is often not usable in real formalization workflows, and may not be reproducible across environments (especially if the high resource bounds are not clearly specified).\n",
      "\n",
      "11 See https://leanprover-community.github.io/mathlib4\\_docs/Mathlib/Tactic/Linter/FlexibleLinter.html for details.\n",
      "\n",
      "Lean exposes heartbeat and other resource limits directly:\n",
      "\n",
      "```\n",
      "set_option maxHeartbeats 40000 set_option synthInstance.maxHeartbeats 40000 set_option maxRecDepth 2000\n",
      "```\n",
      "\n",
      "These resource bounds can be exceeded by complex elaborations or typeclass searches. Sometimes this is due to the complexity of the underlying mathematics, but other times it is from 'fighting the elaborator'. For instance, leaving instance arguments implicit can trigger expensive unification/typeclass search, sometimes consuming hundreds of thousands of heartbeats before timing out. This suggests that datasets and benchmarks should treat resource budgets as first-class metadata, and report resource usage alongside whether the code passed or failed.\n",
      "\n",
      "## 3.2.7 Redundant Scaffolding and Accidental Complexity in Proof Scripts\n",
      "\n",
      "Formal corpora often contain a lot of proof-script structure that is not mathematically essential, including redundant tactic steps, arguments to automation that are not used, and lemma variants that exist only to satisfy local engineering constraints. As dataset artifacts, these patterns can dominate the training signal while teaching models brittle or unidiomatic behavior.\n",
      "\n",
      "Example (no-op tactic after automation). A common failure mode when replaying mined proof scripts is that one tactic already closes the goal, making subsequent steps ill-typed. For example, in Lean, this sequence can fail:\n",
      "\n",
      "```\n",
      "simp -- closes the goal aesop --error: no goals to be solved\n",
      "```\n",
      "\n",
      "Example (lintable redundancy in automation arguments). Even when scripts compile, they often include extra automation arguments that are unused in practice. Mathlib-style linting can flag such redundancy and enable construction of a cleaned corpus:\n",
      "\n",
      "```\n",
      "--Linter: unused simp arguments (suggest removing to reduce clutter) simp [pow_two, mul_assoc, mul_add, mul_comm, add_comm, add_left_comm]\n",
      "```\n",
      "\n",
      "In light of this, it may be useful for datasets to include both a raw corpus (for realism) and a linter-cleaned corpus (for learning canonical structure).\n",
      "\n",
      "## 4 The Future - Novel Datasets to Support Mathematical Copilots\n",
      "\n",
      "In this section, we cover various aspects of mathematical research practice that, to date, are not covered at all by any of the existing datasets. This contrasts with the previous section on mathematical datasets that covered parts of mathematical practice, albeit with the noted limitations. Current datasets are exclusively focused on publishing the results rather than the intermediate steps that a mathematician goes through in the process of devising a proof. The different types of workflows, the proof-counterexample dialectic, and proof transfer, etc., are all examples of intermediate processes that do not have clear representations in data that can be used to fine-tune LLMs. Contrasting with the previous section, where we made specific recommendations on how to improve existing datasets, in this section, we do not make specific recommendations, as our recommendation is to simply devise such datasets. While a few inroads have been made into non-proof-based datasets, see Section 3.1.3 these are still very much at their inception, and, to our knowledge, the GHOSTS dataset is the only attempt to date to try to 'data-ify' some aspects (solely related to mathematical literature search) of daily mathematical process.\n",
      "\n",
      "An example of proof transfer is given by the Ax-Grothendieck theorem, which states that every injective polynomial map C n → C n is surjective; rather than proving this directly over C , one observes that the statement is trivial for finite fields - any injective map on a finite set is bijective - and then uses model-theoretic compactness to transfer the result to algebraically closed fields of characteristic zero. Here, one does not redo the argument in the new structure directly; instead, one uses model theory to move the truth of a sentence from one place to another.\n",
      "\n",
      "## 4.1 Mapping Mathematical Workflows to Data\n",
      "\n",
      "The previous sections have shown that existing datasets and benchmarks only deal with specific mathematical tasks. In particular, they focus mostly on question answering and theorem proving, with solutions being presented in a streamlined way (as is typical for results in mathematical textbooks). In consequence, they do not sufficiently cover all steps in typical mathematical workflows, such as surveying mathematical topics, gathering related results, establishing high-level proof strategies and intuition, refactoring proofs, or carrying out field-specific routines. Yet, we would like to evaluate and train mathematical copilots on these tasks, too, in order to obtain full-spectrum assistance across all facets of mathematical research practice. We note that datasets used to pre-train LLMs exhibit broader coverage, e.g., of mathematics-related Q&amp;A communities, blogs, and educational material; however, it is hard to assess and control the quality of the included mathematical content. This results in a weak learning signal and, for a strong mathematical copilot, one likely requires fine-tuning on a comprehensive, high-quality dataset of mathematical workflow steps.\n",
      "\n",
      "Simple methods as precursors. Even relatively simple machine learning approaches have produced genuine mathematical discoveries. Wagner [Wagner, 2021] demonstrated that reinforcement learning, framed as a single-player graph-building game, can find counterexamples to open conjectures in extremal combinatorics. PatternBoost [Charton et al., 2024], built on Karpathy's character-level language model makemore [Karpathy, 2022], alternates between local search and transformer-based generation to discover constructions that refuted a 30-year-old conjecture. These methods succeed precisely because they operate in domains where candidate solutions can be automatically evaluated. This is a form of verifiable feedback that sidesteps the hallucination problems plaguing open-ended generation. They are precursors to more general workflows that combine neural generation with symbolic verification and human mathematical insight.\n",
      "\n",
      "We advocate a more bottom-up approach that consists of creating a taxonomy of workflows, i.e., isolating and categorizing workflow steps. Specifically, many mathematical workflows can be modularized into smaller sequential steps, which themselves can be ordered along different dimensions, e.g., their mathematical depth, level of abstraction (from general, high-level proof techniques and principles to specialized, problem-specific approaches), mathematical subject, how well a symbolic approach might handle them, whether the workflow step is strictly mathematical or meta-mathematical (e.g., literature search). With this in view, we distinguish between general global workflows, which are used across mathematical fields, and field-specific local workflows. For instance, we refer to the Tricki 12 for a collection of problem-solving techniques that can be viewed as abstract workflows. General proof techniques, such as linearization and fixed point theory, can be viewed as global workflows. However, we note that specific instantiations of such techniques, e.g., related to dynamical systems, can also be classified as local workflows. Examples of even more local workflows include approaches tailored to specific kinds of partial differential equations, e.g., the method of characteristics, maximum principle, energy estimates, or Green's functions, as these methods frequently occur as individual steps in larger chains of arguments in research-level mathematics.\n",
      "\n",
      "There is no hard boundary for transitioning between local and global workflows. For example, the 'routine' task in analysis to upper bound an expression efficiently, as opposed to evaluating it exactly, is found both in elementary contexts, such as proving inequalities such as versions of arithmetic mean-geometric mean inequality, up to more advanced cases, such as for ordinary differential equations (ODEs) or partial differential equations (PDEs), where a routine task is that of applying an 'energy method' or 'Lyapunov functional\n",
      "\n",
      "12 https://www.tricki.org\n",
      "\n",
      "method'. We, therefore, deliberately do not draw a specific boundary of when to categorize a workflow step as local vs. global.\n",
      "\n",
      "While creating a complete taxonomy of workflow steps is beyond the scope of this article, we want to examine selected workflows and their steps (both local and global) in order to highlight issues in translating these to data and specific challenges. We note that these are merely illustrative examples meant to show how first steps could be undertaken to generate datasets that support such workflows.\n",
      "\n",
      "While a workflow (indicated with arrows below) can, in principle, easily be mapped to data by converting it to n tuples of datapoints (where n denotes the number of workflow steps) and filling text in between, in practice, in certain cases, issues may arise. We give examples of both global and local workflows below and highlight both examples where these can be transformed to data in a straightforward manner, as well as cases in which this transformation is more difficult. We urge the community to explore such translation of workflows to data further. Future work should address how such workflow datasets might be structured, how symbolic or numeric methods can be integrated into individual workflow steps, and which parts of these workflows are best left to humans versus delegated to a mathematical copilot. Once such datasets exist, LLMs can be trained on them using standard methods such as expert iteration [Anthony et al., 2017, Havrilla et al., 2024].\n",
      "\n",
      "Inference-time compute in reinforcement learning needs problem analyses. Several recent math reasoning systems use substantial inference-time compute, for example, by sampling many candidate solutions, running search over intermediate steps, or iterating tool calls, and then using reinforcement learning to train policies that decide which branches to expand. In domains with automatic verification, these loops can rely on a binary reward signal. For most informal mathematical tasks, that signal is too sparse: a model receives feedback only after a long chain of steps, and failure provides little information about where the attempt went off track.\n",
      "\n",
      "One remedy is to include, alongside each problem statement, a structured problem analysis that makes intermediate progress measurable. Concretely, this can include a normalized restatement of the problem, the relevant definitions and constraints, a decomposition into subgoals that a solver would aim to establish, and examples of admissible intermediate claims (or failure modes) that an evaluator can recognize. These annotations can be authored by experts or drafted with model assistance and then checked, but they are rarely present in current math benchmarks. Without this layer, RL methods that spend compute at inference time are pushed toward brute-force sampling on a sparse objective, which does not scale to longer proofs or to tasks whose correctness is not cheaply checkable.\n",
      "\n",
      "Global workflows Literature search is one of the most general workflows across the sciences, having particularities in how it takes place in mathematics. Prior work can be related to the problem a mathematician has at hand in many ways. One might, for instance, want to know what is known about a particular mathematical object at hand. The context of a problem might have many options for what to look for, so the first step in the workflow might be to identify an object that has a good chance of having been seen before. One example is integer sequences: perhaps the solutions to a combinatorial problem are computable in small cases, and it is often useful to know if the resulting sequence has been encountered in the context of other problems (there might be many ways to get a sequence: for instance, we might have to choose variables to fix and then only vary one parameter of the problem). For this particular case, the Online Encyclopedia of Integer Sequences 13 is a well-known, efficient, specialized tool, with many pointers to the literature, and the ability to recognize partial matches (e.g., perhaps one's sequence is coarser than the one from relevant prior work). In case there are no matches, we can try to repeat the process with other candidate objects; in case there are, the challenge turns to assess whether any of the results seem meaningfully related to the current case (and given only a few integers, there are often too many sequences containing them). For instance, the sequence 1 , 4 , 44 appears in path-counting problems in some lattices, or in a few number-theoretic contexts (like products of odd-indexed Lucas numbers), as well as many other contexts, and more context that was not\n",
      "\n",
      "13 https://oeis.org/\n",
      "\n",
      "included during the search might be relevant in identifying which of these sequences might be meaningfully related to the current problem. We could broadly summarize this workflow as:\n",
      "\n",
      "Identify objects to search for → find related work describing the object → assess potential relation to current context\n",
      "\n",
      "We might expect LLMs to help generalize this process beyond what specialized tools, such as the OEIS, are capable of. Most mathematical objects are significantly harder to describe than integer sequences: for instance, one might have a particular topology on a function space that might be unique to the current context, but something of the same 'shape' might have been seen before. The assistant would have to recognize and know how to describe this shape, and identify matches across potential differences in what definitions are implicitly or explicitly used, as well as potential equivalent definitions that might cause a description in the literature to differ on the surface (e.g., mention 'accumulation points' vs 'limit point').\n",
      "\n",
      "Examples of this workflow for training and evaluation might be automatically extractable in a post-hoc fashion from the mathematical literature itself: one might be able to look at how previous work is referenced in existing proofs as a source of examples of relevant previous work (other matches for the same object that are not the paper that was cited are most likely examples of spurious matches).\n",
      "\n",
      "Local workflows We list below examples from several distinct subfields of mathematics (differential equations, knot theory/low-dimensional topology, and number theory).\n",
      "\n",
      "- Suppose, for instance, one has a first-order ODE ∂ t u = F ( u ) with some initial data u (0) = u 0 and wants to know how the solution grows in time. A standard technique is to introduce a key functional E ( u ) of the solution (often something like an 'energy' or 'Lyapunov functional') and then compute the derivative ∂ t E ( u ) using the ODE and the chain rule (for PDEs, one often has to also perform several times integration by parts). Then, one bounds this derivative as best one can. The goal is to reach some differential inequality of Gronwall type, e.g., ∂ t E ( u ) ≤ CE ( u ), so that a Gronwall-type lemma may be applied (but it is not always precisely the Gronwall lemma, but something similar).\n",
      "\n",
      "Summarizing, the workflow is thus of the form:\n",
      "\n",
      "## ODE/PDE → find functional → compute derivative of functional → bound derivative → reach Gronwall-type inequality\n",
      "\n",
      "We note that individual steps of this may be solved using symbolic-numeric methods, which opens the possibility of using a tool-integrated-reasoning (TIR) approach to enable an LLM to discharge these proof steps to symbolic or numeric tools to, e.g., compute derivatives symbolically. Thus, one option to encapsulate these workflow steps in data is by using datapoints whose form is\n",
      "\n",
      "(ODE/PDE, functional, functional derivative, derivative bound, Gronwall-type inequality)\n",
      "\n",
      "with potentially filler text between the raw mathematical objects contained in the datapoint (i.e., the ODE, the functional, etc.). The raw mathematical objects should be symbolically or numerically generated so that custom filler text can be added in between them, and an LLM can observe a range of ways in which to speak and textually connect a fixed tuple of mathematical objects.\n",
      "\n",
      "What one typically wants, as a mathematician, in the case of this workflow, is to try various guesses for the functional. If a TIR approach is used, this data representation of this workflow should specifically offer 'entry points' to make it easy for an LLM to accept arbitrary input at certain places and use tools to automatically run the essential parts of the workflow. An LLM is then much better equipped to run the workflow steps and produce an estimate, either automatically or semi-automatically. One can preliminarily do this already through conversation with current state-of-the-art LLMs, but with a lot of mistakes on the LLM's part, which a stronger grounding in data would help to correct.\n",
      "\n",
      "- A commonly encountered workflow for evolutionary PDEs (i.e., PDEs whose solution evolves with a time variable t ), analogous to the first-order ODE example given above, can be illustrated as follows.\n",
      "\n",
      "One wants to find a functional which will facilitate providing useful information about solutions of the PDE. To illustrate how such functionals are found in practice, we begin by considering the simple example of the linear wave equation ∂ tt u -∆ u = 0 on a bounded domain Ω ⊂ R n , with smooth boundary ∂ Ω, subject to homogeneous Dirichlet boundary conditions u = 0 on ∂ Ω and initial conditions u ( x, 0) = u 0 ( x ), ∂ t u ( x, 0) = u 1 ( x ). (One can also study this problem on an unbounded domain Ω such as, for instance, R n , and impose that solutions must decay in the unbounded directions of Ω.) To show energy conservation, we define the energy functional\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "which represents the sum of kinetic energy and potential energy. Differentiating E ( t ) in time under the integral sign, we obtain\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "An integration by parts shifts the spatial derivative in the second term, while the boundary contribution vanishes because u and hence ∂ t u are zero on ∂ Ω. By substituting the wave equation ∂ tt u = ∆ u , the expression collapses to zero, showing that dE dt = 0. Thus\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "More generally, on any finite time interval [0 , T ], one can often prove an energy bound of the form E ( t ) ≤ C ( T ) E (0) for 0 ≤ t ≤ T , with C ( T ) possibly being a constant function. When C ( T ) = C &gt; 0 is constant, we write\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "An inequality like (4) is sometimes referred to as an energy estimate . Notice that the conservative situation (3) satisfies the inequality (4) with C = 1. The energy bound (4) can be proved for certain wave equations with non-constant coefficients and for some non-linearities (often under additional structural or smallness assumptions); it provides useful control on the solution. For example, it can be used to prove the uniqueness of solutions of the wave equation [Evans, 2010, Section 2.4, Theorem 5], as well as finite propagation speed for solutions of the (constant-coefficient) wave equation [Evans, 2010, Section 2.4, Theorem 6], and more generally under standard hyperbolicity assumptions.\n",
      "\n",
      "One can invert the previous perspective on this problem: if one had no prior knowledge of this equation and its properties, how could equations (3) or (4) be derived in the first place? In particular, to obtain an energy identity (and hence an energy estimate) for this model problem, one takes the equation ∂ tt u -∆ u = 0 and multiplies both sides of it by ∂ t u , and then integrates by parts on [0 , T ] × Ω, where T &gt; 0. (The exact choice of this multiplier depends on the equation at hand; as we will see in the next paragraph, u turns out to be a good multiplier for the heat equation. Moreover, multiple good choices of multipliers are usually known to exist for each equation, with different multipliers usually leading to different and useful inequalities, among which (4) can be viewed as one of the simplest examples.) Assuming that either u = 0 on ∂ Ω (Dirichlet, hence ∂ t u = 0 on ∂ Ω) or ∂ ν u = 0 on ∂ Ω (Neumann), after a few integrations by parts, one obtains exactly that the energy functional defined in (2) is conserved: E ( t ) = E (0) for all t ≥ 0. It turns out that for many more general wave equations of the form Pu = F (where P is a wave-type differential operator, e.g. ∂ tt -∆ with variable coefficients and lower-order terms, and F is a forcing term), the same choice of multiplier ∂ t u (meaning we multiply the equation by ∂ t u and integrate by parts) yields an energy identity or inequality. Under additional structural assumptions, this can be turned into a uniform-in-time bound of the form (4). In practice, finding good multiplier functions and performing integration by parts in the resulting manipulations is one of the standard ways new inequalities are discovered.\n",
      "\n",
      "Let us modify the wave equation argument slightly to obtain an analogous result for the linear heat equation ∂ t u -∆ u = 0. In this case, as we will now see, one obtains\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "where E ( t ) is defined to be E ( t ) = 1 2 ∫ Ω u 2 dx ; notice that this functional E differs from the functional we used for the wave equation. As in the wave equation case, the inequality (5) can be discovered by multiplying both sides of the heat equation by an appropriate function (here, u ), and then integrating on the domain [0 , T ] × Ω. After a few uses of integration by parts, if u = 0 on ∂ Ω (Dirichlet) or ∂ ν u = 0, then one obtains, for 0 ≤ t ≤ T ,\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Since ∫ t 0 ‖∇ u ( t, · ) ‖ 2 L 2 (Ω) dt ≥ 0, one concludes (5). The inequality (5) can be used to prove the uniqueness of solutions to the heat equation; one can find in [Evans, 2010, Section 2.3, Theorem 10] a proof of uniqueness of solutions forward in time, and [Evans, 2010, Section 2.3, Theorem 11] proves a backward uniqueness property. (Unlike the wave equation, the notions of uniqueness forward and backwards in time do not 'coincide' immediately for the heat equation.) The idea of manipulating a useful functional such as E ( t ) goes by the name of 'energy methods' in the PDE literature. The 'energy method' for the heat equation serves as a template in the following sense: many more difficult non-linear PDEs (of which the Navier-Stokes system is one example) admit a version of this 'multiply by the unknown and integrate' trick, and understanding it in the heat equation model helps one to understand those.\n",
      "\n",
      "The main insight in these 'energy method' arguments for both equations was the introduction of the energy functional E ( t ). We saw that for both the heat and wave equations, this functional can be derived by multiplying the equation with an appropriately chosen multiplier-specifically, u for the heat equation and ∂ t u for the wave equation-and then integrating by parts over the domain [0 , T ] × Ω, with T &gt; 0. A mathematical copilot might successfully identify useful and new energy functionals by exploring various chosen multipliers and then attempting to derive useful quantities that can be brought under control by manipulating the resulting integrals. Newly discovered functionals which can be controlled may provide valuable new advances in mathematicians' understanding of solutions to the corresponding equation(s).\n",
      "\n",
      "We now consider typical workflows in low-dimensional topology, which is an area of mathematics that has a unique flavor and uses tools from geometry, algebra, PDEs, group theory, combinatorics, and mathematical physics. Hence, some of these workflows are more challenging to convert to data. Low-dimensional topology is the study of n -manifolds (certain topological spaces that are locally homeomorphic to R n ) of dimension n ≤ 4. A knot is a simple closed curve embedded in R 3 . Knot theory plays a fundamental role in low-dimensional topology as every 3- and 4-manifold can be represented by a framed link; i.e., a collection of knots labeled by integers, called a Kirby diagram [Kirby, 1978]. We refer the reader to Juhász [2023] for more detail on low-dimensional topology and knot theory and explanations of the mathematical terms used below.\n",
      "\n",
      "- A central type of question in this area is classification, which requires being able to show whether two objects (e.g., knots or manifolds) are equivalent. The first step is finding a suitable representation. In case of knots, this could be a projection to the plane, called a knot diagram, a closure of a braid (a number of parallel strands running around a central axis), or a grid diagram. Knot diagrams can be encoded numerically as PD, DT, or Gauss codes, and braids as braid words. Manifolds can be represented using Kirby diagrams, as triangulations, branched covers along links, or as geometric objects (e.g., hyperbolic 3-manifolds).\n",
      "\n",
      "To show that two representations of the same knot or manifold are equivalent, one finds a sequence of certain moves connecting them, which can be Reidemeister moves [Reidemeister, 1927] in case of knot diagrams, Markov moves for braids, or Kirby moves in case of Kirby diagrams. These search\n",
      "\n",
      "problems lend themselves to techniques such as reinforcement learning; see [Gukov et al., 2021]. The workflow steps can be encoded by a sequence of representations of the object, such that consecutive representations are related by one of the standard moves. Software packages such as SnapPy [Culler et al., 2024] can be used to check whether these moves are valid and can also list valid moves. We can hence represent the workflow as follows:\n",
      "\n",
      "choose type of representation and set of moves → find representations of two mathematical objects → repeatedly apply moves to first object → representation of second object.\n",
      "\n",
      "- To show two objects are inequivalent, one defines invariants, which are typically algebraic objects (numbers, polynomials, or groups) that are unchanged by the above moves. These moves often also capture important topological properties of these objects. The most classical invariants are homology groups and the fundamental group. The Alexander polynomial is a knot invariant derived from the fundamental group of the knot complement. Representations of the knot group give rise to the more modern twisted Alexander polynomials. A recent knot invariant rooted in representation theory is Khovanov homology [Khovanov, 2000]. Floer homology [Ozsváth and Szabó, 2004] and gauge theory [Witten, 1994] give rise to highly sophisticated knot and 3- and 4-manifold invariants. These invariants often give lower bounds on hard-to-compute topological quantities such as the 3- or 4-genus of a knot. SnapPy is capable of computing many of these knot invariants when run in SageMath [The Sage Developers, 2024]. A typical workflow would thus be as follows:\n",
      "\n",
      "pair of mathematical objects → choose suitable invariant → compute invariant for objects → show invariants are inequivalent.\n",
      "\n",
      "- To define a Floer-theoretic invariant, we construct a chain complex whose boundary map counts some pseudo-holomorphic curves with certain Lagrangian boundary conditions in some symplectic manifold. To show these counts are finite, one has to prove transversality results for the moduli spaces using difficult methods from PDEs, then obtain a formula for the dimension of the moduli spaces and compactify the moduli spaces. The next step is showing the boundary map squares to zero, in which case we have a chain complex, or sometimes we end up with a more complicated algebraic structure, such as an A ∞ -module or a differential graded algebra. One often has to use special coefficient systems, such as a Novikov ring, and deal with bubbling phenomena. There are many choices that go into the construction of our chain complexes, so one has to prove independence of these choices up to chain homotopy equivalence, which often relies on continuation maps and pseudo-holomorphic polygon counts. One then extracts more tractable invariants from the chain homotopy type using algebra, such as taking homology. Different invariants are often related by spectral sequences, defined using filtrations of the chain complexes. A similar workflow exists for gauge-theoretic invariants. More sophisticated invariants can be obtained by exploiting certain symmetries, such as Z 2 or Pin(2). In another popular direction, one can sometimes construct a homotopy type from chain complexes; see [Lipshitz and Sarkar, 2018]. Certain constructions from algebraic topology can then be invoked to provide additional algebraic structure, such as Steenrod operations. We can hence represent the high-level workflow for defining a Floer-theoretic topological invariant as follows:\n",
      "- associate a symplectic manifold and Lagrangian submanifolds to our topological object → decide what pseudo-holomorphic curves to count → prove transversality of the moduli spaces → find dimension formula for moduli spaces → compactify moduli spaces → choose coefficients and the right algebraic structure → extract invariant using algebra → prove independence of choices.\n",
      "\n",
      "It is apparent from the complexity of the workflow that this is very difficult to turn into data. As the number of workflow steps increase, the number of datapoints has to increase exponentially to cover all possible combinations between steps (although not all combinations may be possible in all cases, which may mitigate this issue).\n",
      "\n",
      "- After an invariant is defined, we study what topological information it captures and derive applications. Here, it is important to know what applications are mathematically relevant. This workflow is very\n",
      "\n",
      "different from many other areas of mathematics, where tools are developed to tackle specific open problems, and should be compared with Section 4.4 on motivated proofs. This step requires sophisticated intuition and a global vision. One way to turn this into data is to define pairs\n",
      "\n",
      "(invariant, topological application) , which could be used by a mathematical copilot to recommend potential applications of a given type of invariant. We want to choose applications that are related to problems from a list of important open questions.\n",
      "\n",
      "- As many of the invariants are hard to compute, it is crucial to develop methods to compute them. For Floer-theoretic 3- and 4-manifold invariants, for example, experts use surgery formulas, spectral sequences, grading arguments, exact triangles, gluing formulas, or computations in explicit diagrams in simpler cases. The difficulty lies in the fact that one has to solve non-linear PDEs to obtain the pseudo-holomorphic curve counts contributing to the boundary maps in the chain complex, which is not algorithmic. Knot Floer homology now has a combinatorial definition and can be computed by SnapPy. It is often helpful to restrict attention to special classes of objects, such as to the class of alternating knots, 3-braid closures, etc., and perform computations or prove results for these. A schematic of this workflow is the following:\n",
      "\n",
      "invariant → method of computation → class of objects to restrict to .\n",
      "\n",
      "Hence, a potential data point could look like\n",
      "\n",
      "(invariant, method of computation, class of objects) .\n",
      "\n",
      "Certain knot invariants are easy to define, but no algorithm is known to compute them, such as the unknotting number or the 4-ball genus, and topologists use computable invariants, many of them arising from knot Floer homology, to give lower bounds on these. The paper [Davies et al., 2021] describes a workflow for using supervised learning to find correlations between various invariants, which has led to an inequality relating the knot signature and hyperbolic invariants. Upper bounds can be obtained by performing certain moves on knot diagrams, possibly with the help of reinforcement learning or Bayesian optimization; see [Gukov et al., 2023] and [Applebaum et al., 2024]. For example, one could take (4-ball genus, lower bound from Rasmussen s -invariant, torus knots) , which leads to a solution of Milnor's conjecture on the 4-ball genus of torus knots [Rasmussen, 2010].\n",
      "\n",
      "- The classification of smooth 4-manifolds is still wide open. There exist 4-manifolds that are homeomorphic but not diffeomorphic, which are called exotic pairs. To construct these, experts use methods from algebraic and symplectic geometry, such as blow-ups, fiber sums when given Lefschetz fibrations, and other gluings, typically along 3-tori, such as knot surgery. To show two 4-manifolds are homeomorphic, one shows they have isomorphic intersection forms and invokes Freedman's theorem [Freedman, 1982], for which the fundamental group has to satisfy certain restrictions (e.g., being trivial). The intersection form and a presentation of the fundamental group can be read off a Kirby diagram. This fundamental group computation is often the most difficult step, and note that any finitely presented group can arise as the fundamental group of a closed 4-manifold. Furthermore, there is no algorithm to decide whether a finitely presented group is trivial. So, some questions in low-dimensional topology are beyond the reach of computers. As mentioned above, one can use Kirby calculus to show if two 4-manifolds are diffeomorphic. Swenton's Knot-Like Objects software 14 is capable of performing Kirby moves. To show they are not diffeomorphic, one usually distinguishes them using the gauge-theoretic Seiberg-Witten invariants. No algorithm is known to compute these, but they can be computed in some instances using Taubes' non-vanishing result for symplectic 4-manifolds [Taubes, 1994], together with various glueing results, such as Fintushel and Stern's knot surgery formula [Fintushel and Stern, 1998]. A schematic of a typical such workflow in smooth 4-manifold topology is as follows:\n",
      "\n",
      "14 https://community.middlebury.edu/~mathanimations/klo/\n",
      "\n",
      "pair of smooth 4-manifolds → find their Kirby diagrams → show they are simply-connected → show they have isomorphic intersection forms (hence homeomorphic by Freedman) → try to show they are diffeomorphic using Kirby calculus; if this fails → compute Seiberg-Witten invariants to show not diffeomorphic.\n",
      "\n",
      "The last step, the computation of Seiberg-Witten invariants, is discussed in more detail in the previous bullet point. Computer algebra systems can be useful for the computation of the fundamental group and the intersection form.\n",
      "\n",
      "3-manifold topology has a completely different flavour. Here, methods from geometric group theory, hyperbolic geometry, and combinatorial topology dominate. These combinatorial methods include triangulations and normal surface theory. Geometric group theory studies groups using their actions on metric spaces (e.g., on the Cayley graph). Also note Perelman's proof of the 3-dimensional Poincaré conjecture using the Ricci flow, rooted purely in geometric analysis; see [Morgan and Tian, 2007]. Well-developed computer packages exist to aid 3-manifold topologists, including the already mentioned SnapPy for hyperbolic 3-manifolds and Regina [Burton, 2004] for triangulations. We do not provide specific workflows in 3-manifold topology due to the diversity of techniques used.\n",
      "\n",
      "Having discussed differential equations and low dimensional topology, we now turn to the Langlands philosophy, a far-reaching series of conjectures relating the continuous to the discrete. The story starts with Gauss' law of quadratic reciprocity, a striking result explaining a rather counterintuitive relationship between two apparently unrelated questions; given two odd primes p and q , there is an unexpected symmetry relating the questions of whether p is a square modulo q , and whether q is a square modulo p . Gauss' theorem can be thought of as one of the first nontrivial results in arithmetic. In the 1800s Eisenstein extended the result to higher powers, and ultimately the results were recast as class field theory, one of the highlights of 19th century mathematics. The advent of cohomology (a tool initially introduced to study questions in topology) after the war clarified things further, and by the 1950s the main proofs (now in the form of Artin reciprocity) were well-understood, although they even now remain frustratingly long. For example the 1968 book by Cassels and Froehlich Cassels et al. [1976] which contains the proofs of the main theorems is over 300 pages long, and the proofs have not become substantially shorter since.\n",
      "\n",
      "The bombshell dropped by Langlands in the 1960s was to observe that class field theory itself was merely the 1-dimensional case of a far more profound nonabelian reciprocity law relating objects from harmonic analysis (the continuous) and objects from arithmetic (the discrete). In short, there seemed to be a collection of analytic objects ('automorphic representations') each of which was related to a corresponding algebraic object (a 'Galois representation'). Rest assured that the details of the (highly technical) definitions are irrelevant to the story being told here; the key point is that automorphic representations are objects coming from harmonic analysis, and Galois representations are objects coming from arithmetic, and these are (on the face of it) two very different areas of mathematics.\n",
      "\n",
      "Whilst the initial idea was compelling, it took some time before the depths of Langlands' insights could be turned into rigorous mathematical statements , with early attempts involving data whose existence was only conjectural (for example the global Langlands group of a number field, an object whose definition is still unclear). On the other hand, a wealth of evidence accrued very quickly, making it absolutely clear that beneath the 'philosophy' there were some beautiful mathematical statements waiting to be proved. In the 60 years since Langlands' ideas were first enunciated, there have been generalizations to other domains of mathematics; there are local and global Langlands conjectures, a p -adic Langlands philosophy, a mod p philosophy (both of which are still wide-open and whose statements are still not completely understood), and geometric Langlands conjectures, where our understanding is much better and there is even a recent announcement of a proof in many cases, which was picked up by the media, although the papers have not yet been published.\n",
      "\n",
      "Let us stick to the classical local and global conjectures, which already raise plenty of questions. What do the workflows look like in this area?\n",
      "\n",
      "- There is the fight to understand the question. Part of the subtlety of this part of mathematics is that whilst there is a general idea that certain automorphic representations (objects from analysis) should be in correspondence with certain Galois representations (objects from arithmetic), it is still not clear how to uniquely pin down such a correspondence (for general reductive groups over number fields, say). One can write down a list of criteria which the correspondence should satisfy, but what mathematicians really seek is some kind of constructive or 'canonical' construction of an object on one side of the conjectured correspondence, given an object on the other side. Without this definition, it is impossible to formally state the theorems we would like to prove. In particular, the main goal here is not to compute a value or to prove a theorem, it is to construct a function.\n",
      "- However there are plenty of instances of rigorous mathematical conjectures which we can write down and for which we have ample evidence in the form of both numerical examples and partial results. Here is a conjecture which can be made completely precise. Let n be a positive integer. Given an algebraic automorphic representation for the group GL ( n ) over a number field, one expects to be able to associate to it a compatible family of n -dimensional p -adic Galois representations of this number field, and conversely given the family of Galois representations one expects the existence of an automorphic representation. This statement is fully proved in the case n = 1 by class field theory. What little we do know for n &gt; 1 is for the most part restricted to totally real and CM number fields (which together form 0 percent of all number fields, one fact indicating the depths of our ignorance) and what litle we do know has been very hard-won. The key difficulty is to find some kind of bridge taking us between the discrete and the continuous, and there are limited tools available. Certain automorphic representations can be studied using differential geometry. Serre's work linked differential geometry and algebraic geometry in the 1950s; Grothendieck's revolution of algebraic geometry in the 1960s gave links to arithmetic, and thus provided the first viable bridge in the case of n = 2. Algebraic geometry was the source of the first general constructions of elliptic curves from modular forms, provided by Eichler and Shimura at the end of that decade. This work was simultaneously a milestone achievement and a miniscule special case of the Langlands program for GL (2) over the rationals. We had to wait for 25 years before Wiles and Taylor managed to go from an elliptic curve back to a modular form; Fermat's Last Theorem was a consequence. In particular a major problem in number theory was resolved by proving an extremely small part of Langlands' conjectures. One of the major difficulties here is that the arguments need nontrivial inputs from arithmetic, algebra, geometry and analysis, and would take thousands of pages to explain if written out in full. These thousands of pages establish just a very small special case of the Langlands program for GL (2) over the rationals. In some sense it does not bode well that the GL (1) proofs were hundreds of pages long and even a small fragment of the GL (2) proofs are thousands of pages. Within these pages there are of course several profound ideas; many more seem to be needed. There are few living humans who have read and completely understood the full proofs of the modularity theorem linking elliptic curves and modular forms; on the other hand it is certainly possible to understand a high-level overview, and most people take on parts of the argument as 'black boxes' where the statements of the results are comprehensible but the proofs might involve techniques which are not part of their expertise. In particular the workflow here does not involve understanding all the details of everything; communication between mathematicians with different expertise is key here, as well as taking results from other areas of mathematics on trust.\n",
      "- The success of Wiles' program showed in particular that algebraic geometry gave a viable route to cross the chasm between the discrete and the continuous and link objects on both sides. However the techniques will only apply when certain quite restrictive conditions hold. Another tool in the area is 'Langlands functoriality', a tool for producing new analytic objects from old ones based on predictions on the arithmetic side of the picture. If the Langlands conjectures are true then basic constructions in arithmetic should have analogues on the analytic side, and one can independently try and make those constructions. Another motivation for making such constructions is that one might start with an analytic object for which the algebro-geometric tools do not apply, and then attempt to massage it until they do apply. Many of the results known for GL ( n ) for n &gt; 2 are proved in this way; the group GL ( n )\n",
      "\n",
      "does 'not admit Shimura varieties' for n &gt; 2, meaning that algebraic geometry has limited applicability; however if one can use analytic means to move from an automorphic representation on GL ( n ) to an automorphic representation on a unitary group (where the methods do work) then this enables more analytic objects to 'cross the bridge' to the algebraic side. Results in Langlands functoriality thus have the dual role of adding weight to the conjectures (by proving consequences of them) and also enabling us to prove more cases of the conjectures. Here the work is Fourier-analytic and group-theoretic, with less reliance on algebraic and arithmetic geometry.\n",
      "\n",
      "- As is often the case in mathematics, people attempt to understand the questions in this area by generalizing them. The Galois representations which the Langlands philosophy are p -adic in nature, and can move freely in p -adic families. This raises the question of whether one can p -adically deform automorphic representations; initially this sounds ridiculous because these objects are inherently complex-analytic. However p -adic analytic variants of the theory have now been established, and this gives us new freedom, unlocking new tools to establish more cases of the classical program; the key observation here is that p -adic analysis is far more closely related to arithmetic than classical analysis. What is happening in this area is a mixture of insights gleaned from analogy ('what if there is a more arithmetic version of analysis?') turn into an actual theory (the existence of p -adic automorphic representations) but then must be backed up by very technical work showing that the theory can actually be used to solve new cases of the problem at hand. Work in this area thus needs to blend heuristic ideas and technical calculations.\n",
      "\n",
      "Further considerations. An important aspect that needs to be considered when mapping mathematical workflows to data is that different representations of the same data can lead to different mathematical properties that can change the method of proof, as well as the foundational model most suited for the chosen representation.\n",
      "\n",
      "For example, it is an open problem in knot theory whether there exists a polynomial-time algorithm that can detect whether a given knot, which is an embedded circle in R 3 , can be simplified (without breaking it) to a standard round circle. Instead of describing the knot as a curve in R 3 , knots are often specified in terms of their projection to two dimensions, and data representations of the knots keep track of which strands are above and which are below along the projection ray.\n",
      "\n",
      "The above-mentioned question then becomes to find a sequence of deformations of the embedded circle such that its projection has no crossings. It is known [Kauffman and Lambropoulou, 2012] that for some data representations of the projected knot (such as Dowker-Thistlethwaite codes [Dowker and Thistlethwaite, 1983] with Reidemeister moves as allowed deformations [Reidemeister, 1927]), there exist examples where the number of crossings needs to be increased before all crossings can be removed. In another representation (grid representation with Dynnikov moves as allowed deformations), the number of crossings is monotonically decreasing [Dynnikov, 2006], but both the representation of the knot as a grid diagram and the carrying out of the simplification steps are more complicated. These distinct representations of the same mathematical object lend themselves to different foundational models [Gukov et al., 2021, Kauffman et al., 2022]; for example, braid word representations of knot projections are closer to natural language, whereas other representations are closer to vision tasks or graphs. The different representations also inform the type of algorithm that needs to be used. If the simplification is non-monotonic, a local search can get stuck in local minima.\n",
      "\n",
      "One common activity in mathematics, when confronted with a new statement of unknown truth, a conjecture, is to engage in the search for proof - or a counterexample. This comes with its own workflows. The search for counterexamples to conjectures is particularly useful for hard problems with a truth certificate that can be verified in polynomial time, such as NP-hard or NP-complete problems. The idea is to cast the search problem into a Markov Decision Problem (MDP) whose terminal states are counterexamples, and attempt to solve the MDP using data science techniques. In the past, deep reinforcement learning has proven to be a powerful tool. If the RL agent finds a solution to the MDP, its episodic rollouts provide truth certificates for the counterexample, thus establishing a verifiably correct proof by counter-example, see e.g. [Gukov et al.,\n",
      "\n",
      "2024] for a recent summary of these ideas and [Gukov et al., 2021, Wagner, 2021, Gukov et al., 2023, Charton et al., 2024] for some recent concrete applications.\n",
      "\n",
      "In the formal setting, Learning to Disprove frames counterexample search in Lean 4 as proposing a candidate counterexample and then certifying it by producing a Lean proof term, using a large synthetic corpus of counterexample instances [Li et al., 2025b].\n",
      "\n",
      "## 4.2 Meta-Reasoning\n",
      "\n",
      "Many of the current evaluations of mathematical reasoning abilities focus in reasoning inside a particular problem. In a mathematics competition or a class, that is generally enough: it is assumed that all problems are solvable, that solutions are not too long, and furthermore that all the necessary tools come from a known, bounded set (e.g., of topics allowed in competitions, or pre-requisites for the class). At the research frontier, however, a substantial amount of work is spent at the meta-level, where these assumptions are constantly being evaluated. The failure to prove a statement might be due to it being false (and perhaps a counter-example can be extremely non-trivial to produce). Or, the appropriate tools might not yet exist. In either case, the problem-level assumptions above might be violated, and one has to constantly decide whether to keep pushing on the current plan, and what to do if not. To the best of our knowledge, none of the current evaluations for mathematical reasoning directly probe these meta-level judgements, even though they represent an important opportunity for assistance from research copilots.\n",
      "\n",
      "Some of the most basic meta-judgements that we might want to evaluate are (a) is a given statement true? (and does that change in light of given failed attempts to prove it?) (b) if it is, how much work will it take to prove it (or, given a partial proof, how much work is left?), and (c) is a given strategy adequate to prove it? In the formal setting, LeanProgress trains a predictor of proof progress from Lean proof states and uses it to guide search on longer Mathlib proofs [Huang et al., 2025]. Many examples that we could use to evaluate the ability to answer these questions could be extracted from finished works: papers have examples of true statements; removing assumptions from those statements often renders them false (and most papers justify why the assumptions are needed, if not apparent, such as by constructing counter-examples where those assumptions are violated). We can measure the complexity of existing proofs in various ways (e.g., length, or by their dependencies inside a paper) to evaluate predictions of difficulty - these can also be made relative, e.g., identifying which lemma in the context of a broader proof is going to take the most work. On the other hand, many important meta-level decisions cannot be readily observed in finished work, such as when to change the problem locally (such as adding stronger assumptions), or switching problems entirely - perhaps temporarily, e.g., to gain intuition on a special case first, or more firmly, and in that case deciding what to work on instead. For such decisions, one might need to observe mathematicians doing their work , as opposed to their finished products, as we discuss next.\n",
      "\n",
      "## 4.3 Data Collection in Real Environments\n",
      "\n",
      "To gather representative data on mathematical workflow steps, as well as other metamathematical items, such as proof heuristics, limitations of certain proof techniques, etc., it may be desirable to observe and absorb the full process of producing mathematics, including all data that is not represented in written, textual form. For an example of real-time narration of thought processes that arise when doing competitive problem-solving, see co-author Tim Gowers' series of YouTube videos. 15\n",
      "\n",
      "Unfortunately, many of these intermediate process steps become evident only implicitly from data sources. For example, at the elementary level, various workflows to solve various limits, such as lim x → 0 sin x -x x 3 or lim x →∞ x 1 /x , which require repeated applications of known theorems, such as de l'Hôpital's theorem, or related ones to resolve indeterminate cases, are distilled only by solving a large number of exercises; no explicit annotations for these workflows exist. On a more advanced level, these are often conveyed in blogs,\n",
      "\n",
      "15 https://www.youtube.com/@TimothyGowers0/videos\n",
      "\n",
      "talks, and oral discussions between mathematicians at conferences - and not in the typical data sources used to train LLMs.\n",
      "\n",
      "In principle, one could implement longitudinal studies that track research projects from inception to completion. However, such data collection would require classifying, structuring, and recording different tasks throughout the mathematician's daily work. The apparent challenge is to scale such approaches while keeping the overhead as small as possible for the researcher whose work is tracked. The KWARC group at FAU Erlangen-Nürnberg has developed infrastructure for semantic mathematical documents and tested such systems with hundreds of students per semester [Kohlhase, 2006, Berges et al., 2023], demonstrating the feasibility of structured mathematical data collection at scale, though their focus has been primarily on finished documents rather than the discovery process.\n",
      "\n",
      "While such approaches are welcome to be tested, as a first alternative step, we recommend transcribing from online sources such as lectures, panel discussions, seminars, vlogs, etc. VideoMathQA evaluates mathematical reasoning grounded in lecture-style videos, including long videos, with questions that require integrating visual and spoken information across time [Rasheed et al., 2025]. While such data only requires minimal need for postprocessing and is often already collected in the form of videos, it typically covers broader mathematical aspects than textbooks. However, while humans can frequently generalize from a single instance of a certain workflow, LLMs often require a dataset that contains sufficiently many examples. While data collection effort will elicit higher-quality mathematical data, the time lag between the time when new research discoveries are made and when these are represented in lectures can be significant. On the other hand, at venues such as conferences, a lot of information about mathematics is exchanged, but not recorded. While it would technically be possible to record a portion of the conversations that happen at a conference in a privacy-preserving manner, and this would be a great source of data, as one would consistently have data that is at the forefront of research (unlike the mentioned panel discussions, where there is often a time lag), it would require a paradigm shift from conference participants to accept such data collecting measures. We urge further conversations about privacy-preserving ways of curating richer datasets on mathematical workflows, that minimally impinge on mathematicians' naturalistic practices.\n",
      "\n",
      "## 4.3.1 From Artifacts to Trajectories in Formal Proof Engineering\n",
      "\n",
      "One distinctive advantage of formal mathematics is that the development environment already produces fine-grained signals that can be turned into datasets, which are in turn used to improve future systems. These signals include elaborator diagnostics, proof states, tactic traces, resource usage (heartbeats), and even lint warnings about unstable or redundant proof steps. This makes formal proof engineering a promising setting for process-level datasets.\n",
      "\n",
      "One such dataset type is a repair trajectory corpus : sequences of (i) an attempted proof edit, (ii) compiler/LSP diagnostics (type errors, missing imports, failing goals, timeouts), and (iii) the subsequent patch. Such data directly represents the compile-diagnose-fix loop that dominates real formalization workflows, and it can be used to train copilots that are useful beyond one-shot 'fill the sorry ' tasks.\n",
      "\n",
      "A second such dataset type is a refactoring corpus : pairs of proofs before/after refactoring steps that improve readability, reduce instance pollution, decrease resource usage, or align with library style. For example, proof refactoring guidance emphasizes extracting helper lemmas, compiling after each extraction, and using fast diagnostic feedback loops that (i) test compilation after each extraction, and then (ii) use LSP diagnostics to locate natural boundaries. Such datasets would support copilots that can shepherd code through review, not merely produce a valid proof.\n",
      "\n",
      "As a third example, stability and hygiene linting (e.g., flagging tactic instability or unused steps) can be recorded as structured metadata. This enables benchmarks that measure not just whether a proof checks, but whether it is stable across versions and robust under realistic resource constraints.\n",
      "\n",
      "## 4.4 Motivated Proofs\n",
      "\n",
      "We do not doubt that there is substantial value in bespoke AI tools that can automate specific elements of a mathematician's workflow. However, if the goal is not only to provide correct proofs but also to enhance a user's mathematical understanding of the results [Zhang et al., 2023a] and guide new discoveries, we need new tooling - and we argue, new kinds of data. In particular, we argue for data which faithfully represents the process of proof discovery instead of proof exposition . Towards this goal, we introduce motivated proofs [Pólya, 1949, Morris, 2019], which contain and make transparent more of the proof discovery process. We argue for their value as a standard for LLM evaluation. We then provide some examples and discuss some preliminary observations about current LLM's ability to construct and identify motivated proofs. Finally, we speculate on a path towards large-scale evaluation of a model's ability to produce motivated proofs.\n",
      "\n",
      "Broadly speaking, a motivated proof is one that makes clear to the reader where each step comes from. For example, many interesting proofs require one to find a mathematical object with certain properties. An unmotivated proof will simply specify the object and check that it has the desired properties, while a motivated proof will explain how to arrive at the object.\n",
      "\n",
      "## 4.4.1 Examples of motivated proofs\n",
      "\n",
      "We present two theorems here with examples of motivated proofs. Further examples can be found in Appendix A.\n",
      "\n",
      "Cantor's theorem Cantor's theorem states that there is no surjection from a set X to its power set P ( X ). The proof proceeds by letting f : X → P ( X ) be a function and trying to find a subset of X that is not in the image of f . From here, an unmotivated proof will simply exhibit a set, namely, { x ∈ X : x / ∈ f ( x ) } , and verify that it is not in the image of f , which turns out to be straightforward.\n",
      "\n",
      "/negationslash\n",
      "\n",
      "By contrast, a motivated proof will systematically search for the required subset of X . Not knowing which subset to take, we can treat the subset as an unknown, just as we do when solving an equation, and try to narrow down the possibilities. The most general subset of X can be expressed as { x ∈ X : P ( x ) } for some as yet unspecified property P . We now want to prove, for an arbitrary element y of X , that f ( y ) = { x ∈ X : P ( x ) } . So we need either an element x of f ( y ) such that ¬ P ( x ) or an element x of the complement of f ( y ) such that P ( x ). There are not many elements around, so trying y is one of the first things to do, and then we find that we need either y ∈ f ( y ) and ¬ P ( y ) or y / ∈ f ( y ) and P ( y ). And now the property y / ∈ f ( y ) is forced on us as our choice of P , and we end up with the same set as before, but this time with its origin explained.\n",
      "\n",
      "Nilpotent units Now we consider an early result from commutative algebra. Let R be a commutative ring, and let x ∈ R be nilpotent, then (1 + x ) is a unit, meaning it has a multiplicative inverse. This is typically proved by naming an element y = ∑ r -1 k =0 ( -1) k · x k and verifying that this is a multiplicative inverse element by calculation. We regard this as unmotivated, since the discovery process of finding y was left out.\n",
      "\n",
      "For a motivated proof, we need to search for an inverse element. Not knowing which element to take, we parametrize the most generic element we can. Since the only known elements of R are 1 and x , the most generic element is an integer polynomial in x , ∑ m k =0 a k x k . For this to be a right inverse of 1 + x , we must have that 1 = (1 + x )( ∑ m k =0 a k x k ) = a 0 + ∑ m k =1 ( a k + a k -1 ) x k + a m x m +1 . For this to hold, we must eliminate all coefficients of the polynomial besides the constant term, which should be 1. This gives us that a 0 = 1, a k = a k -1 for 1 ≤ k ≤ m and a m = 0. The first two equations give us that a k = ( -1) k for all k , but this contradicts the last equation.\n",
      "\n",
      "But we also know that x is nilpotent, so if we have r such that x r = 0, then all coefficients from x r onwards can be ignored. This solves our issue, as we can let m = r -1 to remove the a m = 0 condition, and we are left with a k = ( -1) k , so (1 + x ) is a unit with inverse ∑ r -1 k =0 ( -1) k · x k .\n",
      "\n",
      "## 4.4.2 Motivated proofs as an evaluation metric\n",
      "\n",
      "The standard for a proof's acceptance into the mathematical literature has historically been correctness rather than a completely motivated account. Moreover, mathematicians have been incentivized to condense and refine their proofs for reasons such as page limits in journals, which has further widened the gap between proof discovery and proof exposition . To assess a model's capabilities in aiding proof discovery, we suggest that holding the proof to the standard of being motivated provides advantages over requiring only correctness.\n",
      "\n",
      "As argued in [Pólya, 1949, Morris, 2019], a motivated proof is more informative to the reader, as it provides more insight into how the proof was discovered and how the reader might discover similar results themselves. More speculatively, a model capable of producing motivated proofs will likely generalize better to novel problems. The first reason for this conjecture is that most results in the literature are not fully motivated, and a model that is capable of recognising this is less likely to blindly recall its training data. Requiring proofs to be motivated also restricts the amount of brute-force search a model can perform, so performance depends more on reasoning ability and less on test-time computational power. A dedicated study to test whether models that generate motivated proofs indeed generalize better to novel problems could be worth pursuing, because it might reshape how we approach automated reasoning.\n",
      "\n",
      "We will now briefly evaluate current LLMs' ability to generate motivated proofs, as well as to evaluate whether a given proof is motivated.\n",
      "\n",
      "Generation of motivated proofs As a preliminary investigation, we prompted o1-preview and Llama 3.2-90B to produce motivated proofs of five mathematical results. Our goal was not to use best-in-class LLMs since they typically differ only by a few percentages on common benchmarks; in contrast, we wanted to obtain a preliminary assessment of whether LLMs have the ability to work with the concept of a motivated proof at all. The results on which we tested the abilities to generate motivated proofs were selected to be elementary but to require a slightly non-trivial construction. After defining motivated proofs, we provided an in-context example of a motivated proof of the other four results and asked the model to generate a new motivated proof. If the model failed to provide a motivated proof, we would prompt it once to fix it, pointing out the unmotivated step. Each model was asked twice per problem, first with only examples of motivated proofs in the context, following a traditional chain-of-thought approach, then again with both examples and non-examples, mimicking contrasting chain-of-thought prompting. The problems and associated proofs we used in the prompts can be found in Appendix A.\n",
      "\n",
      "In most responses, the model showed some evidence of 'understanding\" the concept of motivated proofs by avoiding unmotivated constructions. However, this often led to pseudo-motivations, where some justification was given before a construction, but it didn't provide any useful substance besides obscuring the unmotivated step (and often appealed to some irrelevant fallacies). In the example below, the unmotivated step of Cantor's theorem is relegated to the statement 'An important question arises: Does x belong to its own image f ( x )?\".\n",
      "\n",
      "## o1-preview (excerpt)\n",
      "\n",
      "Our goal is to find a subset of X that cannot possibly be the image of any element under f . To do this, we'll consider how elements of X relate to their images under f .\n",
      "\n",
      "## Exploring the Relationship Between Elements and Their Images:\n",
      "\n",
      "For each element x ∈ X , f ( x ) is a subset of X . An important question arises: Does x belong to its own image f ( x ) ?\n",
      "\n",
      "This leads us to categorize the elements of X into two groups:\n",
      "\n",
      "1. Elements that are members of their image: x ∈ f ( x ).\n",
      "2. Elements that are not members of their image: x / ∈ f ( x ).\n",
      "\n",
      "The other common issue was the model making completely unsound deductions when trying to motivate their\n",
      "\n",
      "construction, only to arrive at the correct result by chance. This behavior is consistent with the model not fully understanding the result but having seen it in their training data and attempting post-hoc rationalization. Below is an example:\n",
      "\n",
      "## Llama-3.2 (excerpt)\n",
      "\n",
      "We want to find a subset Y ⊆ X such that φ ( x ) = Y for some x ∈ X . However, we don't know which subset to choose.\n",
      "\n",
      "Let's try to find a property that Y should satisfy. Since φ ( x ) = Y , we know that x ∈ φ ( x ) = Y . But we also know that x / ∈ φ ( x ), because if x ∈ φ ( x ), then φ ( x ) would contain at least two elements: x and φ ( x ). This suggests that Y should be a subset of X that contains some elements of X , but not all of them. In particular, Y should not contain x itself.\n",
      "\n",
      "With this in mind, let's define Y = { x ∈ X | x / ∈ φ ( x ) } .\n",
      "\n",
      "Here, the contradictory claims that x ∈ φ ( x ) and x / ∈ φ ( x ) have no basis, and if true would imply an immediate contradiction, making the remainder of the proof redundant. Also, the condition for Y is in fact the opposite of what is needed, but this error doesn't seem to affect the remainder of the model's reasoning.\n",
      "\n",
      "Other problems provided more hopeful examples: a motivated proof for the large totients problem was successfully generated both times by o1-preview and once by Llama.\n",
      "\n",
      "Overall, o1-preview seemed to outperform Llama on most problems. We also observed a slight improvement by using negative as well as positive examples to 'explain\" the concept. As an unfortunate side effect, the extra information also increased the tendency to 'obscure\" unmotivated arguments as discussed above.\n",
      "\n",
      "Recognition of motivated proofs To conduct a larger scale investigation of a model's ability to generate motivated proofs, it would help if there was an existing 'judge\" model which could determine whether or not a given proof was motivated. In any case, recognition of motivated proofs represents an easier sub-goal towards generating motivated proofs.\n",
      "\n",
      "We performed a small, preliminary investigation of whether current state-of-the-art models can reliably distinguish a motivated proof from an unmotivated proof. We stress that this is only a preliminary investigation and do not claim to draw any strong conclusion, but hope that this will inspire the community to carry out larger evaluations in this regard and work towards a standard of evaluating motivated proofs.\n",
      "\n",
      "We first tested the model's ability to identify whether a given proof was motivated. We tested the five problems from the previous section, where each problem had three proofs to be evaluated individually. This included two human-written proofs, which we judged to be paradigmatic examples of motivated and unmotivated proofs, one unmotivated machine-generated proof and, where available, one motivated machine-generated proof. The unmotivated machine-generated proof was the first correct but unmotivated proof generated by Llama-3.2 in the previous section. For the machine-generated motivated proof, we took the response in the previous section that we judged to be most motivated, which was generated by Llama in the integer sum problem and o1-preview for nilpotent units and large totients, while for the other problems no responses were deemed sufficiently motivated.\n",
      "\n",
      "For some proofs where there were incorrect but largely inconsequential components, we manually altered some equations to enforce correctness and removed any opening or closing sentences that included the phrase 'motivated proof\" to avoid misleading the model. As with generation, we prompted the model by explaining the definition of a motivated proof and giving an example and non-example for each of the other problems, where the order of examples and non-examples was alternated to ensure that the model wasn't learning the ordering. Each model was given three attempts per proof per problem, and the results are recorded below. We use HM, HU, MM and MU as abbreviations for 'human motivated', 'human unmotivated' and 'machine unmotivated' respectively.\n",
      "\n",
      "|                  | o1-preview HM   | HU    | MM   | MU    | Llama-3.2 HM   | HU   | MM   | MU   | Total TP   | TN    |\n",
      "|------------------|-----------------|-------|------|-------|----------------|------|------|------|------------|-------|\n",
      "| Cantor's theorem | 3/3             | 3/3   | -    | 3/3   | 3/3            | 0/3  | -    | 0/3  | 6/6        | 6/12  |\n",
      "| Small doubling   | 3/3             | 3/3   | -    | 2/3   | 3/3            | 3/3  | -    | 2/3  | 6/6        | 10/12 |\n",
      "| Integer sum      | 3/3             | 3/3   | 3/3  | 2/3   | 2/3            | 3/3  | 3/3  | 0/3  | 11/12      | 8/12  |\n",
      "| Nilpotent units  | 3/3             | 3/3   | 3/3  | 3/3   | 2/3            | 3/3  | 3/3  | 0/3  | 11/12      | 6/12  |\n",
      "| Large totients   | 3/3             | 3/3   | 3/3  | 3/3   | 3/3            | 2/3  | 3/3  | 1/3  | 12/12      | 9/12  |\n",
      "| Total            | 15/15           | 15/15 | 9/9  | 13/15 | 13/15          | 8/15 | 9/9  | 3/15 | 46/48      | 39/60 |\n",
      "\n",
      "Under this setup, o1-preview performed strongly, while Llama was heavily biased towards accepting a proof as motivated. A possible explanation for Llama's poor performance in the MU set is that the machine-generated unmotivated proofs, unlike the human-generated ones, were attempts at generating motivated proofs, so these proofs contained some superficial features such as proof length and 'chatty\" language that the model had associated with motivated proofs.\n",
      "\n",
      "In an effort to address this issue, we re-framed the experiment as a binary choice task. This was largely the same as above, but instead of being given a single proof to judge, the models were given two proofs and were told that one was motivated and one was unmotivated, and to judge which was motivated. Again we recorded the success rate over three valid attempts per (unmotivated,motivated) pair, per problem. The results are shown below.\n",
      "\n",
      "| Unmotivated proof source   | o1-preview Human   | Model   | Llama-3.2 Human   | Model   | Total   |\n",
      "|----------------------------|--------------------|---------|-------------------|---------|---------|\n",
      "| Cantor's theorem           | 3/3                | 3/3     | 3/3               | 3/3     | 12/12   |\n",
      "| Small doubling             | 3/3                | 3/3     | 3/3               | 3/3     | 12/12   |\n",
      "| Integer sum                | 6/6                | 6/6     | 6/6               | 6/6     | 24/24   |\n",
      "| Nilpotent inverses         | 6/6                | 3/6     | 0/6               | 2/6     | 11/24   |\n",
      "| Large totients             | 6/6                | 4/6     | 6/6               | 5/6     | 21/24   |\n",
      "| Total                      | 24/24              | 19/24   | 18/24             | 19/24   | 80/96   |\n",
      "\n",
      "This improved evaluation method narrowed the gap between the two models, especially when using the MU proof which Llama previously struggled with, providing some evidence that the binary choice strategy reduces the model's tendency to be misled by superficial features common to motivated proofs.\n",
      "\n",
      "We stress that this is only a preliminary investigation, and that a larger investigation would be required to make strong claims about an LLM's ability to judge motivated proofs. We welcome the community to conduct a larger investigation, advising the following notes of caution based off our experience and some speculation:\n",
      "\n",
      "- It is easier for models to decide between two proofs, which is motivated, than to make an absolute judgement of a single proof.\n",
      "- Models can be sensitive to the order in which the proofs are presented, so it is important to permute these (including the in-context examples).\n",
      "- Models can be misled by superficial features common to motivated proofs, so it is important that false examples also have these features. We achieved this to some extent by using an LLM's failed attempt at writing a motivated proof, but this could potentially also be achieved with care in a human-generated dataset.\n",
      "\n",
      "Replication with newer models Given the significant improvements in model performance since the release of an earlier preprint, we have replicated these investigations with newer models. Although these are still preliminary investigations, we acknowledge the risk of data contamination.\n",
      "\n",
      "We first replicated the generation task, prompting each model to generate a motivated proof of each result given an example and non-example of each other result. Each task was repeated three times, with each\n",
      "\n",
      "response manually classified by the authors and the results tabulated below:\n",
      "\n",
      "|                    | OpenAI GPT-5.2 Pro   | DeepSeek Prover-V2   | Google Gemini 3 Pro   | OpenAI GPT-OSS 120B   | Total   |\n",
      "|--------------------|----------------------|----------------------|-----------------------|-----------------------|---------|\n",
      "| Cantor's theorem   | 3/3                  | 0/3                  | 3/3                   | 0/3                   | 6/12    |\n",
      "| Small doubling     | 1/3                  | 0/3                  | 2/3                   | 1/3                   | 4/12    |\n",
      "| Integer sum        | 3/3                  | 0/3                  | 3/3                   | 3/3                   | 9/12    |\n",
      "| Nilpotent inverses | 3/3                  | 0/3                  | 1/3                   | 3/3                   | 7/12    |\n",
      "| Large totients     | 3/3                  | 3/3                  | 3/3                   | 3/3                   | 12/12   |\n",
      "| Total              | 13/15                | 3/15                 | 12/15                 | 10/15                 | 38/60   |\n",
      "\n",
      "This simple investigation was sufficient to observe substantial variation between models, although we believe that more difficult problems are needed to meaningfully distinguish the top models. It is also noteworthy that both closed-source models consistently succeeded on Cantor's Theorem, which neither of the older models could succeed on. It is also interesting that DeepSeek Prover-v2 performed poorly despite its mathematical specialization, which is consistent with the hypothesis that heavy fine-tuning for formal proof generation may reduce performance on stylistically different proofs.\n",
      "\n",
      "We then repeated the binary classification task with newer models, concentrating on the more difficult and relevant machine-machine distinction, using proofs generated in the previous task. The first (motivated,unmotivated) pair was generated by GPT-5.2 Pro and DeepSeek Prover respectively, and the differences were stark enough that every model was correct 100% of the time. For the second pair we compared Gemini's correct responses to a varied selection of incorrect model responses (including Gemini's where possible). We used a human-written unmotivated proof for the problem of 'large totients\", since none of the newer models produced an unmotivated proof. The performance of each model on the second pair is shown below.\n",
      "\n",
      "| Theorem            | OpenAI GPT-5.2 Pro   | DeepSeek Prover-V2   | Google Gemini 3 Pro   | OpenAI GPT-OSS 120B   | Total   |\n",
      "|--------------------|----------------------|----------------------|-----------------------|-----------------------|---------|\n",
      "| Cantor's theorem   | 2/3                  | 1/3                  | 3/3                   | 1/3                   | 7/12    |\n",
      "| Small doubling     | 3/3                  | 0/3                  | 0/3                   | 3/3                   | 6/12    |\n",
      "| Integer sum        | 3/3                  | 3/3                  | 3/3                   | 3/3                   | 12/12   |\n",
      "| Nilpotent inverses | 3/3                  | 1/3                  | 3/3                   | 3/3                   | 10/12   |\n",
      "| Large totients     | 3/3                  | 3/3                  | 3/3                   | 3/3                   | 12/12   |\n",
      "| Total              | 14/15                | 8/15                 | 12/15                 | 13/15                 | 47/60   |\n",
      "\n",
      "This generally shows some improvement in distinguishing motivated from unmotivated proofs, although some more subtle distinctions are still consistently missed.\n",
      "\n",
      "Recommendations The existing mathematical literature, machine learning datasets and output evaluation standards are concentrated mostly on proof correctness, and our principal recommendation is to increase emphasis on proof motivation to better reflect the proof discovery process. One natural path forward would be to create a corpus of mathematical results with corresponding motivated and unmotivated proofs, and then to conduct a detailed evaluation of current models' ability to distinguish these. Once it is ascertained that models can judge motivation with high accuracy, one can design metrics for motivated proof generation.\n",
      "\n",
      "## 5 Conclusion\n",
      "\n",
      "We have outlined difficulties that pertain to natural language as well formal language mathematics datasets that we believe are present hindrances to the progress of AI systems towards becoming real mathematical thought partners that are as, if not more, useful to mathematicians as GitHub's Copilot 16 is to programmers.\n",
      "\n",
      "16 https://github.com/features/copilot\n",
      "\n",
      "The advantages and disadvantages of natural language and formal language datasets are frequently complementary. What is easy in one representation of mathematics is often hard in the other. For example, automatic evaluation is easy in formal language but hard in natural language; representing rich interaction modes is often comparatively easy in natural language but harder to express in formal language.\n",
      "\n",
      "We have identified several facets of mathematical practice that are currently not represented in the data used to design and evaluate the deployment of AI systems for mathematics, such as various workflows. We acknowledge that the aspects of workflows we highlight here likely do not capture all possible facets of mathematical practice - but we believe they represent an important start, offering an outline for what we may be able to curate to more human-compatible and explainable mathematical AI systems.\n",
      "\n",
      "While our aim has been exclusively in mathematics, these approaches of mapping processes of scientific discovery to data are not restricted to mathematics and may be adapted to other scientific domains as well. Some of the workflow items, like those related to literature search are directly relevant to other domains. We urge the community to explore how intermediate steps in the process of scientific discovery look like in other areas of science as well.\n",
      "\n",
      "Regarding purely proof creation, time will tell whether human-annotated proofs are necessary-or whether a purely formal approach together with ingenious proof search techniques will succeed. The comparatively short history of machine learning has taught us to expect the unexpected. In the short term, however, it is imperative to establish better datasets and novel benchmarks for mathematics (either in natural language or formal language), to go beyond the current ones that only test proof or result creation - and also test the ability to assess different mathematical workflows, to summarize mathematics, to explain limitations of proof techniques etc. This will support the next generation of machine learning models and AI tools that can help us discover more mathematics fast and, subsequently assist with any other scientific discipline that uses mathematics as a foundation (e.g. systems biology that relies on ordinary differential equations, or physics that relies on several subfields of mathematics - and inspires new subfields at the same time). At the same time, better mathematical copilots may have strong educational benefits. Recent controlled experiments suggest that LLM-based explanations can positively impact learning for high-school level mathematics, particularly when students attempt problems independently before consulting the model [Kumar et al., 2025]. Moreover, Human-AI tutoring systems such as Tutor CoPilot have demonstrated measurable improvements in student outcomes when LLMs provide real-time pedagogical guidance to novice tutors [Wang et al., 2024b].\n",
      "\n",
      "Having a deeper understanding of the processes by which one arrives at a proof (heuristics, workflows, etc.), which are all concentrated in the concept of a motivated proof, mathematical copilots can also teach the next generation of mathematical minds.\n",
      "\n",
      "Even though the history of the concept of 'proof' spans millennia, the story is ongoing, as currently, the search for a machine-learnable 'proof data structure' is an important focus point and represents a new chapter in the ongoing story of what a proof really is.\n",
      "\n",
      "## Acknowledgements\n",
      "\n",
      "Thomas Lukasiewicz was supported by the AXA Research Fund. Katherine M. Collins acknowledges support from the Cambridge Trust. The work of Fabian Ruehle is supported by NSF grants PHY-2210333, PHY2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions), and startup funding from Northeastern University. Timothy Gowers would like to acknowledge generous support from the Astera Institute. We thank Terence Tao for useful remarks.\n",
      "\n",
      "| System                                                                                  | Reference                                                                               | Architecture class                                                                      | How it helps mathematicians                                                                                                         | User un- derstand- ing                                                                  |\n",
      "|-----------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|\n",
      "| Closed-ended problem solving (verifiable / near-verifiable)                             | Closed-ended problem solving (verifiable / near-verifiable)                             | Closed-ended problem solving (verifiable / near-verifiable)                             | Closed-ended problem solving (verifiable / near-verifiable)                                                                         | Closed-ended problem solving (verifiable / near-verifiable)                             |\n",
      "| AlphaGeometry                                                                           | Trinh et al. [2024]                                                                     | Hybrid (neural + symbolic)                                                              | Solves Euclidean geometry problems via learned components plus symbolic deduction; uses a domain-specific formal encoding (JGEX).   | IF                                                                                      |\n",
      "| Newclid                                                                                 | Sicca et al. [2024]                                                                     | Symbolic (search / ATP-style)                                                           | Solves Euclidean geometry problems in JGEX, producing checkable deriva- tions.                                                      | IF                                                                                      |\n",
      "| Inequality prover                                                                       | Wei et al. [2024]                                                                       | Symbolic (automa- tion)                                                                 | Automates structured inequality prov- ing in restricted families, producing derivations that can often be mechani- cally verified.  | IF                                                                                      |\n",
      "| Conjecture, counterexample, and construction discovery (requires downstream validation) | Conjecture, counterexample, and construction discovery (requires downstream validation) | Conjecture, counterexample, and construction discovery (requires downstream validation) | Conjecture, counterexample, and construction discovery (requires downstream validation)                                             | Conjecture, counterexample, and construction discovery (requires downstream validation) |\n",
      "| Graph-building RL counterexamples                                                       | Wagner [2021]                                                                           | RL policy (often MLP/GNN)                                                               | Finds counterexamples by construct- ing discrete objects in an environment with an automatically scored objec- tive.                | PL                                                                                      |\n",
      "| PatternBoost                                                                            | Charton et al. [2024]                                                                   | Transformer + local search                                                              | Discovers combinatorial constructions by alternating neural generation with search, targeting automatically check- able properties. | PL                                                                                      |\n",
      "| Invariant-correlation models (e.g., knot the- ory)                                      | Davies et al. [2021]                                                                    | Supervised regres- sion (often linear / tree-based)                                     | Suggests conjectural relationships be- tween computable invariants; outputs hypotheses that mathematicians then prove or refute.    | OB                                                                                      |\n",
      "| Structured/tabular ML in group theory                                                   | He et al. [2023]                                                                        | Supervised learning (classifier/regressor)                                              | Produces conjectures or predictions from engineered representations of al- gebraic objects; requires mathematical validation.       | OB                                                                                      |\n",
      "| Formal proof engineering (assistance within an ITP ecosystem)                           | Formal proof engineering (assistance within an ITP ecosystem)                           | Formal proof engineering (assistance within an ITP ecosystem)                           | Formal proof engineering (assistance within an ITP ecosystem)                                                                       | Formal proof engineering (assistance within an ITP ecosystem)                           |\n",
      "| ImProver (Lean)                                                                         | Ahuja et al. [2024]                                                                     | Transformer (LLM program synthesis)                                                     | Edits or refactors Lean proofs under constraints (e.g., length), helping with proof maintenance and presentation.                   | IF                                                                                      |\n",
      "\n",
      "User understanding : BB = black-box use; IF = interface or formalism (e.g., Lean, JGEX); PL = pipeline configuration and objective (representation, search, scoring); OB = feature or representation choices and the validation procedure used to interpret outputs.\n",
      "\n",
      "Table 1: Highly specialized ('narrow') systems cited in this paper, summarized by architecture class, primary mathematical role, and the typical user understanding required.\n",
      "\n",
      "Table 2: Issue matrix for datasets used as training corpora (Train) and as benchmarks (Bench), separated by natural language (NL) and formal mathematics. 'X' indicates the issue commonly or sometimes affects that use case. For formal mathematics, proof checking provides automated correctness, so the row 'Lack of trusted automatic evaluation' is typically not applicable.\n",
      "\n",
      "| Issue                                                                        | NL Train   | NL Bench   | Formal Train   | Formal Bench   |\n",
      "|------------------------------------------------------------------------------|------------|------------|----------------|----------------|\n",
      "| Difficulty calibration and coverage                                          | X          | X          | X              | X              |\n",
      "| Binary scoring / success-only reporting                                      | X          | X          | X              | X              |\n",
      "| Lack of trusted automatic evaluation                                         | X          | X          |                |                |\n",
      "| Standardized interaction modes (Q-A, iso- lated lemmas)                      | X          | X          | X              | X              |\n",
      "| Contamination and leakage                                                    | X          | X          | X              | X              |\n",
      "| Binned progress reporting (e.g., Sankey- style)                              |            | X          |                | X              |\n",
      "| Tool misalignment from automation / tactic availability                      |            |            | X              | X              |\n",
      "| Non-trivial duplication (multiple representa- tions / near-copies)           |            |            | X              | X              |\n",
      "| Incomplete proofs and placeholders ( sorry , Admitted )                      |            |            | X              | X              |\n",
      "| Toolchain and automation provenance (scripts, autoformalization, tactics)    |            |            | X              | X              |\n",
      "| Dependence on external libraries and version- ing (commits, environments)    |            |            | X              | X              |\n",
      "| Unclear benchmarking protocol (timeouts, allowed tools, environment pinning) | X          |            |                | X              |\n",
      "\n",
      "Table 3: Selected datasets for evaluating the mathematical capabilities of LLMs.\n",
      "\n",
      "| Dataset                      | Reference                       | Size Field                |                                                                                 | Description                                                                                                                                                   |\n",
      "|------------------------------|---------------------------------|---------------------------|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| Commoncore / MultiArith      | Roy and Roth [2015]             | Elementary to 600         | Middle School Level Basic arithmetic                                            | Word problems with two steps                                                                                                                                  |\n",
      "| MAWPS                        | Koncel-Kedziorski et al. [2016] | 3320                      | Basic arithmetic and algebra                                                    | Word problems requiring systems of up to two linear equations (includes MultiArith)                                                                           |\n",
      "| Math23K                      | Wang et al. [2017]              | 23161                     | Basic arithmetic                                                                | Word problems in Chinese                                                                                                                                      |\n",
      "| ASDiv                        | Miao et al. [2020]              | 2305                      | Basic arithmetic                                                                | Word problems (selected from the internet)                                                                                                                    |\n",
      "| SVAMP                        | Patel et al. [2021]             | 1000                      | Basic arithmetic                                                                | Word problems requiring up to two steps (derived from ASDiv)                                                                                                  |\n",
      "| GSM8K                        | Cobbe et al. [2021]             | 8500                      | Basic arithmetic                                                                | Word problems requiring 2-8 steps to solve                                                                                                                    |\n",
      "| GSM8K-symbolic               | Mirzadeh et al. [2024]          | 5000                      | Basic arithmetic                                                                | Variants of GSM8K problems using symbolic templates                                                                                                           |\n",
      "| Dolphin18K                   | Huang et al. [2016]             | 18460                     | Basic arithmetic and algebra                                                    | Problems and answers from the mathematics category of Yahoo! Answers                                                                                          |\n",
      "| AQuA                         | Ling et al. [2017]              | 100000                    | Basic arithmetic and algebra                                                    | Algebraic multiple-choice word problems (similar to GMAT and GRE)                                                                                             |\n",
      "| MATH                         | Hendrycks et al. [2021]         | 12500                     | Problem-solving                                                                 | Problems from high school math competitions with step-by-step solutions                                                                                       |\n",
      "| NumGLUE                      | Mishra et al. [2022a]           | 101835                    | Arithmetic reasoning                                                            | 8 different mathematical reasoning tasks (including previous datasets).                                                                                       |\n",
      "| GAOKAO (Math)                | Zhang et al. [2023b]            | 844                       | Various high school math top- ics                                               | Questions from the Chinese College Entrance Exam (single-choice, cloze, open-ended)                                                                           |\n",
      "| Hungarian High School Finals | Paster [2023]                   | 33                        | Problem-solving                                                                 | Problems from the 2023 Hungarian National High School Finals in mathematics                                                                                   |\n",
      "| MATH 401                     | Yuan et al. [2023]              | 401                       | Advanced arithmetic and com-                                                    | Arithmetic expressions and numerical results                                                                                                                  |\n",
      "| TAL-SCQ5K                    | TAL Education Group [2023]      | 5000                      | Various                                                                         | Multiple-choice competition questions from elementary to high school levels in English and Chinese                                                            |\n",
      "| AGIEval (Math)               | Zhong et al. [2024]             | 1943                      | Various                                                                         | Problems (in English and Chinese) derived from college admission tests (GRE, Gaokao, SAT) and high school competitions (from MATH and AQuA)                   |\n",
      "| TheoremQA                    | Chen et al. [2023]              | 800                       | Various (including algebra, number theory, graph theory, information theory)    | Questions (from the internet and textbooks) and expert answers based on 354 theorems                                                                          |\n",
      "| ARB                          | Sawada et al. [2023]            | 234                       | Various                                                                         | Problems from university exams and competitions at the undergraduate level                                                                                    |\n",
      "| NaturalProofs                | Welleck et al. [2021]           | Graduate to 25271         | Research Level Various (focus on algebraic ge- ometry)                          | Theorems with proofs and reference graphs from ProofWiki , the Stacks project , and textbooks.                                                                |\n",
      "| GHOSTS                       | Frieder et al. [2023a]          | 709                       | Various                                                                         | Problem-solving, theorem-proving, and retrieval tasks derived from textbooks, StackExchange , and existing datasets (MATH and Symbolic integration)           |\n",
      "| FrontierMath                 | Glazer et al. [2024]            | 35 (unknown)              | Various                                                                         | Challenging problems with automated verification cre- ated by mathematicians                                                                                  |\n",
      "| miniF2F                      | Zheng et al. [2022]             | 488                       | Problem-solving, algebra, number theory                                         | Formalized Olympiad-type problems from AIME, AMC, IMO, as well as high school and undergrad- uate courses (also derived from MATH)                            |\n",
      "| OlympiadBench                | He et al. [2024]                | 8476                      | Various                                                                         | Problems and solutions from mathematics and physics competitions and the Chinese College Entrance Exam (in English or Chinese and partially with images)      |\n",
      "| IMO Small Challenge          | Frieder et al. [2024]           | 100                       | Combinatorics                                                                   | Selected problems and solutions from IMO andBWMC                                                                                                              |\n",
      "| IMO-AG-30                    | Trinh et al. [2024]             | 30                        | Geometry                                                                        | Plane Euclidean geometry problems from IMO in JGEX formal language                                                                                            |\n",
      "| Mathematics                  | Saxton et al. [2019]            | Various / Mixed Generated | Difficulty Arithmetic, algebra, probabil-                                       | Sequential questions generated with modular structure                                                                                                         |\n",
      "| MMLU (Math)                  | Hendrycks et al. [2020]         | 960                       | Elementary, high school, and college mathematics, and ab- stract algebra        | Multiple-choice questions collected from online sources ranging from elementary to college (similar to GRE) mathematics                                       |\n",
      "| INT                          | Wu et al. [2020]                | Generated                 | Inequalities, General Mathe- matics                                             | Generated formalized (in-)equality theorems based on ordered field axioms                                                                                     |\n",
      "| AMPS                         | Hendrycks et al. [2021]         | ≈ 5100000                 | (Linear) algebra, calculus, statistics, geometry, and num- ber theory           | Problems and step-by-step solutions from Khan Academy ( > 100k) and generated using Mathemat- ica ( ≈ 5M)                                                     |\n",
      "| Lila                         | Mishra et al. [2022b]           | 132239                    | Arithmetic, (linear) algebra, calculus, statistics, geometry, and number theory | Derived from 20 existing datasets (including NumGLUE, SVAMP, MultiArith, Dolphin18K, Math- ematics, AMPS, GSM8K, MATH) with numbers or expressions as outputs |\n",
      "| CMMLU (Math)                 | Li et al. [2023]                | 499                       | Various                                                                         | Multiple-choice questions (in Chinese) from elementary to college level collected from freely available resources                                             |\n",
      "\n",
      "## References\n",
      "\n",
      "- Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature , 625(7995):476-482, 2024.\n",
      "- OpenAI. GPT-4 technical report. arXiv preprint 2303.0877 , 2023.\n",
      "- Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner. Mathematical capabilities of ChatGPT. In Advances in Neural Information Processing Systems , volume 36, 2023a.\n",
      "- Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024.\n",
      "- DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\n",
      "- Qwen Team. QwQ: Reflect deeply on the boundaries of the unknown. https://qwenlm.github.io/blog/ qwq-32b-preview/ , 2024.\n",
      "- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, and Heewoo Jun et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n",
      "- Eshta Bhardwaj, Harshit Gujral, Siyi Wu, Ciara Zogheib, Tegan Maharaj, and Christoph Becker. The state of data curation at NeurIPS: An assessment of dataset development practices in the datasets and benchmarks track. In Advances in Neural Information Processing Systems , volume 37, pages 53626-53648, 2024. doi: 10.52202/079017-1698. NeurIPS 2024, Datasets and Benchmarks Track.\n",
      "- Vladmir Sicca, Tianxiang Xia, Mathïs Fédérico, Philip John Gorinski, Simon Frieder, and Shangling Jui. Newclid: A user-friendly replacement for AlphaGeometry. arXiv preprint arXiv:2411.11938 , 2024.\n",
      "- Chenrui Wei, Mengzhou Sun, and Wei Wang. Proving olympiad algebraic inequalities without human demonstrations. arXiv preprint arXiv:2406.14219 , 2024.\n",
      "- Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, and Daniel Zheng et al. Advancing mathematics by guiding human intuition with AI. Nature , 600(7887):70-74, 2021.\n",
      "- Yang-Hui He, Vishnu Jejjala, Challenger Mishra, and Max Sharnoff. Learning to be simple. arXiv preprint arXiv:2312.05299 , 2023.\n",
      "- Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature , 625(7995):468-475, 2024.\n",
      "- Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature , 610(7930):47-53, 2022.\n",
      "- Baran Hashemi, Roderic G Corominas, and Alessandro Giacchetto. Can transformers do enumerative geometry? arXiv preprint arXiv:2408.14915 , 2024.\n",
      "- Adam Zsolt Wagner. Constructions in combinatorics via neural networks, 2021. URL https://arxiv.org/ abs/2104.14516 .\n",
      "\n",
      "- François Charton, Jordan S. Ellenberg, Adam Zsolt Wagner, and Geordie Williamson. PatternBoost: Constructions in mathematics with a little help from AI, 2024. URL https://arxiv.org/abs/2411.00566 .\n",
      "- Riyaz Ahuja, Jeremy Avigad, Prasad Tetali, and Sean Welleck. Improver: Agent-based automated proof optimization. arXiv preprint arXiv:2410.04753 , 2024.\n",
      "- Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, William Hart, et al. Evaluating language models for mathematics through interactions. Proceedings of the National Academy of Sciences , 121(24):e2318124121, 2024a.\n",
      "- Katherine M Collins, Ilia Sucholutsky, Umang Bhatt, Kartik Chandra, Lionel Wong, Mina Lee, Cedegao E Zhang, Tan Zhi-Xuan, Mark Ho, Vikash Mansinghka, et al. Building machines that learn and think with people. Nature Human Behaviour , 8(10):1851-1863, 2024b.\n",
      "- Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. ToRA: A tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=Ep0TtjVoap .\n",
      "- Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. A survey on mixture of experts. Authorea Preprints , 2024.\n",
      "- Simon Frieder, Julius Berner, Philipp Petersen, and Thomas Lukasiewicz. Large language models for mathematicians. arXiv preprint arXiv:2312.04556 , 2023b.\n",
      "- Alexandre Riazanov and Andrei Voronkov. The design and implementation of VAMPIRE. AI Communications , 15(2-3):91-110, 2002.\n",
      "- Laura Kovács and Andrei Voronkov. First-order theorem proving and VAMPIRE. In International Conference on Computer Aided Verification , pages 1-35. Springer, 2013.\n",
      "- Stephan Schulz. E-a brainiac theorem prover. AI Communications , 15(2-3):111-126, 2002.\n",
      "- Sean B. Holden. Machine learning for automated theorem proving: Learning to solve SAT and QSAT. Foundations and Trends® in Machine Learning , 14(6):807-989, 2021. ISSN 1935-8237.\n",
      "- Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292 , 2024a.\n",
      "- Yoshua Bengio and Nikolay Malkin. Machine learning and information theory concepts towards an AI mathematician. Bulletin of the American Mathematical Society , 61(3):457-469, 2024.\n",
      "- Richard S Sutton. The bitter lesson. http://www.incompleteideas.net/IncIdeas/BitterLesson.html , 2019. Blog post.\n",
      "- Edward Hughes, Michael Dennis, Jack Parker-Holder, Feryal Behbahani, Aditi Mavalankar, Yuge Shi, Tom Schaul, and Tim Rocktäschel. Open-endedness is essential for artificial superhuman intelligence. arXiv preprint arXiv:2406.04268 , 2024.\n",
      "- Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation. In International Conference on Learning Representations , 2024.\n",
      "- Emily First, Markus Rabe, Talia Ringer, and Yuriy Brun. Baldur: Whole-proof generation and repair with large language models. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering , pages 1229-1241, 2023.\n",
      "- Peiyang Song, Kaiyu Yang, and Anima Anandkumar. Towards large language models as copilots for theorem proving in lean. arXiv preprint arXiv:2404.12534 , 2024a.\n",
      "\n",
      "- John Harrison, Josef Urban, and Freek Wiedijk. History of interactive theorem proving. In Computational Logic , volume 9, pages 135-214, 2014.\n",
      "- Jasmin Christian Blanchette, Andrei Popescu, Daniel Wand, and Christoph Weidenbach. More SPASS with Isabelle: Superposition with hard sorts and configurable simplification. In Interactive Theorem Proving: Third International Conference, ITP 2012, Princeton, NJ, USA, August 13-15, 2012. Proceedings 3 , pages 345-360. Springer, 2012.\n",
      "- Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks , volume 1. Curran, 2021.\n",
      "- Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E Gonzalez, and Ion Stoica. Rethinking benchmark and contamination for language models with rephrased samples. arXiv preprint arXiv:2311.04850 , 2023.\n",
      "- Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models. arXiv preprint arXiv:2404.18824 , 2024.\n",
      "- Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. FrontierMath: a benchmark for evaluating advanced mathematical reasoning in AI. arXiv preprint arXiv:2411.04872 , 2024.\n",
      "- Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deeptheorem: Advancing llm reasoning for theorem proving through natural language and reinforcement learning. arXiv preprint arXiv:2505.23754 , 2025a.\n",
      "- Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, and Jenia Jitsev. Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models. arXiv preprint arXiv:2406.02061 , 2024. doi: 10.48550/arXiv.2406.02061.\n",
      "- Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\n",
      "- Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. DeepSeek-V2: a strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434 , 2024.\n",
      "- Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. DeepSeek-Coder-V2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931 , 2024.\n",
      "- Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. DeepSeek-prover-V1. 5: Harnessing proof assistant feedback for reinforcement learning and Monte-Carlo tree search. arXiv preprint arXiv:2408.08152 , 2024.\n",
      "- Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023.\n",
      "- An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024a.\n",
      "- Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2.5-Coder technical report. arXiv preprint arXiv:2409.12186 , 2024.\n",
      "\n",
      "- An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122 , 2024b.\n",
      "- Shima Imani, Liang Du, and Harsh Shrivastava. MathPrompter: Mathematical reasoning using large language models. In Sunayana Sitaram, Beata Beigman Klebanov, and Jason D Williams, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track) , pages 37-42. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.acl-industry.4. URL https://aclanthology.org/2023.acl-industry.4 .\n",
      "- Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations , 2024b. URL https://openreview.net/forum?id=KUNzEQMWU7 .\n",
      "- Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. WizardMath: Empowering mathematical reasoning for large language models via Reinforced Evol-Instruct. arXiv preprint arXiv:2308.09583 , 2023.\n",
      "- Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview .net/forum?id=4WnqRR915j .\n",
      "- Tobias Nipkow, Markus Wenzel, and Lawrence C Paulson. Isabelle/HOL: a proof assistant for higher-order logic . Springer, 2002.\n",
      "- Leonardo de Moura and Sebastian Ullrich. The Lean 4 theorem prover and programming language. In André Platzer and Geoff Sutcliffe, editors, Automated Deduction - CADE 28 , pages 625-635, Cham, 2021. Springer International Publishing.\n",
      "- Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, and Yu Li. Lyra: Orchestrating dual correction in automated theorem proving. arXiv preprint arXiv:2309.15806 , 2023.\n",
      "- Haiming Wang, Huajian Xin, Chuanyang Zheng, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, and Xiaodan Liang. LEGO-prover: Neural theorem proving with growing libraries. In The Twelfth International Conference on Learning Representations , 2024a. URL https://openreview.net/forum?id=3f5PALef5B .\n",
      "- Christian Szegedy. A promising path towards autoformalization and general artificial intelligence. In Intelligent Computer Mathematics: 13th International Conference, CICM 2020, Bertinoro, Italy, July 26-31, 2020, Proceedings 13 , pages 3-20. Springer, 2020.\n",
      "- Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, and Yuhuai Wu. Draft, Sketch, and Prove: Guiding formal theorem provers with informal proofs. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=SMa9EAovKMC .\n",
      "- Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1460514631. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.acl-long.817. URL https://aclanthology.org/2023.acl-long.817 .\n",
      "\n",
      "- Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223 , 2023.\n",
      "- Cedegao E Zhang, Katherine M Collins, Adrian Weller, and Joshua B Tenenbaum. AI for mathematics: A cognitive science perspective. arXiv preprint arXiv:2310.13021 , 2023a.\n",
      "- Stanislas Dehaene. The number sense: How the mind creates mathematics . Oxford University Press USA, 2011.\n",
      "- Lisa Feigenson, Stanislas Dehaene, and Elizabeth Spelke. Core systems of number. Trends in Cognitive Sciences , 8(7):307-314, 2004.\n",
      "- Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world GitHub issues? In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id= VTF8yNQM66 .\n",
      "- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, and Henrique Ponde de Oliveira Pinto et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\n",
      "- Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, and Animashree Anandkumar. LeanDojo: Theorem proving with retrieval-augmented language models. Advances in Neural Information Processing Systems , 36, 2024c.\n",
      "- Peiyang Song, Kaiyu Yang, and Anima Anandkumar. Towards large language models as copilots for theorem proving in Lean, 2024b. URL https://arxiv.org/abs/2404.12534 .\n",
      "- Rose E Wang, Ana T Ribeiro, Carly D Robinson, Susanna Loeb, and Dora Demszky. Tutor CoPilot: A human-AI approach for scaling real-time expertise. arXiv preprint arXiv:2410.03017 , 2024b.\n",
      "- Patrick Massot. Talk on formalizing limits in Lean (video recording). Video recording of a talk, 2022. URL https://www.youtube.com/watch?v=1iqlhJ1-T3A . See around the 45:30 mark for the '256 definitions' and '4096 plumbing lemmas' discussion in the context of naive limit variants.\n",
      "- Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=9ZPegFuFTFv .\n",
      "- Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, and Jeremy Avigad. ProofNet: Autoformalizing and formally proving undergraduate-level mathematics. arXiv preprint arXiv:2302.12433 , 2023.\n",
      "- Peter Scholze. Liquid tensor experiment. Experimental Mathematics , 31(2):349-354, 2022.\n",
      "- Anthony Bordg, Lawrence Paulson, and Wenda Li. Simple type theory is not too simple: Grothendieck's schemes without dependent types. Experimental Mathematics , 31(2):364-382, 2022. doi: 10.1080/10586458 .2022.2062073. URL https://doi.org/10.1080/10586458.2022.2062073 .\n",
      "- Tomas Skrivan. Scilean: A scientific computing library in lean. https://github.com/lecopivo/SciLean , 2025.\n",
      "- Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, and Kai Chen. Lean workbook: A large-scale lean problem set formalized from natural language math problems. arXiv preprint arXiv:2406.03847 , 2024.\n",
      "- Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. OpenMathInstruct-1: A 1.8 million math instruction tuning dataset. arXiv preprint arXiv:2402.10176 , 2024.\n",
      "\n",
      "- Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Lluís Màrquez, Chris CallisonBurch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1743-1752, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1202. URL https://aclanthology.org/D15-1202 .\n",
      "- Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies , pages 1152-1157, 2016.\n",
      "- Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Proceedings of the 2017 conference on empirical methods in natural language processing , pages 845-854, 2017.\n",
      "- Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 975-984, 2020.\n",
      "- Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2080-2094. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.168.\n",
      "- Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229 , 2024.\n",
      "- Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, and Wei-Ying Ma. How well do computers solve math word problems? Large-scale dataset construction and evaluation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 887-896, 2016.\n",
      "- Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , pages 158-167. Association for Computational Linguistics, 2017. doi: 10.18653/v1/P17-1015.\n",
      "- Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3505-3523, 2022a.\n",
      "- Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on GAOKAO benchmark. arXiv preprint arXiv:2305.12474 , 2023b.\n",
      "- Keiran Paster. Testing language models on a held-out high school national finals exam. https://huggingf ace.co/datasets/keirp/hungarian\\_national\\_hs\\_finals\\_exam , 2023.\n",
      "- Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015 , 2023.\n",
      "- TAL Education Group. TAL-SCQ5K-EN/TAL-SCQ5K-CN. https://github.com/math-eval/TAL-SCQ5K , 2023.\n",
      "- Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: A human-centric benchmark for evaluating foundation models. In Findings of the Association for Computational Linguistics: NAACL 2024 , pages 2299-2314, 2024.\n",
      "\n",
      "- Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. TheoremQA: A theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 7889-7901, 2023.\n",
      "- Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J Nay, Kshitij Gupta, and Aran Komatsuzaki. ARB: Advanced reasoning benchmark for large language models. arXiv preprint arXiv:2307.13692 , 2023.\n",
      "- Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. NaturalProofs: Mathematical theorem proving in natural language. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1) , 2021. URL https://openreview.net/forum?id=Jvxa8adr3iY .\n",
      "- Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. OlympiadBench: A challenging benchmark for promoting AGI with Olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024.\n",
      "- Simon Frieder, Mirek Olšák, Julius Berner, and Thomas Lukasiewicz. The IMO small challenge: Not-too-hard Olympiad math datasets for LLMs. In The Second Tiny Papers Track at ICLR 2024 , 2024.\n",
      "- David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations , 2019.\n",
      "- Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020.\n",
      "- Yuhuai Wu, Albert Qiaochu Jiang, Jimmy Ba, and Roger Grosse. INT: An inequality benchmark for evaluating generalization in theorem proving. arXiv preprint arXiv:2007.02924 , 2020.\n",
      "- Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. LILA: A unified benchmark for mathematical reasoning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 5807-5832, Abu Dhabi, United Arab Emirates, December 2022b. Association for Computational Linguistics. doi: 10.18653/v1/20 22.emnlp-main.392. URL https://aclanthology.org/2022.emnlp-main.392 .\n",
      "- Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212 , 2023.\n",
      "- Donald T. Campbell. Assessing the impact of planned social change. Evaluation and Program Planning , 2(1): 67-90, January 1979. ISSN 0149-7189. doi: 10.1016/0149-7189(79)90048-x. URL http://dx.doi.org/10. 1016/0149-7189(79)90048-X .\n",
      "- Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. Dynabench: Rethinking benchmarking in NLP. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 4110-4124, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.324. URL https://aclanthology.org/2021.naacl-main.324/ .\n",
      "- Simon Frieder, Martin Alawadhi, Trimmel, Rashid, and Klaus Gy. LLM vs ITP. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23 , 2023c.\n",
      "\n",
      "- Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. NaturalProver: grounded mathematical proof generation with language models. Advances in Neural Information Processing Systems , 35:4913-4927, 2022.\n",
      "- Wenhao Liu, Zhenyi Lu, Xinyu Hu, Jierui Zhang, Dailin Li, Jiacheng Cen, Huilin Cao, Haiteng Wang, Yuhan Li, Kun Xie, Dandan Li, Pei Zhang, Chengbo Zhang, Yuxiang Ren, Xiaohong Huang, and Yan Ma. Storm-born: A challenging mathematical derivations dataset curated via a human-in-the-loop multi-agent framework. arXiv preprint arXiv:2506.01531 , 2025. Findings of ACL 2025.\n",
      "- Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunović, Nikola Jovanović, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934 , 2025.\n",
      "- Ricardo Dominguez-Olmedo, Florian E Dorner, and Moritz Hardt. Training on the test task confounds evaluation and emergence. arXiv preprint arXiv:2407.07890 , 2024.\n",
      "- Alexandra N Uma, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara Plank, and Massimo Poesio. Learning from disagreement: A survey. Journal of Artificial Intelligence Research , 72:1385-1470, 2021.\n",
      "- Katherine M Collins, Umang Bhatt, and Adrian Weller. Eliciting and learning with soft labels from every annotator. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , volume 10, pages 40-52, 2022.\n",
      "- Ilia Sucholutsky, Ruairidh M Battleday, Katherine M Collins, Raja Marjieh, Joshua Peterson, Pulkit Singh, Umang Bhatt, Nori Jacoby, Adrian Weller, and Thomas L Griffiths. On the informativeness of supervision signals. In Uncertainty in Artificial Intelligence , pages 2036-2046. PMLR, 2023.\n",
      "- Mitchell L Gordon, Michelle S Lam, Joon Sung Park, Kayur Patel, Jeff Hancock, Tatsunori Hashimoto, and Michael S Bernstein. Jury learning: Integrating dissenting voices into machine learning models. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems , pages 1-19, 2022.\n",
      "- Guillaume Lample and François Charton. Deep learning for symbolic mathematics. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=S1eZYeHFDS .\n",
      "- Jacques Hadamard. Sur la distribution des zéros de la fonction ζ ( s ) et ses conséquences arithmétiques. Bulletin de la Société Mathématique de France , 24:199-220, 1896.\n",
      "- Atle Selberg. An elementary proof of the prime-number theorem. Annals of Mathematics , 50(2):305-313, 1949.\n",
      "- Paul Erdős. On a new method in elementary number theory which leads to an elementary proof of the prime number theorem. Proceedings of the National Academy of Sciences of the United States of America , 35(7): 374-384, 1949.\n",
      "- Roger B. Nelsen. Proofs without Words: Exercises in Visual Thinking . Mathematical Association of America, 1993.\n",
      "- Peter Selinger. A survey of graphical languages for monoidal categories. In New Structures for Physics , volume 813 of Lecture Notes in Physics , pages 289-355. Springer, 2011. Also available as arXiv:0908.3347.\n",
      "- Mark F. Bernstein. Mind of a mathematician. Princeton Alumni Weekly , November 2019. URL https: //paw.princeton.edu/article/mind-mathematician . Article recounts Terence Tao's initial reaction to Emmanuel Candès's compressed sensing problem, first doubting and then solving it.\n",
      "- Yinghui Li, Jiayi Kuang, Haojing Huang, Zhikun Xu, Xinnian Liang, Yi Yu, Wenlian Lu, Yangning Li, Xiaoyu Tan, Chao Qu, Ying Shen, Hai-Tao Zheng, and Philip S. Yu. One example shown, many concepts known! counterexample-driven conceptual reasoning in mathematical LLMs. arXiv preprint arXiv:2502.10454 , 2025a. ICML 2025.\n",
      "\n",
      "- Haocheng Ju and Bin Dong. Mirb: Mathematical information retrieval benchmark. arXiv preprint arXiv:2505.15585 , 2025.\n",
      "- Johannes Schmitt, Gergely Bérczi, Jasper Dekoninck, Jeremy Feusi, Tim Gehrunger, Raphael Appenzeller, Jim Bryan, Niklas Canova, Timo de Wolff, Filippo Gaia, Michel van Garrel, Baran Hashemi, David Holmes, Aitor Iribar Lopez, Victor Jaeck, Martina Jørgensen, Steven Kelk, Stefan Kuhlmann, Adam Kurpisz, Chiara Meroni, Ingmar Metzler, Martin Möller, Samuel Muñoz-Echániz, Robert Nowak, Georg Oberdieck, Daniel Platt, Dylan Possamaï, Gabriel Ribeiro, Raúl Sánchez Galán, Zheming Sun, Josef Teichmann, Richard P. Thomas, and Charles Vial. Improofbench: Benchmarking ai on research-level mathematical proof generation. arXiv preprint arXiv:2509.26076 , 2025.\n",
      "- Shubhra Mishra, Gabriel Poesia, Belinda Mo, and Noah D Goodman. MathCAMPS: Fine-grained synthesis of mathematical problems from human curricula. arXiv preprint arXiv:2407.00900 , 2024.\n",
      "- Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F Wong, Xiaowei Huang, Qiufeng Wang, and Kaizhu Huang. Is your model really a good math reasoner? evaluating mathematical reasoning with checklist. arXiv preprint arXiv:2407.08733 , 2024.\n",
      "- Emanuele La Malfa, Aleksandar Petrov, Simon Frieder, Christoph Weinhuber, Ryan Burnell, Raza Nazar, Anthony Cohn, Nigel Shadbolt, and Michael Wooldridge. Language-Models-as-a-Service: Overview of a new paradigm and its challenges. Journal of Artificial Intelligence Research , 80:1497-1523, 2024.\n",
      "- Jie Zhang, Cezara Petrui, Kristina Nikolić, and Florian Tramèr. Realmath: A continuous benchmark for evaluating language models on research-level mathematics. arXiv preprint arXiv:2505.12575 , 2025b. NeurIPS 2025.\n",
      "- Jasper Dekoninck, Ivo Petrov, Kristian Minchev, Mislav Balunovic, Martin Vechev, Miroslav Marinov, Maria Drencheva, Lyuba Konova, Milen Shumanov, Kaloyan Tsvetkov, Nikolay Drenchev, Lazar Todorov, Kalina Nikolova, Nikolay Georgiev, Vanesa Kalinkova, and Margulan Ismoldayev. The open proof corpus: A large-scale study of llm-generated mathematical proofs. arXiv preprint arXiv:2506.21621 , 2025.\n",
      "- Ronen Eldan and Yuanzhi Li. TinyStories: How small can language models be and still speak coherent English? arXiv preprint arXiv:2305.07759 , 2023.\n",
      "- Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of GPT-4. arXiv preprint arXiv:2306.02707 , 2023.\n",
      "- Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926 , 2023.\n",
      "- Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to program better. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=SaRj2ka1XZ3 .\n",
      "- Lawrence C Paulson and Kong Woei Susanto. Source-level proof reconstruction for interactive theorem proving. In International Conference on Theorem Proving in Higher Order Logics , pages 232-245. Springer, 2007.\n",
      "- Jia Meng and Lawrence C Paulson. Translating higher-order clauses to first-order clauses. Journal of Automated Reasoning , 40:35-60, 2008.\n",
      "- Jiewen Hu, Thomas Zhu, and Sean Welleck. miniCTX: Neural theorem proving with (long-)contexts, 2024. URL https://arxiv.org/abs/2408.03350 .\n",
      "- Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, and Yuhuai Wu. LISA: Language models of ISAbelle proofs. In 6th Conference on Artificial Intelligence and Theorem Proving , pages 378-392, 2021.\n",
      "\n",
      "- Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzygóźdź, Piotr Miłoś, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language models and automated theorem provers. Advances in Neural Information Processing Systems , 35:8360-8373, 2022.\n",
      "- Maciej Mikuła, Szymon Tworkowski, Szymon Antoniak, Bartosz Piotrowski, Albert Q. Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, and Yuhuai Wu. Magnushammer: A transformer-based approach to premise selection. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=oYjPk8mqAV .\n",
      "- Cameron Freer. lean4-skills: Lean 4 theorem-proving skill notes and reference patterns. https://github.c om/cameronfreer/lean4-skills , 2025. GitHub repository. Accessed: 2025-12-18.\n",
      "- Andrej Karpathy. makemore. https://github.com/karpathy/makemore , 2022.\n",
      "- Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 5360-5370, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/d8e1344e27a 5b08cdfd5d027d9b8d6de-Abstract.html .\n",
      "- Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning, 2024. URL https://arxiv.org/abs/2403.04642 .\n",
      "- Lawrence C. Evans. Partial Differential Equations , volume 19 of Graduate Studies in Mathematics . American Mathematical Society, Providence, RI, 2010.\n",
      "- Robion Kirby. A calculus for framed links in S 3 . Invent. Math. , 45(1):35-56, 1978. ISSN 0020-9910,1432-1297. doi: 10.1007/BF01406222. URL https://doi.org/10.1007/BF01406222 .\n",
      "- András Juhász. Differential and low-dimensional topology , volume 104 of London Mathematical Society Student Texts . Cambridge University Press, Cambridge, 2023.\n",
      "- Kurt Reidemeister. Elementare begründung der knotentheorie. Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg , 5(1):24-32, 1927. doi: 10.1007/BF02952507. URL https://doi.org/ 10.1007/BF02952507 .\n",
      "- Sergei Gukov, James Halverson, Fabian Ruehle, and Piotr Sułkowski. Learning to unknot. Machine Learning: Science and Technology , 2(2):025035, apr 2021. doi: 10.1088/2632-2153/abe91f. URL https://dx.doi.org/10.1088/2632-2153/abe91f .\n",
      "- Marc Culler, Nathan M. Dunfield, Matthias Goerner, and Jeffrey R. Weeks. SnapPy, a computer program for studying the geometry and topology of 3-manifolds. Available at http://snappy.computop.org (18/12/2024), 2024.\n",
      "- Mikhail Khovanov. A categorification of the Jones polynomial. Duke Math. J. , 101(3):359-426, 2000. ISSN 0012-7094,1547-7398. doi: 10.1215/S0012-7094-00-10131-7. URL https://doi.org/10.1215/S0012-709 4-00-10131-7 .\n",
      "- Peter Ozsváth and Zoltán Szabó. Holomorphic disks and topological invariants for closed three-manifolds. Ann. of Math. (2) , 159(3):1027-1158, 2004. ISSN 0003-486X,1939-8980. doi: 10.4007/annals.2004.159.1027. URL https://doi.org/10.4007/annals.2004.159.1027 .\n",
      "- Edward Witten. Monopoles and four-manifolds. Math. Res. Lett. , 1(6):769-796, 1994. ISSN 1073-2780. doi: 10.4310/MRL.1994.v1.n6.a13. URL https://doi.org/10.4310/MRL.1994.v1.n6.a13 .\n",
      "\n",
      "- The Sage Developers. SageMath, the Sage Mathematics Software System (Version 10.5) , 2024. https://www.sagemath.org .\n",
      "- Robert Lipshitz and Sucharit Sarkar. Spatial refinements and Khovanov homology. In Proceedings of the International Congress of Mathematicians-Rio de Janeiro 2018. Vol. II. Invited lectures , pages 1153-1173. World Sci. Publ., Hackensack, NJ, 2018.\n",
      "- Sergei Gukov, James Halverson, Ciprian Manolescu, and Fabian Ruehle. Searching for ribbons with machine learning, 2023. URL https://arxiv.org/abs/2304.09304 .\n",
      "- Taylor Applebaum, Sam Blackwell, Alex Davies, Thomas Edlich, András Juhász, Marc Lackenby, Nenad Tomašev, and Daniel Zheng. The unknotting number, hard unknot diagrams, and reinforcement learning, 2024. URL https://arxiv.org/abs/2409.09032 .\n",
      "- Jacob Rasmussen. Khovanov homology and the slice genus. Invent. Math. , 182(2):419-447, 2010. ISSN 00209910,1432-1297. doi: 10.1007/s00222-010-0275-6. URL https://doi.org/10.1007/s00222-010-0275-6 .\n",
      "- Michael Hartley Freedman. The topology of four-dimensional manifolds. J. Differential Geometry , 17(3): 357-453, 1982. ISSN 0022-040X,1945-743X. URL http://projecteuclid.org/euclid.jdg/1214437136 .\n",
      "- Clifford Henry Taubes. The Seiberg-Witten invariants and symplectic forms. Math. Res. Lett. , 1(6):809-822, 1994. ISSN 1073-2780. doi: 10.4310/MRL.1994.v1.n6.a15. URL https://doi.org/10.4310/MRL.1994.v 1.n6.a15 .\n",
      "- Ronald Fintushel and Ronald J. Stern. Knots, links, and 4-manifolds. Invent. Math. , 134(2):363-400, 1998. ISSN 0020-9910,1432-1297. doi: 10.1007/s002220050268. URL https://doi.org/10.1007/s00222005026 8 .\n",
      "- John Morgan and Gang Tian. Ricci flow and the Poincaré conjecture , volume 3 of Clay Mathematics Monographs . American Mathematical Society, Providence, RI; Clay Mathematics Institute, Cambridge, MA, 2007.\n",
      "- Benjamin A. Burton. Introducing Regina, the 3-manifold topology software. Experiment. Math. , 13(3): 267-272, 2004. ISSN 1058-6458,1944-950X. URL http://projecteuclid.org/euclid.em/1103749834 .\n",
      "- J.W.S. Cassels, A. Frölich, London Mathematical Society, and International Mathematical Union. Algebraic Number Theory: Proceedings of an Instructional Conference. Edited by J. W. S. Cassels and A. Fröhlich . 1976. URL https://books.google.co.uk/books?id=0xltHQAACAAJ .\n",
      "- Louis H Kauffman and Sofia Lambropoulou. Hard unknots and collapsing tangles. Introductory lectures on knot theory, Ser. Knots Everything , 46:187-247, 2012.\n",
      "- C.H. Dowker and Morwen B. Thistlethwaite. Classification of knot projections. Topology and its Applications , 16(1):19-31, 1983. ISSN 0166-8641. doi: https://doi.org/10.1016/0166-8641(83)90004-4. URL https://www.sciencedirect.com/science/article/pii/0166864183900044 .\n",
      "- I. A. Dynnikov. Arc-presentations of links: Monotonic simplification. Fundamenta Mathematicae , 190(1): 29-76, 2006. URL http://eudml.org/doc/283163 .\n",
      "- L. H. Kauffman, N. E. Russkikh, and I. A. Taimanov. Rectangular knot diagrams classification with deep learning. Journal of Knot Theory and Its Ramifications , 31(11):2250067, 2022. doi: 10.1142/S02182165225 00675. URL https://doi.org/10.1142/S0218216522500675 .\n",
      "- Sergei Gukov, James Halverson, and Fabian Ruehle. Rigor with machine learning from field theory to the poincaréconjecture. Nature Reviews Physics , 6(5):310-319, 2024. doi: 10.1038/s42254-024-00709-0. URL https://doi.org/10.1038/s42254-024-00709-0 .\n",
      "\n",
      "- Zenan Li, Zhaoyu Li, Kaiyu Yang, Xiaoxing Ma, and Zhendong Su. Learning to disprove: Formal counterexample generation with large language models. OpenReview preprint , 2025b. ICLR 2026 submission.\n",
      "- Suozhi Huang, Peiyang Song, Robert Joseph George, and Anima Anandkumar. Leanprogress: Guiding search for neural theorem proving via proof progress prediction. arXiv preprint arXiv:2502.17925 , 2025.\n",
      "- Michael Kohlhase. OMDoc - An Open Markup Format for Mathematical Documents [version 1.2] , volume 4180 of Lecture Notes in Computer Science . Springer, 2006. doi: 10.1007/11826095. URL https: //doi.org/10.1007/11826095 .\n",
      "- Marc Berges, Jonas Betzendahl, Abhishek Chugh, Michael Kohlhase, Dominic Lohr, and Dennis Müller. Learning support systems based on mathematical knowledge management. In Catherine Dubois and Manfred Kerber, editors, Intelligent Computer Mathematics - 16th International Conference, CICM 2023, Cambridge, UK, September 5-8, 2023, Proceedings , volume 14101 of Lecture Notes in Computer Science , pages 84-97. Springer, 2023. doi: 10.1007/978-3-031-42753-4\\_6. URL https://doi.org/10.1007/97 8-3-031-42753-4\\_6 .\n",
      "- Hanoona Rasheed, Abdelrahman Shaker, Anqi Tang, Muhammad Maaz, Ming-Hsuan Yang, Salman Khan, and Fahad Shahbaz Khan. Videomathqa: Benchmarking mathematical reasoning via multimodal understanding in videos. arXiv preprint arXiv:2506.05349 , 2025.\n",
      "- George Pólya. With, or without, motivation? The American Mathematical Monthly , 56(10):684-691, 1949.\n",
      "- Rebecca Lea Morris. Motivated proofs: What are they, why they matter and how to write them. The Review of Symbolic Logic , 13(1):23-46, 2019. doi: 10.1017/S1755020319000583.\n",
      "- H. Kumar, David M. Rothschild, Daniel G. Goldstein, and Jake M. Hofman. Math education with large language models: Peril or promise? In Alexandra I. Cristea, Erin Walker, Yu Lu, Olga C. Santos, and Seiji Isotani, editors, Artificial Intelligence in Education (AIED 2025) , volume 15880 of Lecture Notes in Computer Science , Cham, 2025. Springer. doi: 10.1007/978-3-031-98459-4\\_5. URL https: //doi.org/10.1007/978-3-031-98459-4\\_5 .\n",
      "\n",
      "## A Appendix: Problems for Motivated Proof Experiments\n",
      "\n",
      "For the motivated proof experiments in 4.4, we used five problems including the two examples. These are presented how they were presented to the models, including the human-written motivated and unmotivated proof used for in-context examples.\n",
      "\n",
      "Cantor's theorem There is no surjection from a set X to its power set P ( X ).\n",
      "\n",
      "/negationslash\n",
      "\n",
      "MOTIVATED PROOF: Let f : X → P ( X ) be our function, we need to find some S ⊆ X not in the image of f . Not knowing which subset of X to take, we can treat the subset as an unknown, just as we do when solving an equation, and try to narrow down the possibilities. The most general subset of X can be expressed as { x ∈ X : P ( x ) } for some as yet unspecified property P . We now want to prove, for an arbitrary element y of X , that f ( y ) = { x ∈ X : P ( x ) } . To obtain our contradiction, we need either an element x of f ( y ) such that ¬ P ( x ) or an element x of the complement of f ( y ) such that P ( x ). There are not many elements around, so trying y is one of the first things to do, and then we find that we need either y ∈ f ( y ) and ¬ P ( y ) or y / ∈ f ( y ) and P ( y ). The choice P ( x ) = x / ∈ f ( x ) satisfies this, so the set S = { x ∈ X : x / ∈ f ( x ) } has the desired property.\n",
      "\n",
      "UNMOTIVATED PROOF: Let f : X → P ( X ) be our function, we claim that the set S = { x ∈ X : x / ∈ f ( x ) } is not in the image of f , hence f is not surjective. Suppose there exists y ∈ X such that f ( y ) = S . But then y ∈ f ( y ) iff y ∈ S iff y / ∈ f ( y ) by the definition of S, giving a contradiction.\n",
      "\n",
      "Small doubling There exists a subset A of the natural numbers with cardinality n, such that the set A + A = a 1 + a 2 : a i , a j ∈ A has cardinality 2 n -1.\n",
      "\n",
      "MOTIVATED PROOF: We don't know which set to take for A so we treat it as an unknown, just as we do when solving an equation, and try to narrow down the possibilities. Since we know that | A | = n , we can write A = { a 1 , ..., a n } for a 1 , ..., a n distinct, so we have A + A = a i + a j : i, j = 1 , ..., n . This isn't immediately helpful since we don't know how many collisions there will be in A + A , but we try to narrow down the possibilities. The simplest way to distinguish natural numbers is by ordering them, so we assume without loss of generality that a 1 &lt; ... &lt; a n . Applying this monotonicity to our sums, we deduce that a i + a j &lt; a i + a k whenever j &lt; k .\n",
      "\n",
      "Trying to distinguish as many elements as possible, we fix some i and take this inequality to its logical conclusion, finding that a 1 + a i &lt; .... &lt; a n + a i , or that we have n distinct elements. However, we can further extend this chain to the left if 1 &lt; i or the right if i &lt; n , giving us the longer chain a 1 + a 1 &lt; ... &lt; a 1 + a i &lt; ... &lt; a n + a 1 &lt; ... &lt; a n + a n , which has 2 n -1 distinct elements. If we want to have | A + A | = 2 n -1, then these are all the elements of A + A , but we still have this free parameter i . Repeating the construction with some j &gt; i and observing the first syntactic different element, we notice that a 1 + a i +1 = a 2 + a i , and isolating the variable i we notice that a 2 -a 1 = a i +1 -a i . This defines an arithmetic progression with first element a 1 and common difference a 2 -a 1 , so let's see if that enough.\n",
      "\n",
      "Suppose A is an arithmetic progression of length n , then A = a, a + d, . . . , a +( n -1) d , then A + A = 2 a, 2 a + d, . . . , 2 a +(2 n -2) d has size 2 n -1, completing the proof.\n",
      "\n",
      "UNMOTIVATED PROOF: Let A = { 1 , ..., n } , which has cardinality n, then A + A = { 2 , ..., 2 n } has cardinality 2 n -1, completing the proof.\n",
      "\n",
      "Integer sums There is a polynomial P ( x ) with rational coefficients such that P ( n ) = ∑ n k =0 k for all non-negative integers n .\n",
      "\n",
      "MOTIVATED PROOF: We can write a generic polynomial as P ( n ) = ∑ m j =0 a j n j . However, since m is unknown the equation is reasonably unwieldy. Consequently, we wish to find an upper bound on the degree of P ( n ). We do this by applying the trivial bound, P ( n ) = ∑ n k =0 k ≤ ∑ n k =0 n = n 2 + n , and since this equation holds for arbitrarily large n , we have that m = deg P ≤ 2, so we can write P ( n ) = a 0 + a 1 n + a 2 n 2 . To find\n",
      "\n",
      "a 0 , a 1 and a 2 , we can substitute some small values of n , so we have a 0 = P (0) = 0, a 0 + a 1 + a 2 = P (1) = 1 and a 0 +2 a 1 +4 a 2 = P (2) = 3. Solving the resulting linear equation, we have that a 0 = 0, a 1 = a 2 = 1 / 2 so P ( n ) = 1 2 ( n 2 + n ).\n",
      "\n",
      "To confirm that this works for all n , we can use induction. We have checked the base case already so we only need to do the induction step, so suppose that ∑ n k =0 k = 1 2 ( n 2 + n ), then we need to show that ∑ n +1 k =0 k = 1 2 (( n +1) 2 +( n +1)). In order to use the induction assumption, we split the LHS to give us ∑ n k =0 k +( n +1) and substitute in the induction assumption. By expanding and simplifying, we find that both sides of the equation are equal, completing our proof.\n",
      "\n",
      "UNMOTIVATED PROOF: We show that P ( n ) = 1 2 n ( n +1) works using induction. First we observe that P (0) = 0 = ∑ 0 k =0 k . Then suppose that ∑ n k =0 k = 1 2 n ( n +1), then we have that ∑ n +1 k =0 k = ∑ n k =0 k +( n +1) = 1 2 n ( n +1) + ( n +1) = ( n +1)( n 2 +1) = 1 2 ( n +1)( n +2), completing the proof.\n",
      "\n",
      "Nilpotent units Let R be a commutative ring, and let x ∈ R be nilpotent. Then (1 + x ) is a unit.\n",
      "\n",
      "MOTIVATED PROOF: To show that 1 + x is a unit, we need to find an inverse element. Not knowing which element to take, we parametrize the most generic element we can. Since the only known elements of R are 1 and x , the most generic element is an integer polynomial in x , ∑ m k =0 a k x k . For this to be a right inverse of 1 + x , we must have that 1 = (1 + x )( ∑ m k =0 a k x k ) = a 0 + ∑ m k =1 ( a k + a k -1 ) x k + a m x m +1 . For this to hold, we must eliminate all coefficients of the polynomial besides the constant term, which should be 1. This gives us that a 0 = 1, a k = a k -1 for 1 ≤ k ≤ m and a m = 0. The first two equations give us that a k = ( -1) k for all k , but this contradicts the last equation.\n",
      "\n",
      "But we also know that x is nilpotent, so if we have r such that x r = 0, then all coefficients from x r onwards can be ignored. This solves our issue, as we can let m = r -1 to remove the a m = 0 condition, and we are left with a k = ( -1) k , so (1 + x ) is a unit with inverse ∑ r -1 k =0 ( -1) k · x k .\n",
      "\n",
      "UNMOTIVATED PROOF: Let r be such that x r = 0 from nilpotency, and observe that (1 + x )( ∑ r -1 k =0 ( -1) k · x k ) = 1 + -x r = 1 by the formula for summing geometric progressions, hence ∑ r -1 k =0 ( -1) k · x k is an inverse of 1 + x and 1 + x is a unit.\n",
      "\n",
      "Large totients We define Euler's totient function φ as follows. If n = ∏ m i =1 p k i i is its prime factorisation, then φ ( n ) = ∏ m i =1 p k i -1 i ( p i -1). Show that for all /epsilon1 &gt; 0, φ ( n ) /n can take values in (1 -/epsilon1, 1).\n",
      "\n",
      "MOTIVATED PROOF: We first try to simplify φ ( n ) /n . Expressing n in its prime factorisation to match the definition of φ ( n ), we have φ ( n ) /n = ∏ m i =1 ( p k i -1 i ( p i -1) /p k i i ) = ∏ m i =1 (1 -1 /p i ). We notice that each factor is between 0 and 1, so the product is large where there are few factors so we let m = 1 (or equivalently, letting n = p k ), giving us φ ( n ) /n = (1 -1 /p ). Since this is clearly less than 1, we only need to choose p such that 1 -/epsilon1 &lt; 1 -1 /p , which simplifies easily to p &gt; 1 //epsilon1 . Since there are infinitely many prime numbers, we can always choose such a p .\n",
      "\n",
      "UNMOTIVATED PROOF: Let p be the smallest prime number such that p &gt; 1 //epsilon1 , which exists as there are infinitely many primes. Then we have φ ( p ) /p = 1 -1 /p , and as p &gt; 1 //epsilon1 , we see that 1 -/epsilon1 &lt; 1 -1 /p &lt; 1 as required.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ad8365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning\n",
       "\n",
       "Simon Frieder ∗,1 , Jonas Bayer 2 , Sam Looi 4 , Jacob Loader 2 , Julius Berner 4 , Katherine M. Collins 2 , András Juhász 1 , Fabian Ruehle 9 , Sean Welleck 7 , Gabriel Poesia 10 , Ryan-Rhys Griffiths 11 , Adrian Weller 2 , Anirudh Goyal 12 , Thomas Lukasiewicz 3,1 , Cameron Freer 13 , Kevin Buzzard 8 , and Timothy Gowers 2,6\n",
       "\n",
       "1 University of Oxford\n",
       "\n",
       "2 University of Cambridge\n",
       "\n",
       "3 Vienna University of Technology 4 Caltech\n",
       "\n",
       "6 Collège de France\n",
       "\n",
       "7 Carnegie Mellon University\n",
       "\n",
       "8 Imperial College London\n",
       "\n",
       "9 Northeastern University\n",
       "\n",
       "10 Stanford University\n",
       "\n",
       "11 FutureHouse Inc.\n",
       "\n",
       "12 Meta\n",
       "\n",
       "13 MIT\n",
       "\n",
       "## Abstract\n",
       "\n",
       "The datasets and benchmarks commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings and misdirections. These range from a restricted scope of mathematical complexity to limited fidelity in capturing aspects beyond the final, written proof (e.g. motivating the proof, or representing the thought processes leading to a proof). These issues are compounded by a dynamic reminiscent of Goodhart's law: as benchmark performance becomes the primary target for model development, the benchmarks themselves become less reliable indicators of genuine mathematical capability. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or 'thought partners'), necessitates a course correction both in the design of mathematical datasets and the evaluation criteria of the models' mathematical ability. In particular, it is necessary for benchmarks to move beyond the existing result-based datasets that map theorem statements directly to proofs, and instead focus on datasets that translate the richer facets of mathematical research practice into data that LLMs can learn from. This includes benchmarks that supervise the proving process and the proof discovery process itself, and we advocate for mathematical dataset developers to consider the concept of 'motivated proof' , introduced by G. Pólya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations.\n",
       "\n",
       "∗ Corresponding author: simon.frieder@cs.ox.ac.uk .\n",
       "\n",
       "## 1 AI Systems for Mathematicians - Present and Future\n",
       "\n",
       "There has been a rapid surge in research around AI for mathematics. For example, AlphaGeometry was shown to solve mathematical geometry problems on the level of the International Math Olympiad (IMO) [Trinh et al., 2024], Numina won the first AIMO Progress Prize, 1 by providing a model capable of turning mathematical questions into executable Python code, and previously GPT-4 [OpenAI, 2023] was shown to attain, in some cases, the performance of an undergraduate university student [Frieder et al., 2023a]. High accuracies on standard mathematics benchmarks [Reid et al., 2024] suggest that many benchmarks may be close to being solved.\n",
       "\n",
       "Update since the December 2024 version. This manuscript was initially circulated in December 2024. Since then, reported performance on boxed-answer benchmarks has continued to increase, and some reasoning-focused systems report near-saturation on MATH -style evaluations [DeepSeek-AI, 2025, Qwen Team, 2024]. At the same time, many of the issues discussed in this paper have seen fewer changes than model scores: public math benchmarks remain largely static, scoring often still collapses rich outputs to a single correctness bit, and contamination risk persists as benchmarks diffuse into training corpora. The revisions in this version therefore focus on incorporating recent benchmark results while keeping the central claim unchanged: progress toward mathematical copilots depends on data and evaluation that represent intermediate reasoning and workflow competence, not only final answers.\n",
       "\n",
       "The rapid progress and proliferation of ideas in this space - while exciting and productive in many ways has led to a series of problems due to a lack of alignment across research directions and a lack of coherence around clear goals. Problems include:\n",
       "\n",
       "- (1) some datasets being overstudied (e.g., the many versions of the GSM8K dataset [Cobbe et al., 2021]), whereas other data-related aspects (few datasets comprising advanced mathematics or reflecting tool use in mathematics) are neglected;\n",
       "- (2) various workflows and reasoning modes of how professional mathematicians conduct their research are not represented in datasets;\n",
       "- (3) different ways to express the same mathematical content (in formal language, vs. in natural language) have not been unified;\n",
       "- (4) scalability of evaluation that aligns with the goals of the tools being developed.\n",
       "\n",
       "These problems are not unique to AI for mathematics, but the broader machine learning community has begun systematic self-assessment efforts that the mathematical AI community has yet to undertake. For instance, Bhardwaj et al. [2024] present an evaluation framework grounded in data curation principles from librarianship and archives, and use it to assess 60 datasets from the NeurIPS Datasets and Benchmarks track. Their analysis shows gaps in documentation around ethical considerations, data management, and environmental footprint, and they provide concrete recommendations for improving peer-review standards. No comparable audit exists for mathematical datasets. Such an effort would be valuable: mathematical datasets come with domain-specific concerns (e.g., difficulty calibration, formal-informal alignment, contamination risks) that generic ML dataset assessments do not address.\n",
       "\n",
       "Despite these problems, the landscape of machine learning models that have been devised (and embraced by parts of the mathematical community) consists primarily of models that lie between the two categories below , based on the amount of interaction with humans they are designed for:\n",
       "\n",
       "1. Highly specialized, 'narrow' models that fulfill all of the criteria of 1) being designed for a precisely defined mathematical domain or mathematical problem, 2) requiring domain knowledge to be used (such as formal languages as input), 3) that cannot explain themselves, and 4) do not allow free-form interaction. Examples of such specialized models vary widely in terms of how their foundation is set up\n",
       "\n",
       "1 https://aimoprize.com/updates/2024-07-20-progress-prize-results\n",
       "\n",
       "(model architecture, training data), and in what they assist mathematicians. Specialized models could rely on symbolic reasoners, such as the recent solver for elementary geometry (Newclid [Sicca et al., 2024], which supersedes AlphaGeometry [Trinh et al., 2024]) or for inequality problems ([Wei et al., 2024]). Olympiad-level mathematics problems are within reach of both, and these assist mathematicians solely in solving closed-ended questions. Alternatively, such specialized models could simply be regression models, relying on raw mathematical data to infer new relationships between mathematical objects, such as for knot theory or representation theory [Davies et al., 2021] or group theory [He et al., 2023], assisting mathematicians in finding conjectures (which then need to be proved by hand). Many further approaches exist [Romera-Paredes et al., 2024, Fawzi et al., 2022, Hashemi et al., 2024], see Table 1 for more information. These systems typically require a significant amount of specialization by a user, such as mastering a formal input language (e.g., AlphaGeometry and Newclid, which both require mastery of the JGEX formal language) or understanding the machine learning model itself (e.g., [Davies et al., 2021]) beyond the necessary mathematical domain knowledge. The data on which they operate also varies strongly: Symbolic solvers rely on a symbolic language; models like [Davies et al., 2021] rely on tabular data, while models like [Hashemi et al., 2024] combine different data types.\n",
       "\n",
       "2. General purpose, 'broad' models that can interact with humans via natural language, understand input images, and use tools (being able to call, in particular, specialized models, which can be viewed as a particular kind of 'tool', as we argue below), can provide general assistance to a mathematician (both with routine tasks, such as literature search, and domain-dependent tasks, such as counterexample search) and are not tied to a specific area of mathematics. These are what we refer to as 'Mathematical Copilots' , as their goal is not simply to provide correct proofs but also to enhance the user's understanding of the results and guide them towards their own discoveries [Collins et al., 2024a]. Currently, the best candidates for these systems are large language models (LLMs), which we take to include multi-modal models that also admit images as components of the prompt. We contend that LLMs represent early predecessors of future general-purpose models (mathematical copilots), ultimately acting as mathematicians' 'thought partners' [Collins et al., 2024b]. Anecdotal evidence of such usefulness of current LLMs has been presented in various discussion forums, 2 while noting the failures that LLMs often succumb to. However, there are concrete ways in which LLMs can be improved to become mathematical copilots: As indicated by the tool-integrated-reasoning approach, championed by models such as ToRA [Gou et al., 2024] or Numina, 3 general LLMs can incorporate other symbolic systems to which they can delegate symbolic tasks to. The mixture-of-experts paradigm [Cai et al., 2024], on the other hand, highlights different ways in which LLMs can incorporate other LLMs as specially trained 'modules' to which they can delegate mathematical tasks that are not symbolic in nature but still require particular mathematical skills. Such a hierarchical system can help mathematicians both on routine academic tasks (see [Frieder et al., 2023b] for a short, high-level overview) and specialized ones. We envision that such models will require significantly less effort from users to use them, compared to specialized models, being self-explanatory, where natural language is the main mode of interaction. (While the outlined architectural approaches provide a viable step forward, in some cases, no data exists on which to train these models; see below.)\n",
       "\n",
       "In addition, a third class of models will likely be developed:\n",
       "\n",
       "3. Fully automated, 'universal' models that can generate mathematical theorems and theories autonomously, merely by being directed to a mathematical result. We envision that these systems will be the true successors of automated theorem provers (ATPs) such as the Vampire [Riazanov and Voronkov, 2002, Kovács and Voronkov, 2013] or E [Schulz, 2002] ATPs. These systems have not been widely adopted by the mathematical community. While there have been attempts to imbue these systems with (non-LLM) machine learning techniques [Holden, 2021], these have not yet dramatically increased their performance. Nonetheless, once general-purpose models have been established, it is conceivable one could put these in automation loops to obtain 'AI mathematicians'. For the domain of\n",
       "\n",
       "2 https://www.reddit.com/r/math/comments/14p6j5c/terence\\_tao\\_on\\_using\\_gpt4\\_to\\_help\\_with\\_math/\n",
       "\n",
       "3 https://huggingface.co/AI-MO/NuminaMath-7B-TIR\n",
       "\n",
       "AI, this has already been recently investigated in the form of an 'AI Scientist' [Lu et al., 2024a] that autonomously generated machine learning research articles. These are systems that conceivably will require minimal human input, merely guiding them towards certain theorems that one wants to see analyzed (proved, refuted, with comments on whether slightly different variations of a theorem, with slightly altered hypotheses, admit a proof).\n",
       "\n",
       "We focus in this article on general purpose models , as outlined above, which are mathematical copilots . LLMs are currently the systems that hold the most promise to become, given a better data foundation, general purpose models. Fully automated, universal models are currently out of reach, as they are contingent on the former category of models. These are the systems that are ultimately sought when one speaks of 'AI Mathematicians' [Bengio and Malkin, 2024]. However, without the proper datasets, it is unlikely that these will come into existence. Hence, our focus is on the data that is needed to arrive at general-purpose models, the mathematical copilots . We have deliberately avoided drawing a hard boundary among the properties a model must fulfill to be placed in one of these three because the boundaries will be fluid, as the levels of automation and required human interaction are continuous scales.\n",
       "\n",
       "Currently, the existing datasets only support highly specialized models and do not put LLMs on a trajectory to achieve general-purpose models. In this article, we highlight the limitations of current datasets (Section 3, which is retrospective) and necessary changes in terms of data that will be needed to train the next generation of LLMs that approximate what we outlined general-purpose models to be (Section 4, which is prospective).\n",
       "\n",
       "Several general principles have emerged from decades of machine learning research that shape expectations about AI progress. Perhaps the most influential is Sutton's 'bitter lesson' [Sutton, 2019], which observes that general methods leveraging computation (particularly search and learning) consistently outperform approaches that encode human domain knowledge. More recently, researchers have argued that openendedness - the capacity for systems to continually generate novel, learnable artifacts - is essential for artificial superhuman intelligence [Hughes et al., 2024]. However, there are reasons to believe these principles may not straightforwardly transfer to mathematical AI. Mathematics demands logically rigorous proofs that must be verifiable, not merely plausible; pattern matching and statistical regularities, which suffice in domains like vision or game-playing, are insufficient for mathematical reasoning. Moreover, the 'bitter lesson' presupposes abundant data that scales with computation, yet high-quality mathematical data, particularly at research level, is scarce, and much mathematical knowledge is implicit in workflows and heuristics that textbooks do not record. Open-endedness in mathematics requires more than generating novel outputs: it requires outputs that are correct and meaningful within a precise logical framework. These considerations suggest that progress in AI for mathematics may require domain-specific innovations in data, evaluation, and methodology that go beyond what has succeeded elsewhere in machine learning.\n",
       "\n",
       "The limits of end-to-end generation for mathematics. A recurring theme in machine learning is that end-to-end approaches - where models learn directly from inputs to outputs without explicit intermediate structure - often outperform systems with hand-designed pipelines [Huang et al., 2024]. However, we argue that for mathematical reasoning, pure end-to-end generation must be abandoned. Mathematics demands proofs that are not merely plausible but logically valid, and errors in intermediate steps propagate to invalidate entire arguments. Recent reasoning models address this by incorporating explicit chain-of-thought processes and, in formal settings, by interacting with proof assistants that verify each step [First et al., 2023, Song et al., 2024a]. The success of these hybrid approaches, which combine neural generation with symbolic verification, suggests that mathematical AI requires structured reasoning modes rather than monolithic sequence-to-sequence generation.\n",
       "\n",
       "Our main reader audience consists of machine learning researchers, as we intend this article to raise awareness of these issues. Automated theorem provers (ATPs) and interactive theorem provers (ITPs), which have a decades-old history [Harrison et al., 2014], also had the ambition of realizing the goal of implementing mathematical copilots. Unfortunately, history has shown that if the computer science community and the mathematical community do not operate in lockstep, there is a risk of obtaining systems that will not be of\n",
       "\n",
       "interest to practicing research mathematicians; maybe controversially, Blanchette et al. [2012] mentions that certain efforts to make these systems more user-friendly have perhaps reached a 'plateau'. (Nonetheless, ATPs and ITPs have, in other domains, contributed significantly to driving the field of software automation forward.)\n",
       "\n",
       "One of the goals of this paper is to make sure that the renewed effort by computer scientists and machine learning researchers to provide mathematicians with tools that make their trade easier this time is on a track to converge to the needs of mathematicians. We hope that one of the first manifestations of this will be in the form of datasets that better capture the various rich facets of mathematical research so LLM creators can provide scores on such benchmarks that are more informative for mathematicians regarding how useful the LLM is for actual daily mathematical practice. While the most-used datasets, GSM8K and MATH, are indicative of LLMs' reasoning abilities, they are largely irrelevant to mathematical practice.\n",
       "\n",
       "## 2 An Overview of Data-Related Issues\n",
       "\n",
       "Without using tools specialized for mathematics, the current general models, LLMs, simply by well-designed training methodologies and trained on mathematical data, have demonstrated unprecedented capabilities in generating humanlike mathematical text, solving complex problems, and even engaging in creative problemsolving. For example, GPT-4 has performed promisingly on undergraduate-level mathematics formulated in natural language, as pointed out by Frieder et al. [2023a]. Math-Specialized Gemini 1.5 Pro [Reid et al., 2024], a commercial model by Google not available to the public, has been reported to have an accuracy of over 90% on the MATH dataset [Hendrycks et al., 2021], one of the most widely-used datasets for testing mathematical ability. This score has recently been replicated by an open-weight model, QwQ. 4 Attaining such a high score is equivalent to achieving the ability of an IMO gold medallist (according to [Hendrycks et al., 2021]). However, to date, no reports have been made where the mathematical benchmarks of QwQ are tested for contamination; not undertaking tests may lead to this score not being reproducible on a different dataset from the same difficulty and problem type distribution [Yang et al., 2023, Xu et al., 2024].\n",
       "\n",
       "Benchmark saturation on MATH. Recent reasoning models approach or exceed 90% accuracy on the MATH benchmark [Hendrycks et al., 2021], which was considered extremely challenging at introduction (models scored below 7%). On MATH -500, a commonly used 500-problem subset, DeepSeek-R1 reports 97.3% DeepSeek-AI [2025] and QwQ-32B-Preview reports 90.6% Qwen Team [2024]. Earlier models scored lower: Gemini 1.5 Pro achieved 77.9% on full MATH with majority voting Reid et al. [2024]. This rapid progression leaves limited headroom under boxed-answer scoring and motivates harder benchmarks like FrontierMath [Glazer et al., 2024].\n",
       "\n",
       "DeepTheorem reports a large corpus of informal IMO-style theorems paired with proofs, and uses entailed and contradictory variants to provide a binary reward signal for reinforcement learning in informal theorem proving [Zhang et al., 2025a]. Notably, this is a step beyond standard boxed-answer scoring: the supervision and evaluation are tied to logical consistency across theorem variants rather than only matching a numeric final answer.\n",
       "\n",
       "These benchmark improvements do not, by themselves, resolve the evaluation and data problems that matter for mathematical copilots. First, high accuracy under boxed-answer scoring (still common in public benchmarks) leaves ambiguity about what changed, since multiple failure modes in the reasoning trace can be invisible once only the final answer is checked. Second, evidence from out-of-distribution tests suggests that strong benchmark scores can coexist with brittle reasoning on simple variants of problems [Nezhurina et al., 2024]. Third, as scores rise, contamination and memorization become harder to rule out with simple string-based decontamination, especially when mathematically equivalent problems can be phrased in many surface forms [Xu et al., 2024]. These points motivate evaluation sets with stronger verification and a shift toward datasets that expose intermediate structure.\n",
       "\n",
       "4 https://qwenlm.github.io/blog/qwq-32b-preview/\n",
       "\n",
       "Further notable general-purpose LLMs are open-weight models with strong reported performance on baseline datasets, such as MATH and GSM8K [Cobbe et al., 2021], the DeepSeek family of models [Shao et al., 2024, Liu et al., 2024, Zhu et al., 2024, Xin et al., 2024], and the Qwen family of models [Bai et al., 2023, Yang et al., 2024a, Hui et al., 2024, Yang et al., 2024b]. Other LLMs focus more on specific abilities such as MathPrompter [Imani et al., 2023], which associates a confidence value to arithmetic problem solutions, MathVista [Lu et al., 2024b], which solves geometric reasoning problems, WizardMath [Luo et al., 2023], or Llemma [Azerbayev et al., 2024], which was specialized through further pretraining on a math corpus, and is both open-weight and open-source. A special use case where LLMs have found success is in interactive theorem provers (ITPs) such as Isabelle [Nipkow et al., 2002] or Lean [de Moura and Ullrich, 2021], where they are used to prove theorems most prominently in the context of generating proofs of given formal statements [First et al., 2023, Zheng et al., 2023, Wang et al., 2024a], autoformalization [Szegedy, 2020, Jiang et al., 2023], or providing code snippets of formal mathematics [Song et al., 2024a]. For a survey of deep learning approaches on both formal and informal (natural language) mathematics, we refer to Lu et al. [2023], and for more recent information on language models, including their performance on mainstream datasets, we refer to Zhao et al. [2023].\n",
       "\n",
       "This paper argues that the current dataset landscape does not support the advancement of such generalpurpose AI systems, in particular LLMs, towards a level of mathematical performance that makes them usable as daily mathematical thought partners [Collins et al., 2024b] that help mathematicians push the boundaries of what is known, and capture the richness of mathematical reasoning and invention that human mathematical minds are capable of [Zhang et al., 2023a, Dehaene, 2011, Feigenson et al., 2004]. Our focus thus includes, in particular, research-level mathematics, which involves many steps beyond deriving or formalizing proofs - the aspects of mathematical practice that are currently most strongly represented in data.\n",
       "\n",
       "The spectrum of tasks a copilot can assist with is not solely related to mathematics: programmers presently carry out programming tasks, such as fixing bugs or other GitHub issues [Jimenez et al., 2024], using various code-generating copilots, such as the GitHub Copilot [Chen et al., 2021]; the emerging Lean Copilot ecosystem [Yang et al., 2024c, Song et al., 2024b] assists users in the task of formalizing a mathematical theorem; Tutor CoPilot [Wang et al., 2024b] provides real-time guidance to human tutors that teach school students mathematics at the level of math word problems.\n",
       "\n",
       "Limits of relying on a single formal system. Interactive theorem provers such as Lean provide machinecheckable correctness, and stronger automation inside these systems is valuable. However, there are limits to what is natural to express and maintain inside a single formal foundation and library, even when the underlying mathematics is standard. The following non-exhaustive examples illustrate why a mathematical copilot cannot be reduced to a Lean-only workflow.\n",
       "\n",
       "First, formal developments can incur large overhead when a concept has many surface variants. Patrick Massot has pointed out this issue using the elementary calculus notion of a limit: a naive approach that treats each variant of 'limit at a point' as a distinct definition leads to a combinatorial explosion in the number of definitions and routine plumbing lemmas (his talk gives concrete counts and examples) [Massot, 2022]. In practice, mathematicians avoid learning and manipulating hundreds of separate limit definitions by teaching an abstract generator behind them, or by moving to a more uniform formalism (for example, ultrafilter-based formulations). Both approaches can be implemented formally, but they change what is convenient to teach, state, and reuse.\n",
       "\n",
       "Second, some common mathematical activities are more naturally carried out in informal language. Concise proof exposition, proof summarization, and 'zooming out' to communicate the core idea of a long argument are straightforward in natural language, while formal proofs typically expose low-level structure. Producing readable summaries from formal developments is a separate task that requires its own representations and data.\n",
       "\n",
       "Third, conjecturing and exploration tend to happen before the final formal statement is stable. Early-stage work often involves proposing definitions, testing variants, and revising hypotheses. Doing all of this directly\n",
       "\n",
       "in Lean can add technical overhead at a point where the main goal is to move quickly through candidate formulations.\n",
       "\n",
       "Fourth, the choice of foundations affects what counts as 'the same' object. For example, work in homotopy type theory was driven in part by a refined analysis of equality and equivalence. Informal mathematics routinely treats objects as equal 'at the level that matters' in a given theory, even when they are not definitional equalities in a fixed foundational encoding. Formal developments must commit to a specific notion of equality and transport, and that commitment can make some styles of reasoning less direct.\n",
       "\n",
       "In the course of proving a theorem, one may need to convert between more abstract mathematics and less abstract mathematics. Flexible conversion might prove to be more difficult in Lean.\n",
       "\n",
       "These limitations do not argue against formalization. They suggest that Lean is one tool in a broader toolbox, and that mathematical copilots will likely need a federation of tools and representations, including automation over informal mathematics, to support rapid iteration in research workflows.\n",
       "\n",
       "Mathematics, with its rich set of domain-dependent workflows, tools, and modes of thinking, requires copilots with exceptionally broad capabilities. A mathematical copilot that is useful to a mathematician must significantly exceed the capabilities of the three exemplary copilots mentioned above, which are systems that are designed to help with a narrow technical set of tasks.\n",
       "\n",
       "Our paper is split into three main sections : In the ensuing Section 3, we review past datasets that may have limitations of various kinds. We split these into two further categories, relating to natural language and formal language datasets.\n",
       "\n",
       "Training corpora vs benchmarks. In this paper, the term dataset covers two use cases: (i) corpora used for pretraining, fine-tuning, or reinforcement learning, and (ii) held-out benchmarks used to report model performance. Some collections are used in both ways. Benchmarks are also sometimes converted into training data, for example by creating instruction-style variants of benchmark problems. As a result, many of the issues discussed below apply to both training and benchmarking, but they affect different parts of the pipeline. Difficulty calibration and interaction-mode coverage affect the supervision signal during training and the amount of headroom in a benchmark. Contamination affects both use cases: benchmark items can appear in training corpora, and benchmark scores can be inflated when test problems, or close variants, leak into training data. Other issues depend more strongly on the use case. Binary scoring and the evaluation protocol determine what a benchmark score measures. For formal mathematics, placeholders ( sorry , Admitted ), duplication across alternative formalizations, and toolchain provenance affect both the quality of training data and the interpretation of evaluation results. Table 2 summarizes these distinctions.\n",
       "\n",
       "To give an overview of our contributions in that section, current natural language datasets typically model a question-response interaction, such as posing a problem and writing a solution. We identify dimensions where such datasets frequently fall short of being able to assess advanced mathematics:\n",
       "\n",
       "- Difficulty : Most datasets are situated on a level of difficulty at or below the level of an undergraduate degree in mathematics, or impossibly hard, with few datasets at an appropriate level of difficulty, from which model creators can better learn about the strengths and weaknesses of their system.\n",
       "- Binary evaluation : Benchmarks typically only support assessing if the output was correct or incorrect, but in case of errors, do not provide information about how the output failed to be correct, which is crucial to support.\n",
       "- Lack of trusted automation evaluation : Benchmarks rely on checking a final answer (generally, by keyword matching), imperfect model-based evaluation, or provide no automatic evaluation at all.\n",
       "- Standardized Interaction Modes : Often, natural language datasets only contain question-answer mathematical problems. The daily experience of a mathematician is much richer than that [Zhang et al., 2023a], since to prove/search for a theorem, all kinds of 'lateral' modes of thinking have to be applied.\n",
       "\n",
       "Examples are intermediate conjecture formulation, (counter)example search, and finding analogies with other theorems. These are typically not found in textbooks, so there is no representation of these in terms of data. We point to Collins et al. [2024a] for an exposition of the problems of static evaluation for natural-language mathematics problems in practice. In short, static evaluation relies on fixed input-output pairs that fail to capture iterative or adaptive interactions, such as clarifying questions and corrections, that are an integral part of effective problem solving.\n",
       "\n",
       "- Contamination : Many datasets released solely for evaluation and benchmarking purposes have a short half-life, as, once released, they are easily leaked into datasets that will be used as training data for future models.\n",
       "- (Binned) Sankey Diagrams : Most benchmarks are snapshots of current models, and don't record clearly how performance is changing. This makes it hard to assess the broader direction of evolution of language models w.r.t. mathematics. (Binned) Sankey Diagram offer an easy remedy, but have not been embraced by the community.\n",
       "\n",
       "Several of these issues are also present for formal mathematics. But benchmarks around formal mathematics frequently suffer from distinct issues than those listed above; for instance, while datasets for formal mathematics come at a range of difficulties, from competition-level problems [Zheng et al., 2022] to undergraduate textbooks [Azerbayev et al., 2023], to some results that are at the mathematical state-of-the-art [Scholze, 2022, Bordg et al., 2022], formalized mathematics evaluation can be easily carried out in a safe and automated manner since proofs can be checked by the ITP, rendering the issue of trusted automatic evaluation irrelevant. On the other hand, the following dimensions are problematic for datasets related to formal mathematics:\n",
       "\n",
       "- Tool misalignment : Various automation tools exist that aid in proving small, intermediate lemmas arising in the formalization of more comprehensive results. This creates incentives to change a proof's structure to maximize the use of these tools, potentially resulting in less readable and 'human looking' proofs. This misalignment often manifests as proof-script scaffolding, such as broad tactics applied repeatedly, large automation argument lists, or redundant steps that make proofs brittle and non-idiomatic. Since such redundancy is frequently detectable by linters (e.g. unused simp arguments), it can be treated as dataset metadata in corpora that include a raw view for realism along with a linter-cleaned view that better reflects canonical proof structure.\n",
       "- Non-trivial data duplication : In mathematics, it is common that one theoretical concept has several formal representations. For example, the concept of a graph can be described formally in multiple ways. Formal libraries typically feature a variety of representations for the same mathematical concept, some being almost exact copies of each other. Such near-duplication can lead to downstream issues when the library is both used as training and test data in machine learning approaches. Duplication also arises at the level of lemma interfaces: when a library lacks a lemma in the form needed, developers introduce thin wrapper lemmas or local variants to patch an API gap. These near-duplicates are often engineering artifacts rather than conceptual novelty, and they can inflate the overlap between training and evaluation in ways that are invisible to string-based de-duplication.\n",
       "- Standardized interaction modes : (Analogous to natural-language, but with different characteristics) Current formal benchmarks focus on proving theorems in isolation or translating individual statements into formal code. These tasks do not capture all aspects of advanced mathematical practice. In practice, formalization is dominated by interactive patterns that are mostly absent from current benchmarks, including iterative repair (compile → inspect diagnostics → patch), systematic search for lemmas or imports, and refactoring for readability and performance. For example, Lean supports interactive lemma search via exact? , apply? , and rw? , which reflect a workflow that interleaves searching and proving, rather than proof synthesis from scratch.\n",
       "- Scope of formalization and handling of incomplete proofs : Formal-math corpora vary substantially in scope and granularity . Many benchmarks emphasize short, isolated theorems or textbook-style exercises [Zheng et al., 2022, Azerbayev et al., 2023], whereas large-scale formalization projects capture\n",
       "\n",
       "long-form arguments that unfold across hundreds of intermediate lemmas before reaching a major theorem [Scholze, 2022, Bordg et al., 2022]. This matters for dataset construction and evaluation: models trained or tested only on short lemmas may appear strong while still failing to manage the dependency structure, planning, and bookkeeping required for long developments.\n",
       "\n",
       "A second, closely related issue is incompleteness during development . In Lean, unfinished results are often recorded using sorry , and in Coq using Admitted ; these placeholders accelerate library growth but may render a corpus logically incomplete (and, in the limit, inconsistent). If such items are included unmarked in training data, a model observes theorems treated as 'proven' without ever seeing a proof term, which can distort learning and confound evaluation. This phenomenon is common in actively developed libraries; one example is SciLean [Skrivan, 2025]. Consequently, formal-math datasets should explicitly document how placeholders are handled (e.g., excluded, retained with flags, or separated into a distinct split), and should report basic completeness statistics (e.g., number or fraction of theorems containing placeholders, and where they occur in dependency graphs).\n",
       "\n",
       "Finally, to better reflect real mathematical practice, there is value in long-form curated corpora where an entire paper or chapter is represented as a structured development: (1) statements, (2) complete machinechecked proofs when available, (3) dependency metadata, and (optionally) (4) high-level commentary about proof strategy and lemma roles. Even when full completion is infeasible, clearly distinguishing finished proofs from partial ones enables more meaningful training objectives (e.g., proving the remaining sorry lemmas) and more reliable benchmarks.\n",
       "\n",
       "- Version fragility and tactic stability : Formal proofs are executable artifacts whose replay can be brittle under prover or library drift. A common failure mode is tactic instability. For instance, in Lean a 'flexible' tactic such as simp changes the goal in a context-dependent way, and downstream steps silently assume a particular postsimp shape (e.g. after simplification one must start a calc chain from the actual goal state, not the expected form).\n",
       "- Hidden context dependence (imports, local instances, and attribute state) : Even with a fixed statement, the behavior of a proof script can depend on surrounding environment, including imports, local instances, and attribute state (e.g. simp lemmas). In Lean, instance 'pollution' can occur when local bindings introduce alternate instances (such as multiple MeasurableSpace structures), causing later lines to elaborate against an unintended instance unless the intended instance is explicitly pinned.\n",
       "- Resource sensitivity (heartbeats and elaboration/typeclass search) : Correctness is not cleanly separable from performance: proofs can fail due to heartbeat limits or timeouts caused by elaboration or typeclass synthesis rather than mathematical content. This is especially salient in large developments where small refactors or additional instances can cause elaboration to take much longer, possibly more than the previous heartbeat limit.\n",
       "- Redundant scaffolding and non-canonical proof artifacts : Formal corpora often contain redundant tactic steps and linter-detectable redundancy (such as unused automation arguments, no-op steps) that are harmless for one-off development but distort the signal for training and evaluation. For instance, a tactic may already close the goal, making subsequent steps ill-typed (e.g. leading to a Lean error 'no goals to be solved'). A useful dataset practice is to include both a raw corpus and a lint-cleaned canonicalized view.\n",
       "- Toolchain and automation : Many formal corpora are now partially produced by automated or semiautomated methods-ranging from autoformalization tools that attempt to translate informal text into formal proofs [Jiang et al., 2023], to specialized rewriting tactics, to code-generation scripts that unify definitions across libraries. Current datasets may not have enough detail about which automated methods were used and how their outputs were curated. A related form of provenance is linter metadata: Mathlibstyle linters flag redundant arguments, unstable patterns, and stylistic violations, and recording which lints were applied (or waived) can help downstream users understand how canonical the resulting proof scripts are. Similarly, explicit set\\_option declarations (e.g. heartbeat or recursion-depth overrides) should be treated as dataset metadata, since proofs that require non-default resource limits may behave differently\n",
       "\n",
       "under standard evaluation harnesses.\n",
       "\n",
       "- Relation of dataset to existing formal libraries : Many formal datasets build upon large existing libraries (e.g. mathlib in Lean). If a new dataset reuses or extends such a library, overlaps or version discrepancies can arise. Two projects may formalize the same theorem in slightly different ways, or rely on different versions of the library's foundational definitions. Version fragility is particularly acute for automation-heavy proofs, as changes to typeclass instance priorities, simp-lemma sets, or default tactic behavior can cause a proof that worked in one commit to fail silently or timeout in the next, even when the underlying mathematics is unchanged.\n",
       "- Unclear benchmarking protocols : Formal mathematics datasets are frequently used as benchmarks for automated theorem proving and related model-based formal reasoning. However, papers often report a single success rate without specifying the task definition and evaluation protocol in enough detail to make results comparable.\n",
       "\n",
       "At minimum, a formal-math benchmark should state which task is being evaluated. Common variants include: (i) theorem proving from scratch (given a statement and imports, produce a proof that checks), (ii) tactic prediction (given a proof state, predict the next tactic or a short tactic script), (iii) proof repair or hole filling (complete a partially written proof term or script), (iv) premise selection (select a set of lemmas from a library that suffices to close a goal), and (v) autoformalization (translate an informal statement into a formal one, optionally followed by proving it).\n",
       "\n",
       "Even within a single task label, the protocol can change difficulty by orders of magnitude. For reproducibility, benchmark authors should specify:\n",
       "\n",
       "- Input specification : what the model receives (the statement only, or also local context such as imported modules, available lemmas, and current proof state), what implicit environment is assumed (active typeclass instances, scoped attributes, and simp-lemma sets), and any context limits (number of retrieved premises, maximum file size, or maximum tokens).\n",
       "- Output specification : what counts as a valid submission (a full proof script, a proof term, a sequence of tactics, or a patch to an existing file).\n",
       "- Success criterion : whether the artifact must be accepted by the checker without placeholders (e.g., no sorry / admit ), and what counts as failure (type error, timeout, unsolved goals).\n",
       "- Tooling and automation : which tactics and automation are permitted (e.g., rewriting and simplification tactics, SMT calls, hammer-style tools), and whether tool use is counted as part of the model or treated as an external oracle. If multiple settings are relevant, separate tracks (for example, 'with automation' and 'restricted automation') avoid conflating results.\n",
       "- Resource constraints : wall-clock time per goal, memory limits, maximum number of proof attempts, and any search limits (tactic budget, backtracking depth). Limits that are internal to the prover matter as well: heartbeat budgets, typeclass-synthesis depth, and recursion limits can all cause a correct proof script to fail silently or timeout when replayed under different settings. These choices interact strongly with inference-time compute and must be reported.\n",
       "- Environment pinning : the exact proof assistant version and library commit (for example, Lean + mathlib revision, or Isabelle + AFP snapshot), since minor version changes can make proofs succeed or fail. An evaluation harness that replays proofs in a fixed environment reduces ambiguity.\n",
       "- Metrics : beyond a single success rate, report pass@ k when multiple attempts are allowed, average time to solution, and optional secondary measures such as proof length or number of tactic steps, since these affect usability.\n",
       "\n",
       "Without these details, two papers can report the same score while solving different tasks under different constraints. Explicit protocols and pinned environments enable reliable comparison across models and\n",
       "\n",
       "make it possible to interpret gains as improvements in formal reasoning rather than changes in tooling or evaluation setup.\n",
       "\n",
       "## 2.0.1 Mixed-mode datasets for tool use\n",
       "\n",
       "Many candidate mathematical copilots are not text-in, text-out systems. They are trained and evaluated in settings where a model is expected to call external tools, such as a computer algebra system, a numerical solver, a proof assistant, a retrieval system, or a specialized symbolic model. Tool-integrated systems already appear in work on tool-integrated reasoning for math (for example, models that generate and execute Python code as part of the solution process [Gou et al., 2024]).\n",
       "\n",
       "A mixed-mode dataset records these interactions explicitly. Rather than storing only a problem statement and a final response, each datapoint is a short trajectory consisting of interleaved representations, for example: (i) natural-language problem context, (ii) intermediate formal objects (expressions, goals, proof states, or formal statements), (iii) tool calls with arguments, (iv) tool outputs (for example, the result of executing code, a CAS simplification, or a proof assistant error message), and (v) a final response that integrates the tool outputs into a mathematically coherent explanation. This structure supports training objectives beyond next-token prediction, including learning when to call a tool, how to translate between representations, and how to recover from tool failures (timeouts, type errors, missing imports, or non-terminating proof searches).\n",
       "\n",
       "A special case of mixed-mode data consists of datasets pairing informal and formal mathematics. These datasets support autoformalization and naturalization, as well as theorem proving conditioned on an informal statement. Examples include miniF2F [Zheng et al., 2022] and ProofNet [Azerbayev et al., 2023]. More recent efforts scale paired informal-formal corpora to substantially larger sizes [Ying et al., 2024], which matters for training rather than only evaluation. In these settings, the tool is the proof assistant itself: correctness can be checked by verifying the generated formal artifact, while intermediate feedback takes the form of proof-state information and type errors, which can be recorded in the dataset.\n",
       "\n",
       "Mixed-mode datasets introduce evaluation and reproducibility issues that do not arise in static text-only benchmarks. Tool outputs depend on the toolchain (version, library state, and sometimes randomness). A dataset intended for benchmarking should therefore specify the tool interface and pin versions (for example, by commit hashes for proof assistant libraries), and provide an evaluation harness that can replay tool calls in a controlled environment. Without this, differences in tool versions can change whether a proof checks or whether a symbolic computation succeeds, making scores hard to compare across papers.\n",
       "\n",
       "Recommendations. For mixed-mode datasets used for training or benchmarking, we recommend (1) representing each datapoint as an explicit tool-interaction trace, including tool inputs and outputs; (2) recording the toolchain in sufficient detail to replay the trace (versions, dependencies, and execution settings); and (3) separating tasks where tool use is optional from tasks where tool use is required, so that evaluation can distinguish mathematical reasoning from tool invocation competence. Mixed-mode datasets also interact with the contamination problem: if tool outputs encode the final answer too directly, a model can learn to pattern-match on tool outputs rather than learning to select and use tools effectively. This motivates evaluation protocols that check both the final answer and the correctness of the tool trace itself (for example, by re-executing code or re-checking formal proofs).\n",
       "\n",
       "These limitations have various consequences: For example, evaluating progress of the mathematical capabilities of AI systems, particularly for advanced mathematics, is difficult. For instance, improved accuracy on a standard problem-solving dataset may not provide a meaningful signal about what has improved, while performing well on an existing formal benchmark may not accurately indicate where models currently struggle.\n",
       "\n",
       "In the next section, Section 4, we discuss how certain aspects of the proof creation can be mapped to datasets. We argue that such datasets that describe intermediate stages that mathematicians go through in the long process of devising proofs are potentially highly valuable to support mathematical copilots that assist humans in these tasks. The overt focus of current datasets on result (the final proof), rather than the process\n",
       "\n",
       "that led to that proof, severely diminishes their usefulness for training LLMs to be used as mathematical copilots. While from a perspective of pure proof search, results-based datasets are justified, it is unlikely that mathematics will soon be reduced purely to proof search, as there are several mathematical activities (such as finding the 'right' set of definitions, that makes a mathematical theory clear and easy to follow) that are not solely about proof search. Such datasets that describe these intermediate stages do not exist at all currently! The GHOSTS dataset [Frieder et al., 2023a], to our knowledge, is the only one that preliminarily investigates how well LLMs can engage in literature search. Thus, in Section 4.1, we take a more holistic level to describe how mathematical workflow might be described by datasets. This problem is essential, as a lot of mathematical activity is based on chaining together complex workflows. There is currently no clear way of measuring the advances of AI systems in tackling the full spectrum of mathematical workflows. Such data about intermediate stages in the proof discovery process is often not explicitly contained in textbooks and articles - the data sources on which current LLMs are trained. In Section 4.3, novel ways of capturing data that may contain this information are described. Finally, in Section 4.4, we discuss a stricter proof structure called a 'motivated proof', requiring the author to explain the origin behind each step of a proof. We then argue for an evaluation benchmark enforcing this proof structure from the perspectives of end-user utility and evidence of LLM reasoning rather than LLM memorization (coming up with a motivated proof, given an unmotivated one, requires reasoning and deeper proof understanding) and speculate on paths toward designing such a benchmark.\n",
       "\n",
       "We note that in this article we do not focus on datasets that are designed primarily to be consumed by the current LLM technology stack, for example, various instruction tuning datasets, such as the OpenMathInstruct dataset [Toshniwal et al., 2024]. These are often based on existing datasets (GSM8K and MATH in the case of OpenMathInstruct) and thus could be described as derived datasets. Rather, our focus is on assessing how much and how well primary datasets describe various aspects of doing mathematics .\n",
       "\n",
       "## 3 The Past - Common Pitfalls in Current Datasets\n",
       "\n",
       "In this section, we discuss several issues with current datasets. Note that our limitations are with respect to the landscape of AI for mathematics at the time of writing (Dec 2024); it is possible that some of these limitations are addressed by the time the reader reads this article. Indeed, we hope that they are addressed swiftly.\n",
       "\n",
       "Underlying several of these problems is a dynamic described by Goodhart's law: 'When a measure becomes a target, it ceases to be a good measure.' A related formulation, Campbell's law [Campbell, 1979], warns that quantitative indicators used for decision-making become increasingly subject to corruption pressures. In the context of mathematical AI, when LLM developers optimize for specific benchmark scores (such as accuracy on MATH or GSM8K), models may learn to exploit superficial patterns or benefit from data contamination rather than developing robust mathematical reasoning. High benchmark scores then no longer reliably indicate the capabilities that matter for mathematical copilots. This motivates our call for richer evaluation frameworks that are harder to 'game' and that measure diverse facets of mathematical competence. Notably, the NLP community identified these same issues years ago and proposed solutions such as dynamic, adversarial benchmark creation [Kiela et al., 2021], yet the math-AI community has largely continued to rely on static benchmarks susceptible to the same failure modes.\n",
       "\n",
       "Recent near-saturation results on boxed-answer benchmarks [DeepSeek-AI, 2025, Qwen Team, 2024] are consistent with this dynamic: as optimization concentrates on a small set of public targets, benchmark scores become less informative about broader mathematical competence.\n",
       "\n",
       "We first list issues that are common to both natural-language datasets and datasets comprising formalized mathematics. Then, we list issues that are specific to either natural-language datasets or formal datasets.\n",
       "\n",
       "## 3.1 Common Issues\n",
       "\n",
       "## 3.1.1 Difficulty\n",
       "\n",
       "The difficulty of mathematical problems typically varies along at least two salient dimensions: level of abstraction and proof (or solution) sophistication. Abstraction varies from simple 'one-layer' definitions, such as those that involve properties of topological spaces, to those in areas such as algebraic geometry, where definitions of highly intricate mathematical objects involve several layers.\n",
       "\n",
       "Proof sophistication is related to problem-solving ability and is often tested (for humans and AI systems alike) in mathematical competitions, such as the International Mathematical Olympiad (IMO). Typical for these is that the statements to be proved are elementary, but highly ingenious proof techniques need to be used to arrive at the solution.\n",
       "\n",
       "Several datasets have been proposed which explore proof sophistication, such as the OlympiadBench [He et al., 2024], the IMO Small Challenge, or the IMO-AG-30 dataset [Trinh et al., 2024], but very few existing datasets reach a level of difficulty that is more advanced than that of an undergraduate degree in mathematics in terms of level of abstraction. For a (non-exhaustive) overview of datasets, ordered by their difficulty, see Table 3.\n",
       "\n",
       "For example, MATH focuses on high-school competition problems, while the recent TheoremQA [Chen et al., 2023] and Advanced Reasoning Benchmark (ARB) [Sawada et al., 2023] are upper-undergraduate level. Many commonly used datasets test mathematics below the undergraduate level, such as GSM8k and the 23 tasks in the Lila benchmark [Mishra et al., 2022b].\n",
       "\n",
       "Two datasets that go beyond upper-undergraduate level are the GHOSTS dataset [Frieder et al., 2023c], as well as the NaturalProofs(-Gen) [Welleck et al., 2021, 2022] datasets, which contain problems, theorems, and proofs on a wide range of more advanced mathematical topics. However, these suffer from a lack of automated evaluation and contain many problems that could, in principle, be in the training corpora of modern LLMs, as we discuss later. FrontierMath [Glazer et al., 2024] is also a more advanced dataset, but is currently not publicly accessible. STORM-BORN is a recent derivation-style benchmark curated from recent research papers; it reports that strong models solve only a small fraction of the selected problems without task-specific training [Liu et al., 2025].\n",
       "\n",
       "Most current LLMS (e.g., the Qwen [Yang et al., 2024a] family of models, or commercial models released by OpenAI and Google) are evaluated on the MATH dataset [Hendrycks et al., 2021] and the GSM8K dataset [Cobbe et al., 2021], with a recent trend of some LLMs using held-out datasets such as the Hungarian National Finals high-school exam as an additional test of generalization [Paster, 2023]. The MATH dataset uses five categories of difficulty and is strictly more difficult than GSM8K. Nonetheless, state-of-the-art LLMs have been found to reach an acceptable performance on it [Frieder et al., 2023a]. For instance, Gemini reaches a performance of about 50%. 5 , while solving almost all problems on GSM8K. Progress in LLMs is proceeding at a rapid pace, and the 'Math-Specialized' version of Gemini 1.5 reaches (at best) a score of 91% on MATH, which is representative of the current state of the art (although, due to the closed nature of this line of research, it is difficult to place this achievement into context, such as by analyzing how similar training data has been to the MATH dataset, and how much was needed).\n",
       "\n",
       "While this performance may appear impressive, neither of these datasets covers a high degree of difficulty in terms of abstractness and proof sophistication. Hence, the current performance of LLMs on these datasets is not yet indicative of advanced mathematical understanding, which is relevant for professional mathematicians.\n",
       "\n",
       "This is particularly striking in the case of AlphaGeometry, which can solve 25 geometry problems from the IMO out of a test set of 30 problems [Trinh et al., 2024] (and 213 out of 231 on a larger benchmark). We argue that the accompanying dataset, while indeed difficult in terms of proof sophistication, is not sufficiently large to be able to accurately compare AlphaGeometry with other models.\n",
       "\n",
       "5 https://blog.google/technology/ai/google-gemini-ai/performance\n",
       "\n",
       "Moreover, recent work casts doubt on whether high benchmark scores reflect genuine mathematical reasoning. Nezhurina et al. [2024] demonstrate that GPT-4, Claude 3 Opus, and other state-of-the-art LLMs at the time, exhibit dramatic reasoning failures on simple common-sense math problems (the 'Alice in Wonderland' or AIW problems) that are trivially solvable by humans. These failures persist even with chain-of-thought prompting and multi-step re-evaluation, and models express strong overconfidence in their incorrect answers. If LLMs struggle with such elementary problems, it is difficult to credit their high performance on substantially harder benchmarks like MATH to genuine reasoning rather than to memorization or pattern-matching on familiar problem types. Proof or Bluff? reports expert grading of model solutions on the 2025 USAMO and finds low proof scores even for models that score well on short-answer benchmarks [Petrov et al., 2025]. This tension motivates two directions: first, the development of out-of-distribution evaluation sets - for instance, via procedural generation of novel problems - that are less susceptible to memorization; and second, decontamination methods tailored specifically to mathematics, which must account for the fact that mathematically equivalent problems can appear syntactically different.\n",
       "\n",
       "Recommendations. We believe that it is crucial to introduce more difficult datasets into the evaluation repertoire of math-AI. And importantly, the skill level of these datasets should be graded , such that we can form a nuanced characterization of model capabilities - this can be achieved, in part, by interpolating difficulty level between existing datasets and those representative of advanced mathematics. This includes datasets on graduate-level mathematical domains, such as functional analysis (to measure handling of arguments involving high levels of abstraction), partial differential equations (to measure handling of intricate computational arguments), as well as other domains that come with their own 'mathematical flavor'. At the same time, more comprehensive datasets illustrating and measuring advanced problem-solving skills are needed. The miniF2F [Zheng et al., 2022] dataset and the GHOSTS dataset are just the first steps in this direction. For natural language datasets, we note that simply scraping many competition-level problem sets, coupled with an evaluation principle that only verifies a 'final answer', is not sufficient to ensure that systems can progress to becoming useable mathematical copilots. Such problem sets, even if some of them ultimately are only math word problems, require expert, specialized knowledge and training for humans to be proficient. Without a detailed evaluation protocol (see Section 3.1.2), not much insight will be gained into why poor performance was obtained - and, conversely, human evaluation will be very costly (see Section 3.2.1).\n",
       "\n",
       "## 3.1.2 Binary Benchmarks\n",
       "\n",
       "Most datasets tailored for machine learning, except GHOSTS and NaturalProofs-Gen, including the dataset for AlphaGeometry, use a simple correct-incorrect rating scheme. The given feedback indicates if a system fails to solve a problem correctly, but without any indication of how it fails. Binary evaluation thus provides limited information on how to improve or interpret performance.\n",
       "\n",
       "Benchmark diagnosticity and 'training on the test task.' A further limitation of many mathematics benchmarks is that they offer little insight into why a model succeeded or failed, beyond a single correctness bit. In practice, this can make it hard to improve a system other than by training on large quantities of data that is closely matched to the benchmark task distribution. Dominguez-Olmedo et al. [Dominguez-Olmedo et al., 2024] describe this as training on the test task : unlike leakage or contamination, it is a legitimate use of knowledge about benchmark tasks at training time, but it can confound both relative model comparisons and claims about emergent abilities. In mathematics, the problem is amplified by the concentration of benchmarks on a small set of task formats; Frieder et al. [Frieder et al., 2023a] describe a 'vast unpopulated landscape of benchmarks' beyond the few overstudied datasets. One response is to make benchmarks more diagnostic by attaching structured feedback to failures, such as error codes (as in GHOSTS) that distinguish common mathematical failure modes, which can guide targeted data curation and model improvements rather than generic task-matching fine-tuning.\n",
       "\n",
       "GHOSTS and NaturalProofs-Gen address this problem by using labels that are attached to language model outputs, which provide a classification of (mathematical) errors. For instance, GHOSTS distinguishes two\n",
       "\n",
       "types of labels, called 'error codes' and 'warning codes', which make up, in total, 15 error or warning labels, while NaturalProofs-Gen distinguishes between 11 types of errors. A small number of datasets use non-binary error rating schemes, to our knowledge these are only TheoremQA (3-point scheme) and ARB (5-point).\n",
       "\n",
       "Finally, formalized mathematics offers feedback from the interactive theorem prover (ITP) when a system fails to produce a correct proof (one case being timeouts). However, this feedback is more akin to a compiler error than high-level feedback on the mathematical reasoning process. We speculate that this feedback may be useful but still limited in its ability to diagnose AI systems.\n",
       "\n",
       "Recommendations. We believe that it is crucial to develop datasets that support multi-aspect feedback. Relying on a single numerical representation, in the face of the diversity of mathematical failure modes, risks providing a signal that is too weak to be suitable for learning or evaluation - which is especially important to guard against when we consider deploying systems alongside people, who may care not only that a model is correct but also that the response is appropriately helpful [Collins et al., 2024a].\n",
       "\n",
       "One of the reasons that AI researchers may have avoided multi-aspect feedback, particularly subjective feedback, is for fear of the difficulty of obtaining consistent human annotations. While annotator disagreement is a challenge, we suggest that the math-AI community look to the ways in which computer vision researchers have worked on developing new theoretical and empirical tools which lean in to the diversity of human responses [Uma et al., 2021, Collins et al., 2022, Sucholutsky et al., 2023, Gordon et al., 2022]. Appropriately interleaving formality and subjectivity - especially for advanced mathematics, where there is a limited pool of possible human annotators - is a challenge, but one which demands cross-disciplinary collaboration and is ripe for future work. As a middle-ground, we also recommend considering evaluation methods that do not assign numerical ratings at all - but rather annotate the outputs of AI systems with error codes and warning codes, then derive ratings from the codes directly (where no error codes mean a perfect rating).\n",
       "\n",
       "## 3.1.3 Standardized Interaction Modes\n",
       "\n",
       "Almost all datasets to date that are designed to evaluate LLMs capture only textbook-like questions. Moreover, owing to issues of automatic evaluation described in Section 3.2.1, the majority of datasets focus on a smaller subset of questions with a unique, typically numerical, answer. While such questions might span a range of different topics, from numerical (e.g., GSM8K, problems of level 1-2 from MATH, MultArith) and symbolic evaluations (the dataset associated with the work of [Lample and Charton, 2020]) to somewhat complex problem-solving tasks (problems of level 4-5 from MATH), they do not come close to covering all aspects of the mathematical tasks a mathematician might encounter in their daily practice. We believe that any future mathematical assistant should provide support for these tasks.\n",
       "\n",
       "In the following list, we describe different dimensions of interaction modes for natural-language mathematics (both question types as well as session types), across which we believe that mathematical assistants should be evaluated.\n",
       "\n",
       "## Question Types.\n",
       "\n",
       "1. School/University Curriculum-Like Questions that are encountered in educational settings and have well-defined answers. This includes questions with arithmetic, symbolic content, math word problems, as well as problem-solving and proofs.\n",
       "2. Proof-Specific Questions as the concept of a mathematical proof is varied and gives rise to a host of specific questions about proofs, which go beyond the type of questions related to proofs from the previous point. These are advanced questions concerning matters such as: Establishing whether a proof is effective or not, in the sense of proving a general statement about a mathematical object merely by proving its existence as opposed to producing a witness or an algorithm that computes; investigating what distinct proof strategies might be used to prove a certain statement; exploring the distinction between whether an 'elementary' or an 'advanced' proof is given. For example, the prime number\n",
       "\n",
       "theorem has classical complex-analytic proofs (via properties of the Riemann zeta function) [Hadamard, 1896] as well as later 'elementary' proofs [Selberg, 1949, Erdős, 1949].\n",
       "\n",
       "3. Pictorial representation. Many mathematical objects and arguments have standard diagrammatic encodings that support reasoning and communication. Examples include plotting functions to inspect qualitative behavior (monotonicity, convexity, asymptotics), diagram-based proofs and explanations (for example, 'proofs without words' collections [Nelsen, 1993]), and commutative or string diagrams used in areas such as algebra, topology, and category theory [Selinger, 2011]. A mathematical copilot that supports research practice should therefore be evaluated on tasks that translate between symbolic or textual descriptions and graphical representations, and on tasks that interpret diagrams to extract relevant properties or propose next steps in an argument.\n",
       "4. Mathematical explanations and intuition , e.g., by providing (counter-)examples. Such questions are open-ended and have potentially (infinitely) many correct answers. This freeform exploration is crucial in the process of establishing the truth value of a given (advanced) mathematical statement, where one swings between attempting a direct proof of the statement and finding a counterexample to the statement. Terence Tao has recounted encountering a counterintuitive statement in compressed sensing, first attempting to construct a counterexample, and then redirecting that effort into a proof [Bernstein, 2019].\n",
       "\n",
       "One recent counterexample-focused benchmark is CounterMATH, which asks models to refute undergraduatelevel statements by producing explicit counterexamples (1,216 statement-rationale pairs) [Li et al., 2025a].\n",
       "\n",
       "5. Proficiency in using mathematical tools , where the assistant is given access to external tools such as computer algebra systems (SageMath, Mathematica, Maple), numerical solvers, and domain-specific packages (for example, Singular, GAP, SnapPy), and is evaluated on four steps: selecting an appropriate tool, translating the mathematical intent into the tool's input language, validating and interpreting the tool's output, and composing multiple tool calls into a coherent mathematical argument. Tool-integrated reasoning approaches already use this pattern in restricted form (for example, delegating substeps to executable code) [Gou et al., 2024]. This also clarifies why Lean alone does not cover the full workflow of research mathematics: Lean can verify formal proofs, but many routine sub-tasks (symbolic simplification, numeric exploration, combinatorial search, heuristic experimentation) are more naturally handled by specialized systems. A practical copilot therefore needs a federation of tools, each with its own interface and notion of correctness, and datasets should record tool traces so that evaluation can replay the same sequence of calls.\n",
       "6. Retrieval Tasks for definitions and mathematical facts, either from name to statement or reverse ('mathematical search engine'). MIRB provides a benchmark suite for mathematical retrieval, including premise retrieval and formula retrieval, which can serve as components of literature-search workflows [Ju and Dong, 2025]. Retrieval tasks can range from a well-defined answer (e.g., ' Define this mathematical object! ') to open-ended questions (' what are the most important theorems in a certain field?; which results are relevant to prove a certain statement?; has a form of this statement already been proved? '). This tests the capabilities of mathematical assistants to provide high-level explanations similar to those of a human expert in a certain mathematical field.\n",
       "7. Informal Proof Completion , where a proof (or generally a mathematical statement) needs to be checked or completed. In the best case, the mathematical assistant should be able to provide a correction and fill in gaps. In such functionality, mathematical assistants can be running in the background to act as 'copilots' and 'mathematical grammar-checkers' - as the current generation of copilots 6 does successfully for programmers.\n",
       "8. Mixed-mode interactions , where translation between formal and informal mathematics needs to be\n",
       "\n",
       "6 Such as GitHub's Copilot: https://github.com/features/copilot\n",
       "\n",
       "performed. These are particularly important for the combination of ITPs and general-purpose LLMs, which has recently picked up interest (e.g., [Jiang et al., 2023, First et al., 2023, Frieder et al., 2023c]).\n",
       "\n",
       "To our knowledge, GHOSTS is the only natural-language dataset that focuses on all of the first four items from above. Accompanying this variety of questions ( what is asked) are session types that describe how the questions are posed.\n",
       "\n",
       "## Session Types.\n",
       "\n",
       "1. Zero-Shot , where the model is prompted without further information.\n",
       "2. In-Context Learning , where additional background information is provided, ranging from clarifications of the notation to context (e.g., the chapter of the book or paper), to similar questions with corresponding proof.\n",
       "3. Interactive , where the prompter (whose mathematical abilities can span a wide spectrum) solves mathematical questions together with the language model in an interactive way as in Collins et al. [2024a].\n",
       "\n",
       "In practice, typical mathematical workflows require a mixture of different question and session types. Even if a question is not open-ended, the amount of mathematical detail and rigor the answer should contain will depend on the user's background. In part, this should be deduced from the way the question is phrased. Currently, no datasets offer support for this. We note that with the exception of Collins et al. [2024a] and the collection of CheckMate, almost all datasets are zero-shot. Interactive training modes are extensively employed in a formal language setup. IMProofBench evaluates research-level problems in an agentic setting with tools (web search and computer algebra), combining automated scoring on subproblems with expert grading of full proofs [Schmitt et al., 2025].\n",
       "\n",
       "For formal mathematics, different considerations apply, as outlined below. While modern LLMs can, to a degree, perform the tasks below, they are not specialized for these tasks:\n",
       "\n",
       "- Refactoring formal mathematics to yield nicer-looking proof is a significant undertaking. A tool that takes as input a formal proof and refactors it along desired criteria would be a welcome addition to the toolbox of researchers working in formalizing mathematics. To our knowledge, the only work to date that attempts this is the ImProver [Ahuja et al., 2024] for Lean, which allows formal proof creation of specific length.\n",
       "- Interoperability between different libraries of formal proofs (such as Lean's Mathlib library and Isabelle's AFP) is a long-term (but still largely unachieved 7 ) goal. Currently, it takes intimate knowledge of the various formal libraries in order to assess which library, and ensuing ITP, would best be used to formalize a given mathematical theorem. A tool that can assess the suitability of a certain ITP and its library before starting the formalization process could make formalization more accessible.\n",
       "\n",
       "Recommendations. We advise exploring more diverse mathematical interaction modes - across a variety of question and session types - as outlined here, both for formal as well as informal datasets. Our list is not exhaustive and can also be broken down to a more detailed level. In particular, oral, mathematical communication is not well mapped to data (see Section 4.3). Mathematical collaboration often involves an exchange of very high-level ideas at a rapid pace. Specific, wide-ranging recommendations can be found in Section 4. We believe capturing this in data will help to lead to more widespread mathematical assistants. Finally, we believe that input from research in mathematical education must be taken into account to derive curated datasets that exhibit richer interaction modes.\n",
       "\n",
       "7 A score of overlap by assessing how many theorems out of 100 theorems have been formalized in different ITPs is provided here: https://www.cs.ru.nl/~freek/100/\n",
       "\n",
       "## 3.1.4 Contamination\n",
       "\n",
       "When releasing a dataset, control is lost over whether the dataset ends up as training data for machine learning models. For several of the state-of-the-art models, such as GPT-4 [OpenAI, 2023], that perform well on benchmarks, no information is available in the ensuing on whether efforts were made to ensure that training datasets were decontaminated and no datapoints from the evaluation benchmark were included in the training set. For Gemini 1.5 [Reid et al., 2024], Qwen2 [Yang et al., 2024a] and DeepSeekMath [Shao et al., 2024], only n -gram decontamination approaches have been tested. For complex datapoints, such as math, n -grams are unfortunately not always sufficient to ensure the training dataset is clean. There is evidence that suggests that several such modern models are already contaminated [Xu et al., 2024]. While mitigating approaches have been proposed, such as generating new data, either from scratch [Mishra et al., 2024] or by using existing datasets as seeds [Zhou et al., 2024], it is not clear whether these can scale to more sophisticated mathematics, as the methods were tested using mathematics on the level of grade-school, and it is unclear how well they would scale to much higher levels of mathematical abstractions and problem-solving difficulty (the two main metrics of difficulty, as outlined in 3.1.1.\n",
       "\n",
       "Recommendations. If the dataset is sufficiently large, it is advisable to keep a part of the dataset hidden from the public so that the dataset creators can compare how well newly-released, publicly accessible openweight models score in the public versus the hidden part of the dataset. Diverging scores on two splits of the dataset can indicate contamination. Care must be taken that datapoints from the hidden datasets are from the same distribution compared. For mathematics, this means at minimum: Same domain, difficulty range, and proof technique. This can be challenging: Given a specific problem P , it takes effort to source a new problem P /star , that on all relevant metrics (including the mentioned ones) is similar to P , but at the same time is not completely analogous to P - as it would be by, e.g., merely changing items in the problem statement that have a negligible on the proof (this is an approach followed by GSM8K-Symbolic dataset [Mirzadeh et al., 2024], which nonetheless turned out to be challenging for LLMs, highlighting their current limited reasoning capabilities). The approach advocated here has limitations for models released as-a-service [La Malfa et al., 2024], via APIs or GUIs, as running the hidden dataset through risks it automatically being used at a later stage as training data for those models. RealMath proposes a continuously refreshed benchmark mined from arXiv papers and forum discussions, which reduces the half-life of static public test sets [Zhang et al., 2025b].\n",
       "\n",
       "## 3.1.5 Binned Sankey diagrams\n",
       "\n",
       "Math benchmarks are often reported with aggregate statistics such as accuracy, pass@ k , or majority-vote accuracy. These statistics compress item-level outcomes into a small number of scalars. On mixed benchmarks, an average gain can coincide with regressions on specific slices (topic, modality, or answer format), which the average does not show. When assessing model improvement over time, this information conveys both too little detail and is also hard to read. This issue is not solely applicable to mathematics, but is very prevalent in this case.\n",
       "\n",
       "Recommendation. To support better visualizations, we recommend the use of binned Sankey diagrams. These are item-level transition plots between two evaluations. Each benchmark item is assigned to an outcome bin under a baseline evaluation (for example, model M t ) and to an outcome bin under a second evaluation (model M t +1 ). The diagram draws flows from baseline bins to new bins, with band widths proportional to the number of items making each transition. Sankey-style migration plots have been used in other evaluation contexts; see Fig. 2 in Frieder et al. [2023a].\n",
       "\n",
       "For mathematics, the visualization works best in the following way: Choose two evaluation points and define a small set of outcome bins. The minimal binning is binary ( correct / incorrect ). If annotations exist, bins can reflect error types [Frieder et al., 2023a] (forsexample, algebra error, invalid step, wrong final answer) or partial credit (for example, solved , nearly solved , off-track ). If the evaluation involves tools, bins can separate solved without tools , solved with tools , and failed . The resulting diagram makes\n",
       "\n",
       "regressions explicit. A flow from solved to failed remains visible even when net accuracy increases. For heterogeneous benchmarks, the same transition plot can be computed within slices such as topic, difficulty level, or representation (symbolic, word problem, diagram-based, or formal proof assistant). Slice-level transitions show whether gains are concentrated in a subset of items and whether any slice exhibits systematic regressions.\n",
       "\n",
       "Benchmark reports can include a binned Sankey diagram alongside aggregate metrics when comparing model versions or training regimes. This requires storing per-item outcomes (and, when applicable, slice labels) so that transitions can be reproduced. Using the same bins across releases supports longitudinal comparisons.\n",
       "\n",
       "## 3.2 Distinct Issues\n",
       "\n",
       "This section will detail issues that specifically appear with either natural language or formal language datasets, or mixed datasets. The most prominent difficulty with natural language datasets is the lack of trusted automatic evaluation as will be explained in Section 3.2.1.\n",
       "\n",
       "A natural way to circumvent the difficulty of evaluating natural language proofs is by making use of formal language datasets. Instead of having an LLM generate natural language mathematical proofs, one can train it to generate proofs in a verifiable formal language such as Lean or Isabelle. Therefore, existing libraries of formal proofs can play a crucial role in the development of machine learning-based automatic theorem provers. Sections 3.2.2 and 3.2.3 will provide details on how some of the largest existing collections of formal proofs have been and are being created. This is followed by an analysis of how this process affects the use of formal proof libraries as evaluation datasets; with some remarks relevant to using formal libraries for training. More concretely, Isabelle's Archive of Formal Proofs (AFP) and Lean's Mathlib will be investigated. This choice is due to the authors' familiarity with these libraries but many of the points mentioned below will also be valid (at least to some extent) for other systems. One particular issue relevant to all datasets is their scale. The lack of a large-scale dataset consisting of pairs of formal and informal mathematics is a significant bottleneck for autoformalization. Current datasets, such as miniF2F are on the order of a few hundred datapoints, which are enough for assessing autoformalization, but not for training models to support autoformalization. A recent effort [Ying et al., 2024] scales this to 57k datapoints of pairs. Nonetheless, this scale is not yet comparable to the scale at which LLMs can be trained on informal mathematics.\n",
       "\n",
       "## 3.2.1 Lack of Trusted Automatic Evaluation\n",
       "\n",
       "The easiest way to automate the evaluation of mathematical prompts is to formulate them in a way such that the answer can be represented as a single token (e.g., a number or a mathematical term). Automatic evaluation can then be performed by keyword-matching the token to the gold-truth answer. A slightly more advanced variant is pursued by the MATH dataset, which allows a reasoning section, but encloses the final answer in a \\boxed environment (and only this is keyword-matched).\n",
       "\n",
       "No large machine learning dataset on mathematics currently exists that allows an arbitrary proof of an AI system to be checked for correctness against the gold-standard proof in the evaluation dataset. The Open Proof Corpus collects expert judgments on model-generated natural-language proofs and reports large gaps between boxed-answer accuracy and proof validity [Dekoninck et al., 2025]. A proof-of-concept, highlighted by the IMO Small Challenge 8 , shows how partial automation might be achieved, where a necessary test for correctness is carried out using detailed proof annotation. This builds on the idea that a system like an LLM, combined with a deterministic form of matching, can at least exclude incorrect proof candidates. If the ground truth is sufficiently rich, the annotations help the LLM break down a proof and understand its main features (accepting that subtler points of proofs may still be currently out of reach of LLMs' understanding).\n",
       "\n",
       "Some success has been attained in using an LLM to grade (or teach) another LLM [Eldan and Li, 2023, Mukherjee et al., 2023]. Yet, for mathematics, we believe that this is not necessarily the right approach\n",
       "\n",
       "8 www.imo-small-challenge.io\n",
       "\n",
       "forward. The current generation of models does not have a sufficiently high performance on mathematics to be used as graders: for tasks that would often be deemed simpler than mathematics, this approach can fail [Wang et al., 2023], although for solving programming puzzles this approach was successful [Haluptzok et al., 2023]. Anecdotal evidence suggests that adapting the approach to proofs is challenging because of the multitude of potential proof variations for a single statement, which gives rise to a diverse set of textual expressions. At present, grading can, at best, be automated by the use of detailed human annotation for each problem in combination with LLM assistance based on such annotation.\n",
       "\n",
       "Recommendations. In general, we recommend developing evaluation methodologies that allow for natural language proofs to be checked for correctness. This is difficult in general. A middle ground is to include as much metadata as possible when annotating datasets with manual effort, with a view towards supporting LLMs in using this metadata to assess proof candidates. Second, we recommend more research that quantifies the degree to which LLMs can evaluate mathematics. This will help in making objective statements about how good or how poor LLMs are at evaluating different kinds of mathematics, and help in identifying areas for improvement.\n",
       "\n",
       "## 3.2.2 Tool Misalignment\n",
       "\n",
       "As writing formal proofs imposes slightly different challenges to writing informal proofs, it is important to distinguish mathematical proficiency from proficiency in using tools provided by the formal environment.\n",
       "\n",
       "One cause of this discrepancy is that side conditions deemed trivial in conventional mathematical literature have to be formalized as diligently as the rest of the mathematical text. On the other hand, proving many statements considered routine by mathematicians can be mechanized. For example, proving that a function is continuous often corresponds to choosing the correct subset of a few relevant lemmas and this can be achieved with a simple search algorithm, such as Lean's continuity tactic. Next to this domain-specific formalization aid, there are also general-purpose automation tools that can be used to formalize certain simple statements. For example, Isabelle's Sledgehammer tool [Paulson and Susanto, 2007, Meng and Paulson, 2008] translates the given statement so that it can be understood by a number of external automatic theorem provers (ATPs). The ATPs, in turn, attempt to prove the statement and send the proof back to Isabelle.\n",
       "\n",
       "Automation and Proofs Naturally, the existence of automation affects how the task of formalization is approached: Without automation, formalization would consist of translating a mathematical proof step-bystep into the ITP's logic. But having access to advanced tactics and general-purpose ATPs, one only needs to repeatedly break down the proof into smaller pieces until these can be tackled by automation.\n",
       "\n",
       "Eventually, this can lead to proofs being formalized in a way that is not very close to the original natural language proof. For example, elementary homework-style problems often consist of long calculations that might correspond to one single tactic invocation in the formal system. Also, in more advanced topics, the automation of a theorem prover can make the proof less readable or skew its focus. For example, natural language proofs typically make it very clear which definitions need to be unfolded and when. In Lean, such unfolding does not have to be made explicit, which can widen the gap between formal and informal.\n",
       "\n",
       "The impact of these mechanisms needs to be kept in mind when formal datasets such as Lean's Mathlib or Isabelle's AFP are used for LLM evaluation. In particular, it is likely that current datasets overly reward models for their ability to deploy the language-specific automation, rather than complete 'understanding\" of the underlying mathematics. As some evidence of this, Hu et al. [2024] show that LLM performance drops significantly when common automation tactics are turned off, even for theorems that have human-written proofs that do not rely on the automated tactics.\n",
       "\n",
       "Strong automation and the Archive of Formal Proofs (AFP) These matters are especially significant with the Isabelle AFP because of the powerful general-purpose automation that has been widely used in its development over the last decade. It can therefore be expected that a large subset of theorems in today's\n",
       "\n",
       "AFP have been formalized by repeatedly breaking them down into smaller subproblems until Sledgehammer could find their proof.\n",
       "\n",
       "A natural way to turn the AFP into a proof dataset is by splitting up all formal proofs into their individual steps [Jiang et al., 2021]. However, by the above characterization, such a dataset might be skewed towards being tractable for automation because of the particular way in which AFP theorems have come into existence. This can make it hard to assess the level of difficulty in the problem set.\n",
       "\n",
       "Especially when equipping LLMs with the ability to access existing automation, careful evaluation is essential. It is important to determine whether the model genuinely 'understands' complex mathematical concepts and can scale this understanding up to more difficult problems; the alternative being that the model merely excels at finding 'low-hanging fruits', which, when combined with non-ML automation, might appear deceptively impressive.\n",
       "\n",
       "We would like to stress that we believe that the use of formal mathematics datasets such as the AFP is a valid evaluation tool. The results of recent publications [First et al., 2023, Jiang et al., 2022, Mikuła et al., 2024] are especially promising, and we are looking forward to their continuation. Nevertheless, we believe that future evaluation should carefully consider the impact of existing automation in ITPs on their libraries.\n",
       "\n",
       "Recommendations. The available formal mathematical libraries that include deep, research-level results have generally been created using a significant amount of automation. Researchers could create small, specific additional test sets of proofs that are formalized without automation and evaluate mathematical assistants on these.\n",
       "\n",
       "## 3.2.3 Non-Trivial Data Duplication\n",
       "\n",
       "Next to these aspects related to the nature of theorem provers, there are also challenges intrinsic to mathematics itself. In particular, mathematics has a self-similar structure: there are profound connections between areas that are seemingly disconnected, and many mathematical statements have several related versions, depending on the perspective and level of generality chosen. This section will detail some of the inherent challenges this creates for the construction of formal mathematical libraries and then consider the impact of using them as datasets.\n",
       "\n",
       "Ideally, a formal library should have as little duplication as possible to allow for a smoother user experience and to reduce the amount of code maintenance required. At the same time, this ideal can seldom be upheld fully since formalizing theorems only in their most general form is intractable. Some level of non-trivial duplication is therefore accepted and common in formalization.\n",
       "\n",
       "As an example, Lean's Mathlib often provides several versions of a lemma, 9 which makes it more convenient to use the library. Instead of having to 'import' a lemma and then transform it into the right format, one can directly refer to commonly used variants. For example, the statement that\n",
       "\n",
       "<!-- formula-not-decoded -->\n",
       "\n",
       "in a commutative semigroup is explicitly formalized even though it could easily be derived from associativity and commutativity whenever needed. Furthermore, there is support for automatically generating lemmas, e.g., to generate the additive version of an abstract algebra statement given in terms of multiplication.\n",
       "\n",
       "Next to this, the rules and customs of how a collection of formal proofs is built and maintained have a significant impact on its properties as a machine-learning dataset. Compared with Mathlib, the Isabelle AFP is more static and aims at being archival, i.e., providing long-lasting support for its content and not removing previously defined concepts. This makes duplication inevitable as soon as parts of the library go through a larger restructuring or even redevelopment. For example, there are two separate algebra libraries for Isabelle,\n",
       "\n",
       "9 We note that this facility is not exclusive to Lean; e.g., Isabelle provides several versions of a lemma.\n",
       "\n",
       "one in the AFP and a second one using a different formalization approach in the Isabelle HOL-Library, which often gets used in conjunction with the AFP.\n",
       "\n",
       "As a further example, consider the AFP section on category theory. Currently, there are at least five separate formalizations of category theory at various stages of development with all of them taking different approaches 10 . This is likely to be for historical reasons, but also because exploring different representations of category theory is an interesting topic in mathematical foundations.\n",
       "\n",
       "Naturally, if duplication is present in the formal library, a lot of care becomes necessary when designing machine-learning datasets from it. If data is just randomly split into a training and validation set, then leakage can occur on one hand. Of course, the duplication will typically be on the level of mathematical concepts and not exist as a verbatim doubling of formal language code. Nevertheless, disregarding the issue of duplication makes it hard to gauge to what extent a model manages to produce formal proofs from the ground up. Possibly, parts of the model's performance simply stem from learning how to translate between different representations of the same mathematical concept. On the other hand, duplication brings to the forefront the issue of data representation, as machine learning models will only perform well on that representation on which they have been trained, which is in contrast to human mathematical reasoning, which, to a degree, is robust against different formal representations of the same mathematical objects.\n",
       "\n",
       "Translation of formal representations constitutes a highly interesting and relevant skill in its own right. Nevertheless, good test design should enable researchers to judge to which degree the respective skills are achieved.\n",
       "\n",
       "Recommendations. Following the discussion above, we recommend developing methodologies that ensure that train/test splits adequately measure a model's ability to generalize. One direction is to test on new formalizations that occur after a model has been trained, as was explored in Hu et al. [2024]. Nevertheless, care should be taken to ensure that the mathematics being formalized does not occur in previous projects. Hence, we particularly recommend evaluating on domains that have not previously been formalized.\n",
       "\n",
       "More broadly, the ideal evaluation of digital mathematical assistants should include case studies in which previously unformalized proofs get formalized using the assistant. There are various ways to realize this. On a small scale, researchers can qualitatively evaluate the LLM by formalizing some reasonably difficult, previously unformalized proof. On a larger scale, the assistant can be provided as a plugin to theorem provers, which a large number of people working in formalization can then use in real-world scenarios. If there are enough users, such a setup could even use A/B testing for multiple architectures of mathematical assistants.\n",
       "\n",
       "## 3.2.4 Version Fragility and Tactic Stability\n",
       "\n",
       "Formal proof corpora are often treated as static datasets consisting of a theorem statement plus a proof script. In practice, they are tightly coupled to a moving toolchain (involving a specific version of the prover, a particular commit of the library, and details of automation implied by this combination). This can make them fairly brittle as datasets: even small library changes can invalidate proofs or silently change what intermediate tactics do.\n",
       "\n",
       "A common failure mode is 'tactic instability', where a proof uses a flexible tactic (in Lean, most notably simp ) and then applies a subsequent tactic whose success depends on the exact transformed goal. Because the set of parameters used by the automated tactic (e.g., lemmas used by simp ) evolves, the transformed goal may drift over time, making the downstream step brittle even if the overall theorem statement is unchanged.\n",
       "\n",
       "10 cf. https://www.isa-afp.org/topics/mathematics/category-theory/ not counting generalizations of category theory and continuations of previous developments.\n",
       "\n",
       "Recommendations. Recent work in Mathlib 11 introduces a stability linter ('FlexibleLinter') that flags precisely these patterns, encouraging authors to make tactic behavior more explicit (e.g., by specifying a minimal set of lemmas used in the simplification). As a dataset design principle, proofs that pass such stability linting are better candidates for long-lived benchmarks, since their execution is less sensitive to future library drift.\n",
       "\n",
       "## 3.2.5 Hidden Context Dependence: Local Instances and Attribute State\n",
       "\n",
       "Even in a fully formal setting, a dataset item is seldom determined by the goal alone. In Lean, imports, local instances, and attribute state can change the meaning of a line of proof text. This matters for dataset construction because 'the same' proof script can behave differently when replayed under a different environment.\n",
       "\n",
       "Example (instance 'pollution'). In measure-theoretic developments in Lean, the proposition MeasurableSet s is parameterized by an implicit measurable-space instance. If a proof introduces a local measurable space m0 but an ambient instance mW remains in scope, MeasurableSet may silently refer to mW unless the instance is pinned:\n",
       "\n",
       "```\n",
       "--Fails: 'MeasurableSet' uses the ambient instance, not the local one have h_bad : MeasurableSet s := by simpa using hs --Works: pin the intended instance explicitly have h_ok : @MeasurableSet alpha m0 s := by simpa [m0] using hs\n",
       "```\n",
       "\n",
       "This illustrate that storing only the final proof term (or only the local goal) in a dataset can be insufficient to replay a proof deterministically, because replay depends on the surrounding instance environment and imported modules. For more examples from various areas of math, see the reference patterns contained in the Claude Code skill for Lean 4 [Freer, 2025].\n",
       "\n",
       "Example (context-dependent simplification). Automation can also be context-sensitive in a more mundane way. Automated tactics such as an instance of simp in Lean can succeed or fail depending on hypotheses introduced earlier in the proof:\n",
       "\n",
       "```\n",
       "--Works with extra context in scope filter_upwards [hw, hu_eps, hv_eps] with w hu hv simp [Real.norm_eq_abs] --May fail without that context, even if the syntactic goal looks similar simp [Real.norm_eq_abs]\n",
       "```\n",
       "\n",
       "This suggests that proof snippets that are extracted without their surrounding context (imports, local hypotheses, attributes) can be underspecified as dataset items for training or evaluation.\n",
       "\n",
       "## 3.2.6 Resource Sensitivity: Heartbeats and Elaboration as Part of the Task\n",
       "\n",
       "In interactive theorem provers, 'correctness' is not clearly delineated from performance. A proof that compiles only under extremely permissive resource limits (such as 'heartbeats', recursion depth, or wall-clock time) is often not usable in real formalization workflows, and may not be reproducible across environments (especially if the high resource bounds are not clearly specified).\n",
       "\n",
       "11 See https://leanprover-community.github.io/mathlib4\\_docs/Mathlib/Tactic/Linter/FlexibleLinter.html for details.\n",
       "\n",
       "Lean exposes heartbeat and other resource limits directly:\n",
       "\n",
       "```\n",
       "set_option maxHeartbeats 40000 set_option synthInstance.maxHeartbeats 40000 set_option maxRecDepth 2000\n",
       "```\n",
       "\n",
       "These resource bounds can be exceeded by complex elaborations or typeclass searches. Sometimes this is due to the complexity of the underlying mathematics, but other times it is from 'fighting the elaborator'. For instance, leaving instance arguments implicit can trigger expensive unification/typeclass search, sometimes consuming hundreds of thousands of heartbeats before timing out. This suggests that datasets and benchmarks should treat resource budgets as first-class metadata, and report resource usage alongside whether the code passed or failed.\n",
       "\n",
       "## 3.2.7 Redundant Scaffolding and Accidental Complexity in Proof Scripts\n",
       "\n",
       "Formal corpora often contain a lot of proof-script structure that is not mathematically essential, including redundant tactic steps, arguments to automation that are not used, and lemma variants that exist only to satisfy local engineering constraints. As dataset artifacts, these patterns can dominate the training signal while teaching models brittle or unidiomatic behavior.\n",
       "\n",
       "Example (no-op tactic after automation). A common failure mode when replaying mined proof scripts is that one tactic already closes the goal, making subsequent steps ill-typed. For example, in Lean, this sequence can fail:\n",
       "\n",
       "```\n",
       "simp -- closes the goal aesop --error: no goals to be solved\n",
       "```\n",
       "\n",
       "Example (lintable redundancy in automation arguments). Even when scripts compile, they often include extra automation arguments that are unused in practice. Mathlib-style linting can flag such redundancy and enable construction of a cleaned corpus:\n",
       "\n",
       "```\n",
       "--Linter: unused simp arguments (suggest removing to reduce clutter) simp [pow_two, mul_assoc, mul_add, mul_comm, add_comm, add_left_comm]\n",
       "```\n",
       "\n",
       "In light of this, it may be useful for datasets to include both a raw corpus (for realism) and a linter-cleaned corpus (for learning canonical structure).\n",
       "\n",
       "## 4 The Future - Novel Datasets to Support Mathematical Copilots\n",
       "\n",
       "In this section, we cover various aspects of mathematical research practice that, to date, are not covered at all by any of the existing datasets. This contrasts with the previous section on mathematical datasets that covered parts of mathematical practice, albeit with the noted limitations. Current datasets are exclusively focused on publishing the results rather than the intermediate steps that a mathematician goes through in the process of devising a proof. The different types of workflows, the proof-counterexample dialectic, and proof transfer, etc., are all examples of intermediate processes that do not have clear representations in data that can be used to fine-tune LLMs. Contrasting with the previous section, where we made specific recommendations on how to improve existing datasets, in this section, we do not make specific recommendations, as our recommendation is to simply devise such datasets. While a few inroads have been made into non-proof-based datasets, see Section 3.1.3 these are still very much at their inception, and, to our knowledge, the GHOSTS dataset is the only attempt to date to try to 'data-ify' some aspects (solely related to mathematical literature search) of daily mathematical process.\n",
       "\n",
       "An example of proof transfer is given by the Ax-Grothendieck theorem, which states that every injective polynomial map C n → C n is surjective; rather than proving this directly over C , one observes that the statement is trivial for finite fields - any injective map on a finite set is bijective - and then uses model-theoretic compactness to transfer the result to algebraically closed fields of characteristic zero. Here, one does not redo the argument in the new structure directly; instead, one uses model theory to move the truth of a sentence from one place to another.\n",
       "\n",
       "## 4.1 Mapping Mathematical Workflows to Data\n",
       "\n",
       "The previous sections have shown that existing datasets and benchmarks only deal with specific mathematical tasks. In particular, they focus mostly on question answering and theorem proving, with solutions being presented in a streamlined way (as is typical for results in mathematical textbooks). In consequence, they do not sufficiently cover all steps in typical mathematical workflows, such as surveying mathematical topics, gathering related results, establishing high-level proof strategies and intuition, refactoring proofs, or carrying out field-specific routines. Yet, we would like to evaluate and train mathematical copilots on these tasks, too, in order to obtain full-spectrum assistance across all facets of mathematical research practice. We note that datasets used to pre-train LLMs exhibit broader coverage, e.g., of mathematics-related Q&amp;A communities, blogs, and educational material; however, it is hard to assess and control the quality of the included mathematical content. This results in a weak learning signal and, for a strong mathematical copilot, one likely requires fine-tuning on a comprehensive, high-quality dataset of mathematical workflow steps.\n",
       "\n",
       "Simple methods as precursors. Even relatively simple machine learning approaches have produced genuine mathematical discoveries. Wagner [Wagner, 2021] demonstrated that reinforcement learning, framed as a single-player graph-building game, can find counterexamples to open conjectures in extremal combinatorics. PatternBoost [Charton et al., 2024], built on Karpathy's character-level language model makemore [Karpathy, 2022], alternates between local search and transformer-based generation to discover constructions that refuted a 30-year-old conjecture. These methods succeed precisely because they operate in domains where candidate solutions can be automatically evaluated. This is a form of verifiable feedback that sidesteps the hallucination problems plaguing open-ended generation. They are precursors to more general workflows that combine neural generation with symbolic verification and human mathematical insight.\n",
       "\n",
       "We advocate a more bottom-up approach that consists of creating a taxonomy of workflows, i.e., isolating and categorizing workflow steps. Specifically, many mathematical workflows can be modularized into smaller sequential steps, which themselves can be ordered along different dimensions, e.g., their mathematical depth, level of abstraction (from general, high-level proof techniques and principles to specialized, problem-specific approaches), mathematical subject, how well a symbolic approach might handle them, whether the workflow step is strictly mathematical or meta-mathematical (e.g., literature search). With this in view, we distinguish between general global workflows, which are used across mathematical fields, and field-specific local workflows. For instance, we refer to the Tricki 12 for a collection of problem-solving techniques that can be viewed as abstract workflows. General proof techniques, such as linearization and fixed point theory, can be viewed as global workflows. However, we note that specific instantiations of such techniques, e.g., related to dynamical systems, can also be classified as local workflows. Examples of even more local workflows include approaches tailored to specific kinds of partial differential equations, e.g., the method of characteristics, maximum principle, energy estimates, or Green's functions, as these methods frequently occur as individual steps in larger chains of arguments in research-level mathematics.\n",
       "\n",
       "There is no hard boundary for transitioning between local and global workflows. For example, the 'routine' task in analysis to upper bound an expression efficiently, as opposed to evaluating it exactly, is found both in elementary contexts, such as proving inequalities such as versions of arithmetic mean-geometric mean inequality, up to more advanced cases, such as for ordinary differential equations (ODEs) or partial differential equations (PDEs), where a routine task is that of applying an 'energy method' or 'Lyapunov functional\n",
       "\n",
       "12 https://www.tricki.org\n",
       "\n",
       "method'. We, therefore, deliberately do not draw a specific boundary of when to categorize a workflow step as local vs. global.\n",
       "\n",
       "While creating a complete taxonomy of workflow steps is beyond the scope of this article, we want to examine selected workflows and their steps (both local and global) in order to highlight issues in translating these to data and specific challenges. We note that these are merely illustrative examples meant to show how first steps could be undertaken to generate datasets that support such workflows.\n",
       "\n",
       "While a workflow (indicated with arrows below) can, in principle, easily be mapped to data by converting it to n tuples of datapoints (where n denotes the number of workflow steps) and filling text in between, in practice, in certain cases, issues may arise. We give examples of both global and local workflows below and highlight both examples where these can be transformed to data in a straightforward manner, as well as cases in which this transformation is more difficult. We urge the community to explore such translation of workflows to data further. Future work should address how such workflow datasets might be structured, how symbolic or numeric methods can be integrated into individual workflow steps, and which parts of these workflows are best left to humans versus delegated to a mathematical copilot. Once such datasets exist, LLMs can be trained on them using standard methods such as expert iteration [Anthony et al., 2017, Havrilla et al., 2024].\n",
       "\n",
       "Inference-time compute in reinforcement learning needs problem analyses. Several recent math reasoning systems use substantial inference-time compute, for example, by sampling many candidate solutions, running search over intermediate steps, or iterating tool calls, and then using reinforcement learning to train policies that decide which branches to expand. In domains with automatic verification, these loops can rely on a binary reward signal. For most informal mathematical tasks, that signal is too sparse: a model receives feedback only after a long chain of steps, and failure provides little information about where the attempt went off track.\n",
       "\n",
       "One remedy is to include, alongside each problem statement, a structured problem analysis that makes intermediate progress measurable. Concretely, this can include a normalized restatement of the problem, the relevant definitions and constraints, a decomposition into subgoals that a solver would aim to establish, and examples of admissible intermediate claims (or failure modes) that an evaluator can recognize. These annotations can be authored by experts or drafted with model assistance and then checked, but they are rarely present in current math benchmarks. Without this layer, RL methods that spend compute at inference time are pushed toward brute-force sampling on a sparse objective, which does not scale to longer proofs or to tasks whose correctness is not cheaply checkable.\n",
       "\n",
       "Global workflows Literature search is one of the most general workflows across the sciences, having particularities in how it takes place in mathematics. Prior work can be related to the problem a mathematician has at hand in many ways. One might, for instance, want to know what is known about a particular mathematical object at hand. The context of a problem might have many options for what to look for, so the first step in the workflow might be to identify an object that has a good chance of having been seen before. One example is integer sequences: perhaps the solutions to a combinatorial problem are computable in small cases, and it is often useful to know if the resulting sequence has been encountered in the context of other problems (there might be many ways to get a sequence: for instance, we might have to choose variables to fix and then only vary one parameter of the problem). For this particular case, the Online Encyclopedia of Integer Sequences 13 is a well-known, efficient, specialized tool, with many pointers to the literature, and the ability to recognize partial matches (e.g., perhaps one's sequence is coarser than the one from relevant prior work). In case there are no matches, we can try to repeat the process with other candidate objects; in case there are, the challenge turns to assess whether any of the results seem meaningfully related to the current case (and given only a few integers, there are often too many sequences containing them). For instance, the sequence 1 , 4 , 44 appears in path-counting problems in some lattices, or in a few number-theoretic contexts (like products of odd-indexed Lucas numbers), as well as many other contexts, and more context that was not\n",
       "\n",
       "13 https://oeis.org/\n",
       "\n",
       "included during the search might be relevant in identifying which of these sequences might be meaningfully related to the current problem. We could broadly summarize this workflow as:\n",
       "\n",
       "Identify objects to search for → find related work describing the object → assess potential relation to current context\n",
       "\n",
       "We might expect LLMs to help generalize this process beyond what specialized tools, such as the OEIS, are capable of. Most mathematical objects are significantly harder to describe than integer sequences: for instance, one might have a particular topology on a function space that might be unique to the current context, but something of the same 'shape' might have been seen before. The assistant would have to recognize and know how to describe this shape, and identify matches across potential differences in what definitions are implicitly or explicitly used, as well as potential equivalent definitions that might cause a description in the literature to differ on the surface (e.g., mention 'accumulation points' vs 'limit point').\n",
       "\n",
       "Examples of this workflow for training and evaluation might be automatically extractable in a post-hoc fashion from the mathematical literature itself: one might be able to look at how previous work is referenced in existing proofs as a source of examples of relevant previous work (other matches for the same object that are not the paper that was cited are most likely examples of spurious matches).\n",
       "\n",
       "Local workflows We list below examples from several distinct subfields of mathematics (differential equations, knot theory/low-dimensional topology, and number theory).\n",
       "\n",
       "- Suppose, for instance, one has a first-order ODE ∂ t u = F ( u ) with some initial data u (0) = u 0 and wants to know how the solution grows in time. A standard technique is to introduce a key functional E ( u ) of the solution (often something like an 'energy' or 'Lyapunov functional') and then compute the derivative ∂ t E ( u ) using the ODE and the chain rule (for PDEs, one often has to also perform several times integration by parts). Then, one bounds this derivative as best one can. The goal is to reach some differential inequality of Gronwall type, e.g., ∂ t E ( u ) ≤ CE ( u ), so that a Gronwall-type lemma may be applied (but it is not always precisely the Gronwall lemma, but something similar).\n",
       "\n",
       "Summarizing, the workflow is thus of the form:\n",
       "\n",
       "## ODE/PDE → find functional → compute derivative of functional → bound derivative → reach Gronwall-type inequality\n",
       "\n",
       "We note that individual steps of this may be solved using symbolic-numeric methods, which opens the possibility of using a tool-integrated-reasoning (TIR) approach to enable an LLM to discharge these proof steps to symbolic or numeric tools to, e.g., compute derivatives symbolically. Thus, one option to encapsulate these workflow steps in data is by using datapoints whose form is\n",
       "\n",
       "(ODE/PDE, functional, functional derivative, derivative bound, Gronwall-type inequality)\n",
       "\n",
       "with potentially filler text between the raw mathematical objects contained in the datapoint (i.e., the ODE, the functional, etc.). The raw mathematical objects should be symbolically or numerically generated so that custom filler text can be added in between them, and an LLM can observe a range of ways in which to speak and textually connect a fixed tuple of mathematical objects.\n",
       "\n",
       "What one typically wants, as a mathematician, in the case of this workflow, is to try various guesses for the functional. If a TIR approach is used, this data representation of this workflow should specifically offer 'entry points' to make it easy for an LLM to accept arbitrary input at certain places and use tools to automatically run the essential parts of the workflow. An LLM is then much better equipped to run the workflow steps and produce an estimate, either automatically or semi-automatically. One can preliminarily do this already through conversation with current state-of-the-art LLMs, but with a lot of mistakes on the LLM's part, which a stronger grounding in data would help to correct.\n",
       "\n",
       "- A commonly encountered workflow for evolutionary PDEs (i.e., PDEs whose solution evolves with a time variable t ), analogous to the first-order ODE example given above, can be illustrated as follows.\n",
       "\n",
       "One wants to find a functional which will facilitate providing useful information about solutions of the PDE. To illustrate how such functionals are found in practice, we begin by considering the simple example of the linear wave equation ∂ tt u -∆ u = 0 on a bounded domain Ω ⊂ R n , with smooth boundary ∂ Ω, subject to homogeneous Dirichlet boundary conditions u = 0 on ∂ Ω and initial conditions u ( x, 0) = u 0 ( x ), ∂ t u ( x, 0) = u 1 ( x ). (One can also study this problem on an unbounded domain Ω such as, for instance, R n , and impose that solutions must decay in the unbounded directions of Ω.) To show energy conservation, we define the energy functional\n",
       "\n",
       "<!-- formula-not-decoded -->\n",
       "\n",
       "which represents the sum of kinetic energy and potential energy. Differentiating E ( t ) in time under the integral sign, we obtain\n",
       "\n",
       "<!-- formula-not-decoded -->\n",
       "\n",
       "An integration by parts shifts the spatial derivative in the second term, while the boundary contribution vanishes because u and hence ∂ t u are zero on ∂ Ω. By substituting the wave equation ∂ tt u = ∆ u , the expression collapses to zero, showing that dE dt = 0. Thus\n",
       "\n",
       "<!-- formula-not-decoded -->\n",
       "\n",
       "More generally, on any finite time interval [0 , T ], one can often prove an energy bound of the form E ( t ) ≤ C ( T ) E (0) for 0 ≤ t ≤ T , with C ( T ) possibly being a constant function. When C ( T ) = C &gt; 0 is constant, we write\n",
       "\n",
       "<!-- formula-not-decoded -->\n",
       "\n",
       "An inequality like (4) is sometimes referred to as an energy estimate . Notice that the conservative situation (3) satisfies the inequality (4) with C = 1. The energy bound (4) can be proved for certain wave equations with non-constant coefficients and for some non-linearities (often under additional structural or smallness assumptions); it provides useful control on the solution. For example, it can be used to prove the uniqueness of solutions of the wave equation [Evans, 2010, Section 2.4, Theorem 5], as well as finite propagation speed for solutions of the (constant-coefficient) wave equation [Evans, 2010, Section 2.4, Theorem 6], and more generally under standard hyperbolicity assumptions.\n",
       "\n",
       "One can invert the previous perspective on this problem: if one had no prior knowledge of this equation and its properties, how could equations (3) or (4) be derived in the first place? In particular, to obtain an energy identity (and hence an energy estimate) for this model problem, one takes the equation ∂ tt u -∆ u = 0 and multiplies both sides of it by ∂ t u , and then integrates by parts on [0 , T ] × Ω, where T &gt; 0. (The exact choice of this multiplier depends on the equation at hand; as we will see in the next paragraph, u turns out to be a good multiplier for the heat equation. Moreover, multiple good choices of multipliers are usually known to exist for each equation, with different multipliers usually leading to different and useful inequalities, among which (4) can be viewed as one of the simplest examples.) Assuming that either u = 0 on ∂ Ω (Dirichlet, hence ∂ t u = 0 on ∂ Ω) or ∂ ν u = 0 on ∂ Ω (Neumann), after a few integrations by parts, one obtains exactly that the energy functional defined in (2) is conserved: E ( t ) = E (0) for all t ≥ 0. It turns out that for many more general wave equations of the form Pu = F (where P is a wave-type differential operator, e.g. ∂ tt -∆ with variable coefficients and lower-order terms, and F is a forcing term), the same choice of multiplier ∂ t u (meaning we multiply the equation by ∂ t u and integrate by parts) yields an energy identity or inequality. Under additional structural assumptions, this can be turned into a uniform-in-time bound of the form (4). In practice, finding good multiplier functions and performing integration by parts in the resulting manipulations is one of the standard ways new inequalities are discovered.\n",
       "\n",
       "Let us modify the wave equation argument slightly to obtain an analogous result for the linear heat equation ∂ t u -∆ u = 0. In this case, as we will now see, one obtains\n",
       "\n",
       "<!-- formula-not-decoded -->\n",
       "\n",
       "where E ( t ) is defined to be E ( t ) = 1 2 ∫ Ω u 2 dx ; notice that this functional E differs from the functional we used for the wave equation. As in the wave equation case, the inequality (5) can be discovered by multiplying both sides of the heat equation by an appropriate function (here, u ), and then integrating on the domain [0 , T ] × Ω. After a few uses of integration by parts, if u = 0 on ∂ Ω (Dirichlet) or ∂ ν u = 0, then one obtains, for 0 ≤ t ≤ T ,\n",
       "\n",
       "<!-- formula-not-decoded -->\n",
       "\n",
       "Since ∫ t 0 ‖∇ u ( t, · ) ‖ 2 L 2 (Ω) dt ≥ 0, one concludes (5). The inequality (5) can be used to prove the uniqueness of solutions to the heat equation; one can find in [Evans, 2010, Section 2.3, Theorem 10] a proof of uniqueness of solutions forward in time, and [Evans, 2010, Section 2.3, Theorem 11] proves a backward uniqueness property. (Unlike the wave equation, the notions of uniqueness forward and backwards in time do not 'coincide' immediately for the heat equation.) The idea of manipulating a useful functional such as E ( t ) goes by the name of 'energy methods' in the PDE literature. The 'energy method' for the heat equation serves as a template in the following sense: many more difficult non-linear PDEs (of which the Navier-Stokes system is one example) admit a version of this 'multiply by the unknown and integrate' trick, and understanding it in the heat equation model helps one to understand those.\n",
       "\n",
       "The main insight in these 'energy method' arguments for both equations was the introduction of the energy functional E ( t ). We saw that for both the heat and wave equations, this functional can be derived by multiplying the equation with an appropriately chosen multiplier-specifically, u for the heat equation and ∂ t u for the wave equation-and then integrating by parts over the domain [0 , T ] × Ω, with T &gt; 0. A mathematical copilot might successfully identify useful and new energy functionals by exploring various chosen multipliers and then attempting to derive useful quantities that can be brought under control by manipulating the resulting integrals. Newly discovered functionals which can be controlled may provide valuable new advances in mathematicians' understanding of solutions to the corresponding equation(s).\n",
       "\n",
       "We now consider typical workflows in low-dimensional topology, which is an area of mathematics that has a unique flavor and uses tools from geometry, algebra, PDEs, group theory, combinatorics, and mathematical physics. Hence, some of these workflows are more challenging to convert to data. Low-dimensional topology is the study of n -manifolds (certain topological spaces that are locally homeomorphic to R n ) of dimension n ≤ 4. A knot is a simple closed curve embedded in R 3 . Knot theory plays a fundamental role in low-dimensional topology as every 3- and 4-manifold can be represented by a framed link; i.e., a collection of knots labeled by integers, called a Kirby diagram [Kirby, 1978]. We refer the reader to Juhász [2023] for more detail on low-dimensional topology and knot theory and explanations of the mathematical terms used below.\n",
       "\n",
       "- A central type of question in this area is classification, which requires being able to show whether two objects (e.g., knots or manifolds) are equivalent. The first step is finding a suitable representation. In case of knots, this could be a projection to the plane, called a knot diagram, a closure of a braid (a number of parallel strands running around a central axis), or a grid diagram. Knot diagrams can be encoded numerically as PD, DT, or Gauss codes, and braids as braid words. Manifolds can be represented using Kirby diagrams, as triangulations, branched covers along links, or as geometric objects (e.g., hyperbolic 3-manifolds).\n",
       "\n",
       "To show that two representations of the same knot or manifold are equivalent, one finds a sequence of certain moves connecting them, which can be Reidemeister moves [Reidemeister, 1927] in case of knot diagrams, Markov moves for braids, or Kirby moves in case of Kirby diagrams. These search\n",
       "\n",
       "problems lend themselves to techniques such as reinforcement learning; see [Gukov et al., 2021]. The workflow steps can be encoded by a sequence of representations of the object, such that consecutive representations are related by one of the standard moves. Software packages such as SnapPy [Culler et al., 2024] can be used to check whether these moves are valid and can also list valid moves. We can hence represent the workflow as follows:\n",
       "\n",
       "choose type of representation and set of moves → find representations of two mathematical objects → repeatedly apply moves to first object → representation of second object.\n",
       "\n",
       "- To show two objects are inequivalent, one defines invariants, which are typically algebraic objects (numbers, polynomials, or groups) that are unchanged by the above moves. These moves often also capture important topological properties of these objects. The most classical invariants are homology groups and the fundamental group. The Alexander polynomial is a knot invariant derived from the fundamental group of the knot complement. Representations of the knot group give rise to the more modern twisted Alexander polynomials. A recent knot invariant rooted in representation theory is Khovanov homology [Khovanov, 2000]. Floer homology [Ozsváth and Szabó, 2004] and gauge theory [Witten, 1994] give rise to highly sophisticated knot and 3- and 4-manifold invariants. These invariants often give lower bounds on hard-to-compute topological quantities such as the 3- or 4-genus of a knot. SnapPy is capable of computing many of these knot invariants when run in SageMath [The Sage Developers, 2024]. A typical workflow would thus be as follows:\n",
       "\n",
       "pair of mathematical objects → choose suitable invariant → compute invariant for objects → show invariants are inequivalent.\n",
       "\n",
       "- To define a Floer-theoretic invariant, we construct a chain complex whose boundary map counts some pseudo-holomorphic curves with certain Lagrangian boundary conditions in some symplectic manifold. To show these counts are finite, one has to prove transversality results for the moduli spaces using difficult methods from PDEs, then obtain a formula for the dimension of the moduli spaces and compactify the moduli spaces. The next step is showing the boundary map squares to zero, in which case we have a chain complex, or sometimes we end up with a more complicated algebraic structure, such as an A ∞ -module or a differential graded algebra. One often has to use special coefficient systems, such as a Novikov ring, and deal with bubbling phenomena. There are many choices that go into the construction of our chain complexes, so one has to prove independence of these choices up to chain homotopy equivalence, which often relies on continuation maps and pseudo-holomorphic polygon counts. One then extracts more tractable invariants from the chain homotopy type using algebra, such as taking homology. Different invariants are often related by spectral sequences, defined using filtrations of the chain complexes. A similar workflow exists for gauge-theoretic invariants. More sophisticated invariants can be obtained by exploiting certain symmetries, such as Z 2 or Pin(2). In another popular direction, one can sometimes construct a homotopy type from chain complexes; see [Lipshitz and Sarkar, 2018]. Certain constructions from algebraic topology can then be invoked to provide additional algebraic structure, such as Steenrod operations. We can hence represent the high-level workflow for defining a Floer-theoretic topological invariant as follows:\n",
       "- associate a symplectic manifold and Lagrangian submanifolds to our topological object → decide what pseudo-holomorphic curves to count → prove transversality of the moduli spaces → find dimension formula for moduli spaces → compactify moduli spaces → choose coefficients and the right algebraic structure → extract invariant using algebra → prove independence of choices.\n",
       "\n",
       "It is apparent from the complexity of the workflow that this is very difficult to turn into data. As the number of workflow steps increase, the number of datapoints has to increase exponentially to cover all possible combinations between steps (although not all combinations may be possible in all cases, which may mitigate this issue).\n",
       "\n",
       "- After an invariant is defined, we study what topological information it captures and derive applications. Here, it is important to know what applications are mathematically relevant. This workflow is very\n",
       "\n",
       "different from many other areas of mathematics, where tools are developed to tackle specific open problems, and should be compared with Section 4.4 on motivated proofs. This step requires sophisticated intuition and a global vision. One way to turn this into data is to define pairs\n",
       "\n",
       "(invariant, topological application) , which could be used by a mathematical copilot to recommend potential applications of a given type of invariant. We want to choose applications that are related to problems from a list of important open questions.\n",
       "\n",
       "- As many of the invariants are hard to compute, it is crucial to develop methods to compute them. For Floer-theoretic 3- and 4-manifold invariants, for example, experts use surgery formulas, spectral sequences, grading arguments, exact triangles, gluing formulas, or computations in explicit diagrams in simpler cases. The difficulty lies in the fact that one has to solve non-linear PDEs to obtain the pseudo-holomorphic curve counts contributing to the boundary maps in the chain complex, which is not algorithmic. Knot Floer homology now has a combinatorial definition and can be computed by SnapPy. It is often helpful to restrict attention to special classes of objects, such as to the class of alternating knots, 3-braid closures, etc., and perform computations or prove results for these. A schematic of this workflow is the following:\n",
       "\n",
       "invariant → method of computation → class of objects to restrict to .\n",
       "\n",
       "Hence, a potential data point could look like\n",
       "\n",
       "(invariant, method of computation, class of objects) .\n",
       "\n",
       "Certain knot invariants are easy to define, but no algorithm is known to compute them, such as the unknotting number or the 4-ball genus, and topologists use computable invariants, many of them arising from knot Floer homology, to give lower bounds on these. The paper [Davies et al., 2021] describes a workflow for using supervised learning to find correlations between various invariants, which has led to an inequality relating the knot signature and hyperbolic invariants. Upper bounds can be obtained by performing certain moves on knot diagrams, possibly with the help of reinforcement learning or Bayesian optimization; see [Gukov et al., 2023] and [Applebaum et al., 2024]. For example, one could take (4-ball genus, lower bound from Rasmussen s -invariant, torus knots) , which leads to a solution of Milnor's conjecture on the 4-ball genus of torus knots [Rasmussen, 2010].\n",
       "\n",
       "- The classification of smooth 4-manifolds is still wide open. There exist 4-manifolds that are homeomorphic but not diffeomorphic, which are called exotic pairs. To construct these, experts use methods from algebraic and symplectic geometry, such as blow-ups, fiber sums when given Lefschetz fibrations, and other gluings, typically along 3-tori, such as knot surgery. To show two 4-manifolds are homeomorphic, one shows they have isomorphic intersection forms and invokes Freedman's theorem [Freedman, 1982], for which the fundamental group has to satisfy certain restrictions (e.g., being trivial). The intersection form and a presentation of the fundamental group can be read off a Kirby diagram. This fundamental group computation is often the most difficult step, and note that any finitely presented group can arise as the fundamental group of a closed 4-manifold. Furthermore, there is no algorithm to decide whether a finitely presented group is trivial. So, some questions in low-dimensional topology are beyond the reach of computers. As mentioned above, one can use Kirby calculus to show if two 4-manifolds are diffeomorphic. Swenton's Knot-Like Objects software 14 is capable of performing Kirby moves. To show they are not diffeomorphic, one usually distinguishes them using the gauge-theoretic Seiberg-Witten invariants. No algorithm is known to compute these, but they can be computed in some instances using Taubes' non-vanishing result for symplectic 4-manifolds [Taubes, 1994], together with various glueing results, such as Fintushel and Stern's knot surgery formula [Fintushel and Stern, 1998]. A schematic of a typical such workflow in smooth 4-manifold topology is as follows:\n",
       "\n",
       "14 https://community.middlebury.edu/~mathanimations/klo/\n",
       "\n",
       "pair of smooth 4-manifolds → find their Kirby diagrams → show they are simply-connected → show they have isomorphic intersection forms (hence homeomorphic by Freedman) → try to show they are diffeomorphic using Kirby calculus; if this fails → compute Seiberg-Witten invariants to show not diffeomorphic.\n",
       "\n",
       "The last step, the computation of Seiberg-Witten invariants, is discussed in more detail in the previous bullet point. Computer algebra systems can be useful for the computation of the fundamental group and the intersection form.\n",
       "\n",
       "3-manifold topology has a completely different flavour. Here, methods from geometric group theory, hyperbolic geometry, and combinatorial topology dominate. These combinatorial methods include triangulations and normal surface theory. Geometric group theory studies groups using their actions on metric spaces (e.g., on the Cayley graph). Also note Perelman's proof of the 3-dimensional Poincaré conjecture using the Ricci flow, rooted purely in geometric analysis; see [Morgan and Tian, 2007]. Well-developed computer packages exist to aid 3-manifold topologists, including the already mentioned SnapPy for hyperbolic 3-manifolds and Regina [Burton, 2004] for triangulations. We do not provide specific workflows in 3-manifold topology due to the diversity of techniques used.\n",
       "\n",
       "Having discussed differential equations and low dimensional topology, we now turn to the Langlands philosophy, a far-reaching series of conjectures relating the continuous to the discrete. The story starts with Gauss' law of quadratic reciprocity, a striking result explaining a rather counterintuitive relationship between two apparently unrelated questions; given two odd primes p and q , there is an unexpected symmetry relating the questions of whether p is a square modulo q , and whether q is a square modulo p . Gauss' theorem can be thought of as one of the first nontrivial results in arithmetic. In the 1800s Eisenstein extended the result to higher powers, and ultimately the results were recast as class field theory, one of the highlights of 19th century mathematics. The advent of cohomology (a tool initially introduced to study questions in topology) after the war clarified things further, and by the 1950s the main proofs (now in the form of Artin reciprocity) were well-understood, although they even now remain frustratingly long. For example the 1968 book by Cassels and Froehlich Cassels et al. [1976] which contains the proofs of the main theorems is over 300 pages long, and the proofs have not become substantially shorter since.\n",
       "\n",
       "The bombshell dropped by Langlands in the 1960s was to observe that class field theory itself was merely the 1-dimensional case of a far more profound nonabelian reciprocity law relating objects from harmonic analysis (the continuous) and objects from arithmetic (the discrete). In short, there seemed to be a collection of analytic objects ('automorphic representations') each of which was related to a corresponding algebraic object (a 'Galois representation'). Rest assured that the details of the (highly technical) definitions are irrelevant to the story being told here; the key point is that automorphic representations are objects coming from harmonic analysis, and Galois representations are objects coming from arithmetic, and these are (on the face of it) two very different areas of mathematics.\n",
       "\n",
       "Whilst the initial idea was compelling, it took some time before the depths of Langlands' insights could be turned into rigorous mathematical statements , with early attempts involving data whose existence was only conjectural (for example the global Langlands group of a number field, an object whose definition is still unclear). On the other hand, a wealth of evidence accrued very quickly, making it absolutely clear that beneath the 'philosophy' there were some beautiful mathematical statements waiting to be proved. In the 60 years since Langlands' ideas were first enunciated, there have been generalizations to other domains of mathematics; there are local and global Langlands conjectures, a p -adic Langlands philosophy, a mod p philosophy (both of which are still wide-open and whose statements are still not completely understood), and geometric Langlands conjectures, where our understanding is much better and there is even a recent announcement of a proof in many cases, which was picked up by the media, although the papers have not yet been published.\n",
       "\n",
       "Let us stick to the classical local and global conjectures, which already raise plenty of questions. What do the workflows look like in this area?\n",
       "\n",
       "- There is the fight to understand the question. Part of the subtlety of this part of mathematics is that whilst there is a general idea that certain automorphic representations (objects from analysis) should be in correspondence with certain Galois representations (objects from arithmetic), it is still not clear how to uniquely pin down such a correspondence (for general reductive groups over number fields, say). One can write down a list of criteria which the correspondence should satisfy, but what mathematicians really seek is some kind of constructive or 'canonical' construction of an object on one side of the conjectured correspondence, given an object on the other side. Without this definition, it is impossible to formally state the theorems we would like to prove. In particular, the main goal here is not to compute a value or to prove a theorem, it is to construct a function.\n",
       "- However there are plenty of instances of rigorous mathematical conjectures which we can write down and for which we have ample evidence in the form of both numerical examples and partial results. Here is a conjecture which can be made completely precise. Let n be a positive integer. Given an algebraic automorphic representation for the group GL ( n ) over a number field, one expects to be able to associate to it a compatible family of n -dimensional p -adic Galois representations of this number field, and conversely given the family of Galois representations one expects the existence of an automorphic representation. This statement is fully proved in the case n = 1 by class field theory. What little we do know for n &gt; 1 is for the most part restricted to totally real and CM number fields (which together form 0 percent of all number fields, one fact indicating the depths of our ignorance) and what litle we do know has been very hard-won. The key difficulty is to find some kind of bridge taking us between the discrete and the continuous, and there are limited tools available. Certain automorphic representations can be studied using differential geometry. Serre's work linked differential geometry and algebraic geometry in the 1950s; Grothendieck's revolution of algebraic geometry in the 1960s gave links to arithmetic, and thus provided the first viable bridge in the case of n = 2. Algebraic geometry was the source of the first general constructions of elliptic curves from modular forms, provided by Eichler and Shimura at the end of that decade. This work was simultaneously a milestone achievement and a miniscule special case of the Langlands program for GL (2) over the rationals. We had to wait for 25 years before Wiles and Taylor managed to go from an elliptic curve back to a modular form; Fermat's Last Theorem was a consequence. In particular a major problem in number theory was resolved by proving an extremely small part of Langlands' conjectures. One of the major difficulties here is that the arguments need nontrivial inputs from arithmetic, algebra, geometry and analysis, and would take thousands of pages to explain if written out in full. These thousands of pages establish just a very small special case of the Langlands program for GL (2) over the rationals. In some sense it does not bode well that the GL (1) proofs were hundreds of pages long and even a small fragment of the GL (2) proofs are thousands of pages. Within these pages there are of course several profound ideas; many more seem to be needed. There are few living humans who have read and completely understood the full proofs of the modularity theorem linking elliptic curves and modular forms; on the other hand it is certainly possible to understand a high-level overview, and most people take on parts of the argument as 'black boxes' where the statements of the results are comprehensible but the proofs might involve techniques which are not part of their expertise. In particular the workflow here does not involve understanding all the details of everything; communication between mathematicians with different expertise is key here, as well as taking results from other areas of mathematics on trust.\n",
       "- The success of Wiles' program showed in particular that algebraic geometry gave a viable route to cross the chasm between the discrete and the continuous and link objects on both sides. However the techniques will only apply when certain quite restrictive conditions hold. Another tool in the area is 'Langlands functoriality', a tool for producing new analytic objects from old ones based on predictions on the arithmetic side of the picture. If the Langlands conjectures are true then basic constructions in arithmetic should have analogues on the analytic side, and one can independently try and make those constructions. Another motivation for making such constructions is that one might start with an analytic object for which the algebro-geometric tools do not apply, and then attempt to massage it until they do apply. Many of the results known for GL ( n ) for n &gt; 2 are proved in this way; the group GL ( n )\n",
       "\n",
       "does 'not admit Shimura varieties' for n &gt; 2, meaning that algebraic geometry has limited applicability; however if one can use analytic means to move from an automorphic representation on GL ( n ) to an automorphic representation on a unitary group (where the methods do work) then this enables more analytic objects to 'cross the bridge' to the algebraic side. Results in Langlands functoriality thus have the dual role of adding weight to the conjectures (by proving consequences of them) and also enabling us to prove more cases of the conjectures. Here the work is Fourier-analytic and group-theoretic, with less reliance on algebraic and arithmetic geometry.\n",
       "\n",
       "- As is often the case in mathematics, people attempt to understand the questions in this area by generalizing them. The Galois representations which the Langlands philosophy are p -adic in nature, and can move freely in p -adic families. This raises the question of whether one can p -adically deform automorphic representations; initially this sounds ridiculous because these objects are inherently complex-analytic. However p -adic analytic variants of the theory have now been established, and this gives us new freedom, unlocking new tools to establish more cases of the classical program; the key observation here is that p -adic analysis is far more closely related to arithmetic than classical analysis. What is happening in this area is a mixture of insights gleaned from analogy ('what if there is a more arithmetic version of analysis?') turn into an actual theory (the existence of p -adic automorphic representations) but then must be backed up by very technical work showing that the theory can actually be used to solve new cases of the problem at hand. Work in this area thus needs to blend heuristic ideas and technical calculations.\n",
       "\n",
       "Further considerations. An important aspect that needs to be considered when mapping mathematical workflows to data is that different representations of the same data can lead to different mathematical properties that can change the method of proof, as well as the foundational model most suited for the chosen representation.\n",
       "\n",
       "For example, it is an open problem in knot theory whether there exists a polynomial-time algorithm that can detect whether a given knot, which is an embedded circle in R 3 , can be simplified (without breaking it) to a standard round circle. Instead of describing the knot as a curve in R 3 , knots are often specified in terms of their projection to two dimensions, and data representations of the knots keep track of which strands are above and which are below along the projection ray.\n",
       "\n",
       "The above-mentioned question then becomes to find a sequence of deformations of the embedded circle such that its projection has no crossings. It is known [Kauffman and Lambropoulou, 2012] that for some data representations of the projected knot (such as Dowker-Thistlethwaite codes [Dowker and Thistlethwaite, 1983] with Reidemeister moves as allowed deformations [Reidemeister, 1927]), there exist examples where the number of crossings needs to be increased before all crossings can be removed. In another representation (grid representation with Dynnikov moves as allowed deformations), the number of crossings is monotonically decreasing [Dynnikov, 2006], but both the representation of the knot as a grid diagram and the carrying out of the simplification steps are more complicated. These distinct representations of the same mathematical object lend themselves to different foundational models [Gukov et al., 2021, Kauffman et al., 2022]; for example, braid word representations of knot projections are closer to natural language, whereas other representations are closer to vision tasks or graphs. The different representations also inform the type of algorithm that needs to be used. If the simplification is non-monotonic, a local search can get stuck in local minima.\n",
       "\n",
       "One common activity in mathematics, when confronted with a new statement of unknown truth, a conjecture, is to engage in the search for proof - or a counterexample. This comes with its own workflows. The search for counterexamples to conjectures is particularly useful for hard problems with a truth certificate that can be verified in polynomial time, such as NP-hard or NP-complete problems. The idea is to cast the search problem into a Markov Decision Problem (MDP) whose terminal states are counterexamples, and attempt to solve the MDP using data science techniques. In the past, deep reinforcement learning has proven to be a powerful tool. If the RL agent finds a solution to the MDP, its episodic rollouts provide truth certificates for the counterexample, thus establishing a verifiably correct proof by counter-example, see e.g. [Gukov et al.,\n",
       "\n",
       "2024] for a recent summary of these ideas and [Gukov et al., 2021, Wagner, 2021, Gukov et al., 2023, Charton et al., 2024] for some recent concrete applications.\n",
       "\n",
       "In the formal setting, Learning to Disprove frames counterexample search in Lean 4 as proposing a candidate counterexample and then certifying it by producing a Lean proof term, using a large synthetic corpus of counterexample instances [Li et al., 2025b].\n",
       "\n",
       "## 4.2 Meta-Reasoning\n",
       "\n",
       "Many of the current evaluations of mathematical reasoning abilities focus in reasoning inside a particular problem. In a mathematics competition or a class, that is generally enough: it is assumed that all problems are solvable, that solutions are not too long, and furthermore that all the necessary tools come from a known, bounded set (e.g., of topics allowed in competitions, or pre-requisites for the class). At the research frontier, however, a substantial amount of work is spent at the meta-level, where these assumptions are constantly being evaluated. The failure to prove a statement might be due to it being false (and perhaps a counter-example can be extremely non-trivial to produce). Or, the appropriate tools might not yet exist. In either case, the problem-level assumptions above might be violated, and one has to constantly decide whether to keep pushing on the current plan, and what to do if not. To the best of our knowledge, none of the current evaluations for mathematical reasoning directly probe these meta-level judgements, even though they represent an important opportunity for assistance from research copilots.\n",
       "\n",
       "Some of the most basic meta-judgements that we might want to evaluate are (a) is a given statement true? (and does that change in light of given failed attempts to prove it?) (b) if it is, how much work will it take to prove it (or, given a partial proof, how much work is left?), and (c) is a given strategy adequate to prove it? In the formal setting, LeanProgress trains a predictor of proof progress from Lean proof states and uses it to guide search on longer Mathlib proofs [Huang et al., 2025]. Many examples that we could use to evaluate the ability to answer these questions could be extracted from finished works: papers have examples of true statements; removing assumptions from those statements often renders them false (and most papers justify why the assumptions are needed, if not apparent, such as by constructing counter-examples where those assumptions are violated). We can measure the complexity of existing proofs in various ways (e.g., length, or by their dependencies inside a paper) to evaluate predictions of difficulty - these can also be made relative, e.g., identifying which lemma in the context of a broader proof is going to take the most work. On the other hand, many important meta-level decisions cannot be readily observed in finished work, such as when to change the problem locally (such as adding stronger assumptions), or switching problems entirely - perhaps temporarily, e.g., to gain intuition on a special case first, or more firmly, and in that case deciding what to work on instead. For such decisions, one might need to observe mathematicians doing their work , as opposed to their finished products, as we discuss next.\n",
       "\n",
       "## 4.3 Data Collection in Real Environments\n",
       "\n",
       "To gather representative data on mathematical workflow steps, as well as other metamathematical items, such as proof heuristics, limitations of certain proof techniques, etc., it may be desirable to observe and absorb the full process of producing mathematics, including all data that is not represented in written, textual form. For an example of real-time narration of thought processes that arise when doing competitive problem-solving, see co-author Tim Gowers' series of YouTube videos. 15\n",
       "\n",
       "Unfortunately, many of these intermediate process steps become evident only implicitly from data sources. For example, at the elementary level, various workflows to solve various limits, such as lim x → 0 sin x -x x 3 or lim x →∞ x 1 /x , which require repeated applications of known theorems, such as de l'Hôpital's theorem, or related ones to resolve indeterminate cases, are distilled only by solving a large number of exercises; no explicit annotations for these workflows exist. On a more advanced level, these are often conveyed in blogs,\n",
       "\n",
       "15 https://www.youtube.com/@TimothyGowers0/videos\n",
       "\n",
       "talks, and oral discussions between mathematicians at conferences - and not in the typical data sources used to train LLMs.\n",
       "\n",
       "In principle, one could implement longitudinal studies that track research projects from inception to completion. However, such data collection would require classifying, structuring, and recording different tasks throughout the mathematician's daily work. The apparent challenge is to scale such approaches while keeping the overhead as small as possible for the researcher whose work is tracked. The KWARC group at FAU Erlangen-Nürnberg has developed infrastructure for semantic mathematical documents and tested such systems with hundreds of students per semester [Kohlhase, 2006, Berges et al., 2023], demonstrating the feasibility of structured mathematical data collection at scale, though their focus has been primarily on finished documents rather than the discovery process.\n",
       "\n",
       "While such approaches are welcome to be tested, as a first alternative step, we recommend transcribing from online sources such as lectures, panel discussions, seminars, vlogs, etc. VideoMathQA evaluates mathematical reasoning grounded in lecture-style videos, including long videos, with questions that require integrating visual and spoken information across time [Rasheed et al., 2025]. While such data only requires minimal need for postprocessing and is often already collected in the form of videos, it typically covers broader mathematical aspects than textbooks. However, while humans can frequently generalize from a single instance of a certain workflow, LLMs often require a dataset that contains sufficiently many examples. While data collection effort will elicit higher-quality mathematical data, the time lag between the time when new research discoveries are made and when these are represented in lectures can be significant. On the other hand, at venues such as conferences, a lot of information about mathematics is exchanged, but not recorded. While it would technically be possible to record a portion of the conversations that happen at a conference in a privacy-preserving manner, and this would be a great source of data, as one would consistently have data that is at the forefront of research (unlike the mentioned panel discussions, where there is often a time lag), it would require a paradigm shift from conference participants to accept such data collecting measures. We urge further conversations about privacy-preserving ways of curating richer datasets on mathematical workflows, that minimally impinge on mathematicians' naturalistic practices.\n",
       "\n",
       "## 4.3.1 From Artifacts to Trajectories in Formal Proof Engineering\n",
       "\n",
       "One distinctive advantage of formal mathematics is that the development environment already produces fine-grained signals that can be turned into datasets, which are in turn used to improve future systems. These signals include elaborator diagnostics, proof states, tactic traces, resource usage (heartbeats), and even lint warnings about unstable or redundant proof steps. This makes formal proof engineering a promising setting for process-level datasets.\n",
       "\n",
       "One such dataset type is a repair trajectory corpus : sequences of (i) an attempted proof edit, (ii) compiler/LSP diagnostics (type errors, missing imports, failing goals, timeouts), and (iii) the subsequent patch. Such data directly represents the compile-diagnose-fix loop that dominates real formalization workflows, and it can be used to train copilots that are useful beyond one-shot 'fill the sorry ' tasks.\n",
       "\n",
       "A second such dataset type is a refactoring corpus : pairs of proofs before/after refactoring steps that improve readability, reduce instance pollution, decrease resource usage, or align with library style. For example, proof refactoring guidance emphasizes extracting helper lemmas, compiling after each extraction, and using fast diagnostic feedback loops that (i) test compilation after each extraction, and then (ii) use LSP diagnostics to locate natural boundaries. Such datasets would support copilots that can shepherd code through review, not merely produce a valid proof.\n",
       "\n",
       "As a third example, stability and hygiene linting (e.g., flagging tactic instability or unused steps) can be recorded as structured metadata. This enables benchmarks that measure not just whether a proof checks, but whether it is stable across versions and robust under realistic resource constraints.\n",
       "\n",
       "## 4.4 Motivated Proofs\n",
       "\n",
       "We do not doubt that there is substantial value in bespoke AI tools that can automate specific elements of a mathematician's workflow. However, if the goal is not only to provide correct proofs but also to enhance a user's mathematical understanding of the results [Zhang et al., 2023a] and guide new discoveries, we need new tooling - and we argue, new kinds of data. In particular, we argue for data which faithfully represents the process of proof discovery instead of proof exposition . Towards this goal, we introduce motivated proofs [Pólya, 1949, Morris, 2019], which contain and make transparent more of the proof discovery process. We argue for their value as a standard for LLM evaluation. We then provide some examples and discuss some preliminary observations about current LLM's ability to construct and identify motivated proofs. Finally, we speculate on a path towards large-scale evaluation of a model's ability to produce motivated proofs.\n",
       "\n",
       "Broadly speaking, a motivated proof is one that makes clear to the reader where each step comes from. For example, many interesting proofs require one to find a mathematical object with certain properties. An unmotivated proof will simply specify the object and check that it has the desired properties, while a motivated proof will explain how to arrive at the object.\n",
       "\n",
       "## 4.4.1 Examples of motivated proofs\n",
       "\n",
       "We present two theorems here with examples of motivated proofs. Further examples can be found in Appendix A.\n",
       "\n",
       "Cantor's theorem Cantor's theorem states that there is no surjection from a set X to its power set P ( X ). The proof proceeds by letting f : X → P ( X ) be a function and trying to find a subset of X that is not in the image of f . From here, an unmotivated proof will simply exhibit a set, namely, { x ∈ X : x / ∈ f ( x ) } , and verify that it is not in the image of f , which turns out to be straightforward.\n",
       "\n",
       "/negationslash\n",
       "\n",
       "By contrast, a motivated proof will systematically search for the required subset of X . Not knowing which subset to take, we can treat the subset as an unknown, just as we do when solving an equation, and try to narrow down the possibilities. The most general subset of X can be expressed as { x ∈ X : P ( x ) } for some as yet unspecified property P . We now want to prove, for an arbitrary element y of X , that f ( y ) = { x ∈ X : P ( x ) } . So we need either an element x of f ( y ) such that ¬ P ( x ) or an element x of the complement of f ( y ) such that P ( x ). There are not many elements around, so trying y is one of the first things to do, and then we find that we need either y ∈ f ( y ) and ¬ P ( y ) or y / ∈ f ( y ) and P ( y ). And now the property y / ∈ f ( y ) is forced on us as our choice of P , and we end up with the same set as before, but this time with its origin explained.\n",
       "\n",
       "Nilpotent units Now we consider an early result from commutative algebra. Let R be a commutative ring, and let x ∈ R be nilpotent, then (1 + x ) is a unit, meaning it has a multiplicative inverse. This is typically proved by naming an element y = ∑ r -1 k =0 ( -1) k · x k and verifying that this is a multiplicative inverse element by calculation. We regard this as unmotivated, since the discovery process of finding y was left out.\n",
       "\n",
       "For a motivated proof, we need to search for an inverse element. Not knowing which element to take, we parametrize the most generic element we can. Since the only known elements of R are 1 and x , the most generic element is an integer polynomial in x , ∑ m k =0 a k x k . For this to be a right inverse of 1 + x , we must have that 1 = (1 + x )( ∑ m k =0 a k x k ) = a 0 + ∑ m k =1 ( a k + a k -1 ) x k + a m x m +1 . For this to hold, we must eliminate all coefficients of the polynomial besides the constant term, which should be 1. This gives us that a 0 = 1, a k = a k -1 for 1 ≤ k ≤ m and a m = 0. The first two equations give us that a k = ( -1) k for all k , but this contradicts the last equation.\n",
       "\n",
       "But we also know that x is nilpotent, so if we have r such that x r = 0, then all coefficients from x r onwards can be ignored. This solves our issue, as we can let m = r -1 to remove the a m = 0 condition, and we are left with a k = ( -1) k , so (1 + x ) is a unit with inverse ∑ r -1 k =0 ( -1) k · x k .\n",
       "\n",
       "## 4.4.2 Motivated proofs as an evaluation metric\n",
       "\n",
       "The standard for a proof's acceptance into the mathematical literature has historically been correctness rather than a completely motivated account. Moreover, mathematicians have been incentivized to condense and refine their proofs for reasons such as page limits in journals, which has further widened the gap between proof discovery and proof exposition . To assess a model's capabilities in aiding proof discovery, we suggest that holding the proof to the standard of being motivated provides advantages over requiring only correctness.\n",
       "\n",
       "As argued in [Pólya, 1949, Morris, 2019], a motivated proof is more informative to the reader, as it provides more insight into how the proof was discovered and how the reader might discover similar results themselves. More speculatively, a model capable of producing motivated proofs will likely generalize better to novel problems. The first reason for this conjecture is that most results in the literature are not fully motivated, and a model that is capable of recognising this is less likely to blindly recall its training data. Requiring proofs to be motivated also restricts the amount of brute-force search a model can perform, so performance depends more on reasoning ability and less on test-time computational power. A dedicated study to test whether models that generate motivated proofs indeed generalize better to novel problems could be worth pursuing, because it might reshape how we approach automated reasoning.\n",
       "\n",
       "We will now briefly evaluate current LLMs' ability to generate motivated proofs, as well as to evaluate whether a given proof is motivated.\n",
       "\n",
       "Generation of motivated proofs As a preliminary investigation, we prompted o1-preview and Llama 3.2-90B to produce motivated proofs of five mathematical results. Our goal was not to use best-in-class LLMs since they typically differ only by a few percentages on common benchmarks; in contrast, we wanted to obtain a preliminary assessment of whether LLMs have the ability to work with the concept of a motivated proof at all. The results on which we tested the abilities to generate motivated proofs were selected to be elementary but to require a slightly non-trivial construction. After defining motivated proofs, we provided an in-context example of a motivated proof of the other four results and asked the model to generate a new motivated proof. If the model failed to provide a motivated proof, we would prompt it once to fix it, pointing out the unmotivated step. Each model was asked twice per problem, first with only examples of motivated proofs in the context, following a traditional chain-of-thought approach, then again with both examples and non-examples, mimicking contrasting chain-of-thought prompting. The problems and associated proofs we used in the prompts can be found in Appendix A.\n",
       "\n",
       "In most responses, the model showed some evidence of 'understanding\" the concept of motivated proofs by avoiding unmotivated constructions. However, this often led to pseudo-motivations, where some justification was given before a construction, but it didn't provide any useful substance besides obscuring the unmotivated step (and often appealed to some irrelevant fallacies). In the example below, the unmotivated step of Cantor's theorem is relegated to the statement 'An important question arises: Does x belong to its own image f ( x )?\".\n",
       "\n",
       "## o1-preview (excerpt)\n",
       "\n",
       "Our goal is to find a subset of X that cannot possibly be the image of any element under f . To do this, we'll consider how elements of X relate to their images under f .\n",
       "\n",
       "## Exploring the Relationship Between Elements and Their Images:\n",
       "\n",
       "For each element x ∈ X , f ( x ) is a subset of X . An important question arises: Does x belong to its own image f ( x ) ?\n",
       "\n",
       "This leads us to categorize the elements of X into two groups:\n",
       "\n",
       "1. Elements that are members of their image: x ∈ f ( x ).\n",
       "2. Elements that are not members of their image: x / ∈ f ( x ).\n",
       "\n",
       "The other common issue was the model making completely unsound deductions when trying to motivate their\n",
       "\n",
       "construction, only to arrive at the correct result by chance. This behavior is consistent with the model not fully understanding the result but having seen it in their training data and attempting post-hoc rationalization. Below is an example:\n",
       "\n",
       "## Llama-3.2 (excerpt)\n",
       "\n",
       "We want to find a subset Y ⊆ X such that φ ( x ) = Y for some x ∈ X . However, we don't know which subset to choose.\n",
       "\n",
       "Let's try to find a property that Y should satisfy. Since φ ( x ) = Y , we know that x ∈ φ ( x ) = Y . But we also know that x / ∈ φ ( x ), because if x ∈ φ ( x ), then φ ( x ) would contain at least two elements: x and φ ( x ). This suggests that Y should be a subset of X that contains some elements of X , but not all of them. In particular, Y should not contain x itself.\n",
       "\n",
       "With this in mind, let's define Y = { x ∈ X | x / ∈ φ ( x ) } .\n",
       "\n",
       "Here, the contradictory claims that x ∈ φ ( x ) and x / ∈ φ ( x ) have no basis, and if true would imply an immediate contradiction, making the remainder of the proof redundant. Also, the condition for Y is in fact the opposite of what is needed, but this error doesn't seem to affect the remainder of the model's reasoning.\n",
       "\n",
       "Other problems provided more hopeful examples: a motivated proof for the large totients problem was successfully generated both times by o1-preview and once by Llama.\n",
       "\n",
       "Overall, o1-preview seemed to outperform Llama on most problems. We also observed a slight improvement by using negative as well as positive examples to 'explain\" the concept. As an unfortunate side effect, the extra information also increased the tendency to 'obscure\" unmotivated arguments as discussed above.\n",
       "\n",
       "Recognition of motivated proofs To conduct a larger scale investigation of a model's ability to generate motivated proofs, it would help if there was an existing 'judge\" model which could determine whether or not a given proof was motivated. In any case, recognition of motivated proofs represents an easier sub-goal towards generating motivated proofs.\n",
       "\n",
       "We performed a small, preliminary investigation of whether current state-of-the-art models can reliably distinguish a motivated proof from an unmotivated proof. We stress that this is only a preliminary investigation and do not claim to draw any strong conclusion, but hope that this will inspire the community to carry out larger evaluations in this regard and work towards a standard of evaluating motivated proofs.\n",
       "\n",
       "We first tested the model's ability to identify whether a given proof was motivated. We tested the five problems from the previous section, where each problem had three proofs to be evaluated individually. This included two human-written proofs, which we judged to be paradigmatic examples of motivated and unmotivated proofs, one unmotivated machine-generated proof and, where available, one motivated machine-generated proof. The unmotivated machine-generated proof was the first correct but unmotivated proof generated by Llama-3.2 in the previous section. For the machine-generated motivated proof, we took the response in the previous section that we judged to be most motivated, which was generated by Llama in the integer sum problem and o1-preview for nilpotent units and large totients, while for the other problems no responses were deemed sufficiently motivated.\n",
       "\n",
       "For some proofs where there were incorrect but largely inconsequential components, we manually altered some equations to enforce correctness and removed any opening or closing sentences that included the phrase 'motivated proof\" to avoid misleading the model. As with generation, we prompted the model by explaining the definition of a motivated proof and giving an example and non-example for each of the other problems, where the order of examples and non-examples was alternated to ensure that the model wasn't learning the ordering. Each model was given three attempts per proof per problem, and the results are recorded below. We use HM, HU, MM and MU as abbreviations for 'human motivated', 'human unmotivated' and 'machine unmotivated' respectively.\n",
       "\n",
       "|                  | o1-preview HM   | HU    | MM   | MU    | Llama-3.2 HM   | HU   | MM   | MU   | Total TP   | TN    |\n",
       "|------------------|-----------------|-------|------|-------|----------------|------|------|------|------------|-------|\n",
       "| Cantor's theorem | 3/3             | 3/3   | -    | 3/3   | 3/3            | 0/3  | -    | 0/3  | 6/6        | 6/12  |\n",
       "| Small doubling   | 3/3             | 3/3   | -    | 2/3   | 3/3            | 3/3  | -    | 2/3  | 6/6        | 10/12 |\n",
       "| Integer sum      | 3/3             | 3/3   | 3/3  | 2/3   | 2/3            | 3/3  | 3/3  | 0/3  | 11/12      | 8/12  |\n",
       "| Nilpotent units  | 3/3             | 3/3   | 3/3  | 3/3   | 2/3            | 3/3  | 3/3  | 0/3  | 11/12      | 6/12  |\n",
       "| Large totients   | 3/3             | 3/3   | 3/3  | 3/3   | 3/3            | 2/3  | 3/3  | 1/3  | 12/12      | 9/12  |\n",
       "| Total            | 15/15           | 15/15 | 9/9  | 13/15 | 13/15          | 8/15 | 9/9  | 3/15 | 46/48      | 39/60 |\n",
       "\n",
       "Under this setup, o1-preview performed strongly, while Llama was heavily biased towards accepting a proof as motivated. A possible explanation for Llama's poor performance in the MU set is that the machine-generated unmotivated proofs, unlike the human-generated ones, were attempts at generating motivated proofs, so these proofs contained some superficial features such as proof length and 'chatty\" language that the model had associated with motivated proofs.\n",
       "\n",
       "In an effort to address this issue, we re-framed the experiment as a binary choice task. This was largely the same as above, but instead of being given a single proof to judge, the models were given two proofs and were told that one was motivated and one was unmotivated, and to judge which was motivated. Again we recorded the success rate over three valid attempts per (unmotivated,motivated) pair, per problem. The results are shown below.\n",
       "\n",
       "| Unmotivated proof source   | o1-preview Human   | Model   | Llama-3.2 Human   | Model   | Total   |\n",
       "|----------------------------|--------------------|---------|-------------------|---------|---------|\n",
       "| Cantor's theorem           | 3/3                | 3/3     | 3/3               | 3/3     | 12/12   |\n",
       "| Small doubling             | 3/3                | 3/3     | 3/3               | 3/3     | 12/12   |\n",
       "| Integer sum                | 6/6                | 6/6     | 6/6               | 6/6     | 24/24   |\n",
       "| Nilpotent inverses         | 6/6                | 3/6     | 0/6               | 2/6     | 11/24   |\n",
       "| Large totients             | 6/6                | 4/6     | 6/6               | 5/6     | 21/24   |\n",
       "| Total                      | 24/24              | 19/24   | 18/24             | 19/24   | 80/96   |\n",
       "\n",
       "This improved evaluation method narrowed the gap between the two models, especially when using the MU proof which Llama previously struggled with, providing some evidence that the binary choice strategy reduces the model's tendency to be misled by superficial features common to motivated proofs.\n",
       "\n",
       "We stress that this is only a preliminary investigation, and that a larger investigation would be required to make strong claims about an LLM's ability to judge motivated proofs. We welcome the community to conduct a larger investigation, advising the following notes of caution based off our experience and some speculation:\n",
       "\n",
       "- It is easier for models to decide between two proofs, which is motivated, than to make an absolute judgement of a single proof.\n",
       "- Models can be sensitive to the order in which the proofs are presented, so it is important to permute these (including the in-context examples).\n",
       "- Models can be misled by superficial features common to motivated proofs, so it is important that false examples also have these features. We achieved this to some extent by using an LLM's failed attempt at writing a motivated proof, but this could potentially also be achieved with care in a human-generated dataset.\n",
       "\n",
       "Replication with newer models Given the significant improvements in model performance since the release of an earlier preprint, we have replicated these investigations with newer models. Although these are still preliminary investigations, we acknowledge the risk of data contamination.\n",
       "\n",
       "We first replicated the generation task, prompting each model to generate a motivated proof of each result given an example and non-example of each other result. Each task was repeated three times, with each\n",
       "\n",
       "response manually classified by the authors and the results tabulated below:\n",
       "\n",
       "|                    | OpenAI GPT-5.2 Pro   | DeepSeek Prover-V2   | Google Gemini 3 Pro   | OpenAI GPT-OSS 120B   | Total   |\n",
       "|--------------------|----------------------|----------------------|-----------------------|-----------------------|---------|\n",
       "| Cantor's theorem   | 3/3                  | 0/3                  | 3/3                   | 0/3                   | 6/12    |\n",
       "| Small doubling     | 1/3                  | 0/3                  | 2/3                   | 1/3                   | 4/12    |\n",
       "| Integer sum        | 3/3                  | 0/3                  | 3/3                   | 3/3                   | 9/12    |\n",
       "| Nilpotent inverses | 3/3                  | 0/3                  | 1/3                   | 3/3                   | 7/12    |\n",
       "| Large totients     | 3/3                  | 3/3                  | 3/3                   | 3/3                   | 12/12   |\n",
       "| Total              | 13/15                | 3/15                 | 12/15                 | 10/15                 | 38/60   |\n",
       "\n",
       "This simple investigation was sufficient to observe substantial variation between models, although we believe that more difficult problems are needed to meaningfully distinguish the top models. It is also noteworthy that both closed-source models consistently succeeded on Cantor's Theorem, which neither of the older models could succeed on. It is also interesting that DeepSeek Prover-v2 performed poorly despite its mathematical specialization, which is consistent with the hypothesis that heavy fine-tuning for formal proof generation may reduce performance on stylistically different proofs.\n",
       "\n",
       "We then repeated the binary classification task with newer models, concentrating on the more difficult and relevant machine-machine distinction, using proofs generated in the previous task. The first (motivated,unmotivated) pair was generated by GPT-5.2 Pro and DeepSeek Prover respectively, and the differences were stark enough that every model was correct 100% of the time. For the second pair we compared Gemini's correct responses to a varied selection of incorrect model responses (including Gemini's where possible). We used a human-written unmotivated proof for the problem of 'large totients\", since none of the newer models produced an unmotivated proof. The performance of each model on the second pair is shown below.\n",
       "\n",
       "| Theorem            | OpenAI GPT-5.2 Pro   | DeepSeek Prover-V2   | Google Gemini 3 Pro   | OpenAI GPT-OSS 120B   | Total   |\n",
       "|--------------------|----------------------|----------------------|-----------------------|-----------------------|---------|\n",
       "| Cantor's theorem   | 2/3                  | 1/3                  | 3/3                   | 1/3                   | 7/12    |\n",
       "| Small doubling     | 3/3                  | 0/3                  | 0/3                   | 3/3                   | 6/12    |\n",
       "| Integer sum        | 3/3                  | 3/3                  | 3/3                   | 3/3                   | 12/12   |\n",
       "| Nilpotent inverses | 3/3                  | 1/3                  | 3/3                   | 3/3                   | 10/12   |\n",
       "| Large totients     | 3/3                  | 3/3                  | 3/3                   | 3/3                   | 12/12   |\n",
       "| Total              | 14/15                | 8/15                 | 12/15                 | 13/15                 | 47/60   |\n",
       "\n",
       "This generally shows some improvement in distinguishing motivated from unmotivated proofs, although some more subtle distinctions are still consistently missed.\n",
       "\n",
       "Recommendations The existing mathematical literature, machine learning datasets and output evaluation standards are concentrated mostly on proof correctness, and our principal recommendation is to increase emphasis on proof motivation to better reflect the proof discovery process. One natural path forward would be to create a corpus of mathematical results with corresponding motivated and unmotivated proofs, and then to conduct a detailed evaluation of current models' ability to distinguish these. Once it is ascertained that models can judge motivation with high accuracy, one can design metrics for motivated proof generation.\n",
       "\n",
       "## 5 Conclusion\n",
       "\n",
       "We have outlined difficulties that pertain to natural language as well formal language mathematics datasets that we believe are present hindrances to the progress of AI systems towards becoming real mathematical thought partners that are as, if not more, useful to mathematicians as GitHub's Copilot 16 is to programmers.\n",
       "\n",
       "16 https://github.com/features/copilot\n",
       "\n",
       "The advantages and disadvantages of natural language and formal language datasets are frequently complementary. What is easy in one representation of mathematics is often hard in the other. For example, automatic evaluation is easy in formal language but hard in natural language; representing rich interaction modes is often comparatively easy in natural language but harder to express in formal language.\n",
       "\n",
       "We have identified several facets of mathematical practice that are currently not represented in the data used to design and evaluate the deployment of AI systems for mathematics, such as various workflows. We acknowledge that the aspects of workflows we highlight here likely do not capture all possible facets of mathematical practice - but we believe they represent an important start, offering an outline for what we may be able to curate to more human-compatible and explainable mathematical AI systems.\n",
       "\n",
       "While our aim has been exclusively in mathematics, these approaches of mapping processes of scientific discovery to data are not restricted to mathematics and may be adapted to other scientific domains as well. Some of the workflow items, like those related to literature search are directly relevant to other domains. We urge the community to explore how intermediate steps in the process of scientific discovery look like in other areas of science as well.\n",
       "\n",
       "Regarding purely proof creation, time will tell whether human-annotated proofs are necessary-or whether a purely formal approach together with ingenious proof search techniques will succeed. The comparatively short history of machine learning has taught us to expect the unexpected. In the short term, however, it is imperative to establish better datasets and novel benchmarks for mathematics (either in natural language or formal language), to go beyond the current ones that only test proof or result creation - and also test the ability to assess different mathematical workflows, to summarize mathematics, to explain limitations of proof techniques etc. This will support the next generation of machine learning models and AI tools that can help us discover more mathematics fast and, subsequently assist with any other scientific discipline that uses mathematics as a foundation (e.g. systems biology that relies on ordinary differential equations, or physics that relies on several subfields of mathematics - and inspires new subfields at the same time). At the same time, better mathematical copilots may have strong educational benefits. Recent controlled experiments suggest that LLM-based explanations can positively impact learning for high-school level mathematics, particularly when students attempt problems independently before consulting the model [Kumar et al., 2025]. Moreover, Human-AI tutoring systems such as Tutor CoPilot have demonstrated measurable improvements in student outcomes when LLMs provide real-time pedagogical guidance to novice tutors [Wang et al., 2024b].\n",
       "\n",
       "Having a deeper understanding of the processes by which one arrives at a proof (heuristics, workflows, etc.), which are all concentrated in the concept of a motivated proof, mathematical copilots can also teach the next generation of mathematical minds.\n",
       "\n",
       "Even though the history of the concept of 'proof' spans millennia, the story is ongoing, as currently, the search for a machine-learnable 'proof data structure' is an important focus point and represents a new chapter in the ongoing story of what a proof really is.\n",
       "\n",
       "## Acknowledgements\n",
       "\n",
       "Thomas Lukasiewicz was supported by the AXA Research Fund. Katherine M. Collins acknowledges support from the Cambridge Trust. The work of Fabian Ruehle is supported by NSF grants PHY-2210333, PHY2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions), and startup funding from Northeastern University. Timothy Gowers would like to acknowledge generous support from the Astera Institute. We thank Terence Tao for useful remarks.\n",
       "\n",
       "| System                                                                                  | Reference                                                                               | Architecture class                                                                      | How it helps mathematicians                                                                                                         | User un- derstand- ing                                                                  |\n",
       "|-----------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|\n",
       "| Closed-ended problem solving (verifiable / near-verifiable)                             | Closed-ended problem solving (verifiable / near-verifiable)                             | Closed-ended problem solving (verifiable / near-verifiable)                             | Closed-ended problem solving (verifiable / near-verifiable)                                                                         | Closed-ended problem solving (verifiable / near-verifiable)                             |\n",
       "| AlphaGeometry                                                                           | Trinh et al. [2024]                                                                     | Hybrid (neural + symbolic)                                                              | Solves Euclidean geometry problems via learned components plus symbolic deduction; uses a domain-specific formal encoding (JGEX).   | IF                                                                                      |\n",
       "| Newclid                                                                                 | Sicca et al. [2024]                                                                     | Symbolic (search / ATP-style)                                                           | Solves Euclidean geometry problems in JGEX, producing checkable deriva- tions.                                                      | IF                                                                                      |\n",
       "| Inequality prover                                                                       | Wei et al. [2024]                                                                       | Symbolic (automa- tion)                                                                 | Automates structured inequality prov- ing in restricted families, producing derivations that can often be mechani- cally verified.  | IF                                                                                      |\n",
       "| Conjecture, counterexample, and construction discovery (requires downstream validation) | Conjecture, counterexample, and construction discovery (requires downstream validation) | Conjecture, counterexample, and construction discovery (requires downstream validation) | Conjecture, counterexample, and construction discovery (requires downstream validation)                                             | Conjecture, counterexample, and construction discovery (requires downstream validation) |\n",
       "| Graph-building RL counterexamples                                                       | Wagner [2021]                                                                           | RL policy (often MLP/GNN)                                                               | Finds counterexamples by construct- ing discrete objects in an environment with an automatically scored objec- tive.                | PL                                                                                      |\n",
       "| PatternBoost                                                                            | Charton et al. [2024]                                                                   | Transformer + local search                                                              | Discovers combinatorial constructions by alternating neural generation with search, targeting automatically check- able properties. | PL                                                                                      |\n",
       "| Invariant-correlation models (e.g., knot the- ory)                                      | Davies et al. [2021]                                                                    | Supervised regres- sion (often linear / tree-based)                                     | Suggests conjectural relationships be- tween computable invariants; outputs hypotheses that mathematicians then prove or refute.    | OB                                                                                      |\n",
       "| Structured/tabular ML in group theory                                                   | He et al. [2023]                                                                        | Supervised learning (classifier/regressor)                                              | Produces conjectures or predictions from engineered representations of al- gebraic objects; requires mathematical validation.       | OB                                                                                      |\n",
       "| Formal proof engineering (assistance within an ITP ecosystem)                           | Formal proof engineering (assistance within an ITP ecosystem)                           | Formal proof engineering (assistance within an ITP ecosystem)                           | Formal proof engineering (assistance within an ITP ecosystem)                                                                       | Formal proof engineering (assistance within an ITP ecosystem)                           |\n",
       "| ImProver (Lean)                                                                         | Ahuja et al. [2024]                                                                     | Transformer (LLM program synthesis)                                                     | Edits or refactors Lean proofs under constraints (e.g., length), helping with proof maintenance and presentation.                   | IF                                                                                      |\n",
       "\n",
       "User understanding : BB = black-box use; IF = interface or formalism (e.g., Lean, JGEX); PL = pipeline configuration and objective (representation, search, scoring); OB = feature or representation choices and the validation procedure used to interpret outputs.\n",
       "\n",
       "Table 1: Highly specialized ('narrow') systems cited in this paper, summarized by architecture class, primary mathematical role, and the typical user understanding required.\n",
       "\n",
       "Table 2: Issue matrix for datasets used as training corpora (Train) and as benchmarks (Bench), separated by natural language (NL) and formal mathematics. 'X' indicates the issue commonly or sometimes affects that use case. For formal mathematics, proof checking provides automated correctness, so the row 'Lack of trusted automatic evaluation' is typically not applicable.\n",
       "\n",
       "| Issue                                                                        | NL Train   | NL Bench   | Formal Train   | Formal Bench   |\n",
       "|------------------------------------------------------------------------------|------------|------------|----------------|----------------|\n",
       "| Difficulty calibration and coverage                                          | X          | X          | X              | X              |\n",
       "| Binary scoring / success-only reporting                                      | X          | X          | X              | X              |\n",
       "| Lack of trusted automatic evaluation                                         | X          | X          |                |                |\n",
       "| Standardized interaction modes (Q-A, iso- lated lemmas)                      | X          | X          | X              | X              |\n",
       "| Contamination and leakage                                                    | X          | X          | X              | X              |\n",
       "| Binned progress reporting (e.g., Sankey- style)                              |            | X          |                | X              |\n",
       "| Tool misalignment from automation / tactic availability                      |            |            | X              | X              |\n",
       "| Non-trivial duplication (multiple representa- tions / near-copies)           |            |            | X              | X              |\n",
       "| Incomplete proofs and placeholders ( sorry , Admitted )                      |            |            | X              | X              |\n",
       "| Toolchain and automation provenance (scripts, autoformalization, tactics)    |            |            | X              | X              |\n",
       "| Dependence on external libraries and version- ing (commits, environments)    |            |            | X              | X              |\n",
       "| Unclear benchmarking protocol (timeouts, allowed tools, environment pinning) | X          |            |                | X              |\n",
       "\n",
       "Table 3: Selected datasets for evaluating the mathematical capabilities of LLMs.\n",
       "\n",
       "| Dataset                      | Reference                       | Size Field                |                                                                                 | Description                                                                                                                                                   |\n",
       "|------------------------------|---------------------------------|---------------------------|---------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
       "| Commoncore / MultiArith      | Roy and Roth [2015]             | Elementary to 600         | Middle School Level Basic arithmetic                                            | Word problems with two steps                                                                                                                                  |\n",
       "| MAWPS                        | Koncel-Kedziorski et al. [2016] | 3320                      | Basic arithmetic and algebra                                                    | Word problems requiring systems of up to two linear equations (includes MultiArith)                                                                           |\n",
       "| Math23K                      | Wang et al. [2017]              | 23161                     | Basic arithmetic                                                                | Word problems in Chinese                                                                                                                                      |\n",
       "| ASDiv                        | Miao et al. [2020]              | 2305                      | Basic arithmetic                                                                | Word problems (selected from the internet)                                                                                                                    |\n",
       "| SVAMP                        | Patel et al. [2021]             | 1000                      | Basic arithmetic                                                                | Word problems requiring up to two steps (derived from ASDiv)                                                                                                  |\n",
       "| GSM8K                        | Cobbe et al. [2021]             | 8500                      | Basic arithmetic                                                                | Word problems requiring 2-8 steps to solve                                                                                                                    |\n",
       "| GSM8K-symbolic               | Mirzadeh et al. [2024]          | 5000                      | Basic arithmetic                                                                | Variants of GSM8K problems using symbolic templates                                                                                                           |\n",
       "| Dolphin18K                   | Huang et al. [2016]             | 18460                     | Basic arithmetic and algebra                                                    | Problems and answers from the mathematics category of Yahoo! Answers                                                                                          |\n",
       "| AQuA                         | Ling et al. [2017]              | 100000                    | Basic arithmetic and algebra                                                    | Algebraic multiple-choice word problems (similar to GMAT and GRE)                                                                                             |\n",
       "| MATH                         | Hendrycks et al. [2021]         | 12500                     | Problem-solving                                                                 | Problems from high school math competitions with step-by-step solutions                                                                                       |\n",
       "| NumGLUE                      | Mishra et al. [2022a]           | 101835                    | Arithmetic reasoning                                                            | 8 different mathematical reasoning tasks (including previous datasets).                                                                                       |\n",
       "| GAOKAO (Math)                | Zhang et al. [2023b]            | 844                       | Various high school math top- ics                                               | Questions from the Chinese College Entrance Exam (single-choice, cloze, open-ended)                                                                           |\n",
       "| Hungarian High School Finals | Paster [2023]                   | 33                        | Problem-solving                                                                 | Problems from the 2023 Hungarian National High School Finals in mathematics                                                                                   |\n",
       "| MATH 401                     | Yuan et al. [2023]              | 401                       | Advanced arithmetic and com-                                                    | Arithmetic expressions and numerical results                                                                                                                  |\n",
       "| TAL-SCQ5K                    | TAL Education Group [2023]      | 5000                      | Various                                                                         | Multiple-choice competition questions from elementary to high school levels in English and Chinese                                                            |\n",
       "| AGIEval (Math)               | Zhong et al. [2024]             | 1943                      | Various                                                                         | Problems (in English and Chinese) derived from college admission tests (GRE, Gaokao, SAT) and high school competitions (from MATH and AQuA)                   |\n",
       "| TheoremQA                    | Chen et al. [2023]              | 800                       | Various (including algebra, number theory, graph theory, information theory)    | Questions (from the internet and textbooks) and expert answers based on 354 theorems                                                                          |\n",
       "| ARB                          | Sawada et al. [2023]            | 234                       | Various                                                                         | Problems from university exams and competitions at the undergraduate level                                                                                    |\n",
       "| NaturalProofs                | Welleck et al. [2021]           | Graduate to 25271         | Research Level Various (focus on algebraic ge- ometry)                          | Theorems with proofs and reference graphs from ProofWiki , the Stacks project , and textbooks.                                                                |\n",
       "| GHOSTS                       | Frieder et al. [2023a]          | 709                       | Various                                                                         | Problem-solving, theorem-proving, and retrieval tasks derived from textbooks, StackExchange , and existing datasets (MATH and Symbolic integration)           |\n",
       "| FrontierMath                 | Glazer et al. [2024]            | 35 (unknown)              | Various                                                                         | Challenging problems with automated verification cre- ated by mathematicians                                                                                  |\n",
       "| miniF2F                      | Zheng et al. [2022]             | 488                       | Problem-solving, algebra, number theory                                         | Formalized Olympiad-type problems from AIME, AMC, IMO, as well as high school and undergrad- uate courses (also derived from MATH)                            |\n",
       "| OlympiadBench                | He et al. [2024]                | 8476                      | Various                                                                         | Problems and solutions from mathematics and physics competitions and the Chinese College Entrance Exam (in English or Chinese and partially with images)      |\n",
       "| IMO Small Challenge          | Frieder et al. [2024]           | 100                       | Combinatorics                                                                   | Selected problems and solutions from IMO andBWMC                                                                                                              |\n",
       "| IMO-AG-30                    | Trinh et al. [2024]             | 30                        | Geometry                                                                        | Plane Euclidean geometry problems from IMO in JGEX formal language                                                                                            |\n",
       "| Mathematics                  | Saxton et al. [2019]            | Various / Mixed Generated | Difficulty Arithmetic, algebra, probabil-                                       | Sequential questions generated with modular structure                                                                                                         |\n",
       "| MMLU (Math)                  | Hendrycks et al. [2020]         | 960                       | Elementary, high school, and college mathematics, and ab- stract algebra        | Multiple-choice questions collected from online sources ranging from elementary to college (similar to GRE) mathematics                                       |\n",
       "| INT                          | Wu et al. [2020]                | Generated                 | Inequalities, General Mathe- matics                                             | Generated formalized (in-)equality theorems based on ordered field axioms                                                                                     |\n",
       "| AMPS                         | Hendrycks et al. [2021]         | ≈ 5100000                 | (Linear) algebra, calculus, statistics, geometry, and num- ber theory           | Problems and step-by-step solutions from Khan Academy ( > 100k) and generated using Mathemat- ica ( ≈ 5M)                                                     |\n",
       "| Lila                         | Mishra et al. [2022b]           | 132239                    | Arithmetic, (linear) algebra, calculus, statistics, geometry, and number theory | Derived from 20 existing datasets (including NumGLUE, SVAMP, MultiArith, Dolphin18K, Math- ematics, AMPS, GSM8K, MATH) with numbers or expressions as outputs |\n",
       "| CMMLU (Math)                 | Li et al. [2023]                | 499                       | Various                                                                         | Multiple-choice questions (in Chinese) from elementary to college level collected from freely available resources                                             |\n",
       "\n",
       "## References\n",
       "\n",
       "- Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature , 625(7995):476-482, 2024.\n",
       "- OpenAI. GPT-4 technical report. arXiv preprint 2303.0877 , 2023.\n",
       "- Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, and Julius Berner. Mathematical capabilities of ChatGPT. In Advances in Neural Information Processing Systems , volume 36, 2023a.\n",
       "- Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530 , 2024.\n",
       "- DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.\n",
       "- Qwen Team. QwQ: Reflect deeply on the boundaries of the unknown. https://qwenlm.github.io/blog/ qwq-32b-preview/ , 2024.\n",
       "- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, and Heewoo Jun et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\n",
       "- Eshta Bhardwaj, Harshit Gujral, Siyi Wu, Ciara Zogheib, Tegan Maharaj, and Christoph Becker. The state of data curation at NeurIPS: An assessment of dataset development practices in the datasets and benchmarks track. In Advances in Neural Information Processing Systems , volume 37, pages 53626-53648, 2024. doi: 10.52202/079017-1698. NeurIPS 2024, Datasets and Benchmarks Track.\n",
       "- Vladmir Sicca, Tianxiang Xia, Mathïs Fédérico, Philip John Gorinski, Simon Frieder, and Shangling Jui. Newclid: A user-friendly replacement for AlphaGeometry. arXiv preprint arXiv:2411.11938 , 2024.\n",
       "- Chenrui Wei, Mengzhou Sun, and Wei Wang. Proving olympiad algebraic inequalities without human demonstrations. arXiv preprint arXiv:2406.14219 , 2024.\n",
       "- Alex Davies, Petar Veličković, Lars Buesing, Sam Blackwell, and Daniel Zheng et al. Advancing mathematics by guiding human intuition with AI. Nature , 600(7887):70-74, 2021.\n",
       "- Yang-Hui He, Vishnu Jejjala, Challenger Mishra, and Max Sharnoff. Learning to be simple. arXiv preprint arXiv:2312.05299 , 2023.\n",
       "- Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature , 625(7995):468-475, 2024.\n",
       "- Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature , 610(7930):47-53, 2022.\n",
       "- Baran Hashemi, Roderic G Corominas, and Alessandro Giacchetto. Can transformers do enumerative geometry? arXiv preprint arXiv:2408.14915 , 2024.\n",
       "- Adam Zsolt Wagner. Constructions in combinatorics via neural networks, 2021. URL https://arxiv.org/ abs/2104.14516 .\n",
       "\n",
       "- François Charton, Jordan S. Ellenberg, Adam Zsolt Wagner, and Geordie Williamson. PatternBoost: Constructions in mathematics with a little help from AI, 2024. URL https://arxiv.org/abs/2411.00566 .\n",
       "- Riyaz Ahuja, Jeremy Avigad, Prasad Tetali, and Sean Welleck. Improver: Agent-based automated proof optimization. arXiv preprint arXiv:2410.04753 , 2024.\n",
       "- Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum, William Hart, et al. Evaluating language models for mathematics through interactions. Proceedings of the National Academy of Sciences , 121(24):e2318124121, 2024a.\n",
       "- Katherine M Collins, Ilia Sucholutsky, Umang Bhatt, Kartik Chandra, Lionel Wong, Mina Lee, Cedegao E Zhang, Tan Zhi-Xuan, Mark Ho, Vikash Mansinghka, et al. Building machines that learn and think with people. Nature Human Behaviour , 8(10):1851-1863, 2024b.\n",
       "- Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. ToRA: A tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=Ep0TtjVoap .\n",
       "- Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. A survey on mixture of experts. Authorea Preprints , 2024.\n",
       "- Simon Frieder, Julius Berner, Philipp Petersen, and Thomas Lukasiewicz. Large language models for mathematicians. arXiv preprint arXiv:2312.04556 , 2023b.\n",
       "- Alexandre Riazanov and Andrei Voronkov. The design and implementation of VAMPIRE. AI Communications , 15(2-3):91-110, 2002.\n",
       "- Laura Kovács and Andrei Voronkov. First-order theorem proving and VAMPIRE. In International Conference on Computer Aided Verification , pages 1-35. Springer, 2013.\n",
       "- Stephan Schulz. E-a brainiac theorem prover. AI Communications , 15(2-3):111-126, 2002.\n",
       "- Sean B. Holden. Machine learning for automated theorem proving: Learning to solve SAT and QSAT. Foundations and Trends® in Machine Learning , 14(6):807-989, 2021. ISSN 1935-8237.\n",
       "- Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292 , 2024a.\n",
       "- Yoshua Bengio and Nikolay Malkin. Machine learning and information theory concepts towards an AI mathematician. Bulletin of the American Mathematical Society , 61(3):457-469, 2024.\n",
       "- Richard S Sutton. The bitter lesson. http://www.incompleteideas.net/IncIdeas/BitterLesson.html , 2019. Blog post.\n",
       "- Edward Hughes, Michael Dennis, Jack Parker-Holder, Feryal Behbahani, Aditi Mavalankar, Yuge Shi, Tom Schaul, and Tim Rocktäschel. Open-endedness is essential for artificial superhuman intelligence. arXiv preprint arXiv:2406.04268 , 2024.\n",
       "- Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation. In International Conference on Learning Representations , 2024.\n",
       "- Emily First, Markus Rabe, Talia Ringer, and Yuriy Brun. Baldur: Whole-proof generation and repair with large language models. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering , pages 1229-1241, 2023.\n",
       "- Peiyang Song, Kaiyu Yang, and Anima Anandkumar. Towards large language models as copilots for theorem proving in lean. arXiv preprint arXiv:2404.12534 , 2024a.\n",
       "\n",
       "- John Harrison, Josef Urban, and Freek Wiedijk. History of interactive theorem proving. In Computational Logic , volume 9, pages 135-214, 2014.\n",
       "- Jasmin Christian Blanchette, Andrei Popescu, Daniel Wand, and Christoph Weidenbach. More SPASS with Isabelle: Superposition with hard sorts and configurable simplification. In Interactive Theorem Proving: Third International Conference, ITP 2012, Princeton, NJ, USA, August 13-15, 2012. Proceedings 3 , pages 345-360. Springer, 2012.\n",
       "- Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks , volume 1. Curran, 2021.\n",
       "- Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E Gonzalez, and Ion Stoica. Rethinking benchmark and contamination for language models with rephrased samples. arXiv preprint arXiv:2311.04850 , 2023.\n",
       "- Ruijie Xu, Zengzhi Wang, Run-Ze Fan, and Pengfei Liu. Benchmarking benchmark leakage in large language models. arXiv preprint arXiv:2404.18824 , 2024.\n",
       "- Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, et al. FrontierMath: a benchmark for evaluating advanced mathematical reasoning in AI. arXiv preprint arXiv:2411.04872 , 2024.\n",
       "- Ziyin Zhang, Jiahao Xu, Zhiwei He, Tian Liang, Qiuzhi Liu, Yansi Li, Linfeng Song, Zhenwen Liang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Deeptheorem: Advancing llm reasoning for theorem proving through natural language and reinforcement learning. arXiv preprint arXiv:2505.23754 , 2025a.\n",
       "- Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, and Jenia Jitsev. Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models. arXiv preprint arXiv:2406.02061 , 2024. doi: 10.48550/arXiv.2406.02061.\n",
       "- Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.\n",
       "- Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. DeepSeek-V2: a strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434 , 2024.\n",
       "- Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. DeepSeek-Coder-V2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931 , 2024.\n",
       "- Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. DeepSeek-prover-V1. 5: Harnessing proof assistant feedback for reinforcement learning and Monte-Carlo tree search. arXiv preprint arXiv:2408.08152 , 2024.\n",
       "- Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609 , 2023.\n",
       "- An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671 , 2024a.\n",
       "- Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2.5-Coder technical report. arXiv preprint arXiv:2409.12186 , 2024.\n",
       "\n",
       "- An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122 , 2024b.\n",
       "- Shima Imani, Liang Du, and Harsh Shrivastava. MathPrompter: Mathematical reasoning using large language models. In Sunayana Sitaram, Beata Beigman Klebanov, and Jason D Williams, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track) , pages 37-42. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.acl-industry.4. URL https://aclanthology.org/2023.acl-industry.4 .\n",
       "- Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations , 2024b. URL https://openreview.net/forum?id=KUNzEQMWU7 .\n",
       "- Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. WizardMath: Empowering mathematical reasoning for large language models via Reinforced Evol-Instruct. arXiv preprint arXiv:2308.09583 , 2023.\n",
       "- Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview .net/forum?id=4WnqRR915j .\n",
       "- Tobias Nipkow, Markus Wenzel, and Lawrence C Paulson. Isabelle/HOL: a proof assistant for higher-order logic . Springer, 2002.\n",
       "- Leonardo de Moura and Sebastian Ullrich. The Lean 4 theorem prover and programming language. In André Platzer and Geoff Sutcliffe, editors, Automated Deduction - CADE 28 , pages 625-635, Cham, 2021. Springer International Publishing.\n",
       "- Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, and Yu Li. Lyra: Orchestrating dual correction in automated theorem proving. arXiv preprint arXiv:2309.15806 , 2023.\n",
       "- Haiming Wang, Huajian Xin, Chuanyang Zheng, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, and Xiaodan Liang. LEGO-prover: Neural theorem proving with growing libraries. In The Twelfth International Conference on Learning Representations , 2024a. URL https://openreview.net/forum?id=3f5PALef5B .\n",
       "- Christian Szegedy. A promising path towards autoformalization and general artificial intelligence. In Intelligent Computer Mathematics: 13th International Conference, CICM 2020, Bertinoro, Italy, July 26-31, 2020, Proceedings 13 , pages 3-20. Springer, 2020.\n",
       "- Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda Li, Mateja Jamnik, Guillaume Lample, and Yuhuai Wu. Draft, Sketch, and Prove: Guiding formal theorem provers with informal proofs. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=SMa9EAovKMC .\n",
       "- Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 1460514631. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.acl-long.817. URL https://aclanthology.org/2023.acl-long.817 .\n",
       "\n",
       "- Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223 , 2023.\n",
       "- Cedegao E Zhang, Katherine M Collins, Adrian Weller, and Joshua B Tenenbaum. AI for mathematics: A cognitive science perspective. arXiv preprint arXiv:2310.13021 , 2023a.\n",
       "- Stanislas Dehaene. The number sense: How the mind creates mathematics . Oxford University Press USA, 2011.\n",
       "- Lisa Feigenson, Stanislas Dehaene, and Elizabeth Spelke. Core systems of number. Trends in Cognitive Sciences , 8(7):307-314, 2004.\n",
       "- Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world GitHub issues? In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id= VTF8yNQM66 .\n",
       "- Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, and Henrique Ponde de Oliveira Pinto et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 , 2021.\n",
       "- Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, and Animashree Anandkumar. LeanDojo: Theorem proving with retrieval-augmented language models. Advances in Neural Information Processing Systems , 36, 2024c.\n",
       "- Peiyang Song, Kaiyu Yang, and Anima Anandkumar. Towards large language models as copilots for theorem proving in Lean, 2024b. URL https://arxiv.org/abs/2404.12534 .\n",
       "- Rose E Wang, Ana T Ribeiro, Carly D Robinson, Susanna Loeb, and Dora Demszky. Tutor CoPilot: A human-AI approach for scaling real-time expertise. arXiv preprint arXiv:2410.03017 , 2024b.\n",
       "- Patrick Massot. Talk on formalizing limits in Lean (video recording). Video recording of a talk, 2022. URL https://www.youtube.com/watch?v=1iqlhJ1-T3A . See around the 45:30 mark for the '256 definitions' and '4096 plumbing lemmas' discussion in the context of naive limit variants.\n",
       "- Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics. In International Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=9ZPegFuFTFv .\n",
       "- Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, and Jeremy Avigad. ProofNet: Autoformalizing and formally proving undergraduate-level mathematics. arXiv preprint arXiv:2302.12433 , 2023.\n",
       "- Peter Scholze. Liquid tensor experiment. Experimental Mathematics , 31(2):349-354, 2022.\n",
       "- Anthony Bordg, Lawrence Paulson, and Wenda Li. Simple type theory is not too simple: Grothendieck's schemes without dependent types. Experimental Mathematics , 31(2):364-382, 2022. doi: 10.1080/10586458 .2022.2062073. URL https://doi.org/10.1080/10586458.2022.2062073 .\n",
       "- Tomas Skrivan. Scilean: A scientific computing library in lean. https://github.com/lecopivo/SciLean , 2025.\n",
       "- Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, and Kai Chen. Lean workbook: A large-scale lean problem set formalized from natural language math problems. arXiv preprint arXiv:2406.03847 , 2024.\n",
       "- Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. OpenMathInstruct-1: A 1.8 million math instruction tuning dataset. arXiv preprint arXiv:2402.10176 , 2024.\n",
       "\n",
       "- Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Lluís Màrquez, Chris CallisonBurch, and Jian Su, editors, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1743-1752, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1202. URL https://aclanthology.org/D15-1202 .\n",
       "- Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies , pages 1152-1157, 2016.\n",
       "- Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Proceedings of the 2017 conference on empirical methods in natural language processing , pages 845-854, 2017.\n",
       "- Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 975-984, 2020.\n",
       "- Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 2080-2094. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.168.\n",
       "- Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229 , 2024.\n",
       "- Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, and Wei-Ying Ma. How well do computers solve math word problems? Large-scale dataset construction and evaluation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 887-896, 2016.\n",
       "- Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics , pages 158-167. Association for Computational Linguistics, 2017. doi: 10.18653/v1/P17-1015.\n",
       "- Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 3505-3523, 2022a.\n",
       "- Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evaluating the performance of large language models on GAOKAO benchmark. arXiv preprint arXiv:2305.12474 , 2023b.\n",
       "- Keiran Paster. Testing language models on a held-out high school national finals exam. https://huggingf ace.co/datasets/keirp/hungarian\\_national\\_hs\\_finals\\_exam , 2023.\n",
       "- Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015 , 2023.\n",
       "- TAL Education Group. TAL-SCQ5K-EN/TAL-SCQ5K-CN. https://github.com/math-eval/TAL-SCQ5K , 2023.\n",
       "- Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. AGIEval: A human-centric benchmark for evaluating foundation models. In Findings of the Association for Computational Linguistics: NAACL 2024 , pages 2299-2314, 2024.\n",
       "\n",
       "- Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. TheoremQA: A theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , pages 7889-7901, 2023.\n",
       "- Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J Nay, Kshitij Gupta, and Aran Komatsuzaki. ARB: Advanced reasoning benchmark for large language models. arXiv preprint arXiv:2307.13692 , 2023.\n",
       "- Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho. NaturalProofs: Mathematical theorem proving in natural language. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1) , 2021. URL https://openreview.net/forum?id=Jvxa8adr3iY .\n",
       "- Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. OlympiadBench: A challenging benchmark for promoting AGI with Olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 , 2024.\n",
       "- Simon Frieder, Mirek Olšák, Julius Berner, and Thomas Lukasiewicz. The IMO small challenge: Not-too-hard Olympiad math datasets for LLMs. In The Second Tiny Papers Track at ICLR 2024 , 2024.\n",
       "- David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations , 2019.\n",
       "- Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 , 2020.\n",
       "- Yuhuai Wu, Albert Qiaochu Jiang, Jimmy Ba, and Roger Grosse. INT: An inequality benchmark for evaluating generalization in theorem proving. arXiv preprint arXiv:2007.02924 , 2020.\n",
       "- Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. LILA: A unified benchmark for mathematical reasoning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , pages 5807-5832, Abu Dhabi, United Arab Emirates, December 2022b. Association for Computational Linguistics. doi: 10.18653/v1/20 22.emnlp-main.392. URL https://aclanthology.org/2022.emnlp-main.392 .\n",
       "- Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212 , 2023.\n",
       "- Donald T. Campbell. Assessing the impact of planned social change. Evaluation and Program Planning , 2(1): 67-90, January 1979. ISSN 0149-7189. doi: 10.1016/0149-7189(79)90048-x. URL http://dx.doi.org/10. 1016/0149-7189(79)90048-X .\n",
       "- Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. Dynabench: Rethinking benchmarking in NLP. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages 4110-4124, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.324. URL https://aclanthology.org/2021.naacl-main.324/ .\n",
       "- Simon Frieder, Martin Alawadhi, Trimmel, Rashid, and Klaus Gy. LLM vs ITP. In The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23 , 2023c.\n",
       "\n",
       "- Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. NaturalProver: grounded mathematical proof generation with language models. Advances in Neural Information Processing Systems , 35:4913-4927, 2022.\n",
       "- Wenhao Liu, Zhenyi Lu, Xinyu Hu, Jierui Zhang, Dailin Li, Jiacheng Cen, Huilin Cao, Haiteng Wang, Yuhan Li, Kun Xie, Dandan Li, Pei Zhang, Chengbo Zhang, Yuxiang Ren, Xiaohong Huang, and Yan Ma. Storm-born: A challenging mathematical derivations dataset curated via a human-in-the-loop multi-agent framework. arXiv preprint arXiv:2506.01531 , 2025. Findings of ACL 2025.\n",
       "- Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunović, Nikola Jovanović, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934 , 2025.\n",
       "- Ricardo Dominguez-Olmedo, Florian E Dorner, and Moritz Hardt. Training on the test task confounds evaluation and emergence. arXiv preprint arXiv:2407.07890 , 2024.\n",
       "- Alexandra N Uma, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara Plank, and Massimo Poesio. Learning from disagreement: A survey. Journal of Artificial Intelligence Research , 72:1385-1470, 2021.\n",
       "- Katherine M Collins, Umang Bhatt, and Adrian Weller. Eliciting and learning with soft labels from every annotator. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing , volume 10, pages 40-52, 2022.\n",
       "- Ilia Sucholutsky, Ruairidh M Battleday, Katherine M Collins, Raja Marjieh, Joshua Peterson, Pulkit Singh, Umang Bhatt, Nori Jacoby, Adrian Weller, and Thomas L Griffiths. On the informativeness of supervision signals. In Uncertainty in Artificial Intelligence , pages 2036-2046. PMLR, 2023.\n",
       "- Mitchell L Gordon, Michelle S Lam, Joon Sung Park, Kayur Patel, Jeff Hancock, Tatsunori Hashimoto, and Michael S Bernstein. Jury learning: Integrating dissenting voices into machine learning models. In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems , pages 1-19, 2022.\n",
       "- Guillaume Lample and François Charton. Deep learning for symbolic mathematics. In International Conference on Learning Representations , 2020. URL https://openreview.net/forum?id=S1eZYeHFDS .\n",
       "- Jacques Hadamard. Sur la distribution des zéros de la fonction ζ ( s ) et ses conséquences arithmétiques. Bulletin de la Société Mathématique de France , 24:199-220, 1896.\n",
       "- Atle Selberg. An elementary proof of the prime-number theorem. Annals of Mathematics , 50(2):305-313, 1949.\n",
       "- Paul Erdős. On a new method in elementary number theory which leads to an elementary proof of the prime number theorem. Proceedings of the National Academy of Sciences of the United States of America , 35(7): 374-384, 1949.\n",
       "- Roger B. Nelsen. Proofs without Words: Exercises in Visual Thinking . Mathematical Association of America, 1993.\n",
       "- Peter Selinger. A survey of graphical languages for monoidal categories. In New Structures for Physics , volume 813 of Lecture Notes in Physics , pages 289-355. Springer, 2011. Also available as arXiv:0908.3347.\n",
       "- Mark F. Bernstein. Mind of a mathematician. Princeton Alumni Weekly , November 2019. URL https: //paw.princeton.edu/article/mind-mathematician . Article recounts Terence Tao's initial reaction to Emmanuel Candès's compressed sensing problem, first doubting and then solving it.\n",
       "- Yinghui Li, Jiayi Kuang, Haojing Huang, Zhikun Xu, Xinnian Liang, Yi Yu, Wenlian Lu, Yangning Li, Xiaoyu Tan, Chao Qu, Ying Shen, Hai-Tao Zheng, and Philip S. Yu. One example shown, many concepts known! counterexample-driven conceptual reasoning in mathematical LLMs. arXiv preprint arXiv:2502.10454 , 2025a. ICML 2025.\n",
       "\n",
       "- Haocheng Ju and Bin Dong. Mirb: Mathematical information retrieval benchmark. arXiv preprint arXiv:2505.15585 , 2025.\n",
       "- Johannes Schmitt, Gergely Bérczi, Jasper Dekoninck, Jeremy Feusi, Tim Gehrunger, Raphael Appenzeller, Jim Bryan, Niklas Canova, Timo de Wolff, Filippo Gaia, Michel van Garrel, Baran Hashemi, David Holmes, Aitor Iribar Lopez, Victor Jaeck, Martina Jørgensen, Steven Kelk, Stefan Kuhlmann, Adam Kurpisz, Chiara Meroni, Ingmar Metzler, Martin Möller, Samuel Muñoz-Echániz, Robert Nowak, Georg Oberdieck, Daniel Platt, Dylan Possamaï, Gabriel Ribeiro, Raúl Sánchez Galán, Zheming Sun, Josef Teichmann, Richard P. Thomas, and Charles Vial. Improofbench: Benchmarking ai on research-level mathematical proof generation. arXiv preprint arXiv:2509.26076 , 2025.\n",
       "- Shubhra Mishra, Gabriel Poesia, Belinda Mo, and Noah D Goodman. MathCAMPS: Fine-grained synthesis of mathematical problems from human curricula. arXiv preprint arXiv:2407.00900 , 2024.\n",
       "- Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F Wong, Xiaowei Huang, Qiufeng Wang, and Kaizhu Huang. Is your model really a good math reasoner? evaluating mathematical reasoning with checklist. arXiv preprint arXiv:2407.08733 , 2024.\n",
       "- Emanuele La Malfa, Aleksandar Petrov, Simon Frieder, Christoph Weinhuber, Ryan Burnell, Raza Nazar, Anthony Cohn, Nigel Shadbolt, and Michael Wooldridge. Language-Models-as-a-Service: Overview of a new paradigm and its challenges. Journal of Artificial Intelligence Research , 80:1497-1523, 2024.\n",
       "- Jie Zhang, Cezara Petrui, Kristina Nikolić, and Florian Tramèr. Realmath: A continuous benchmark for evaluating language models on research-level mathematics. arXiv preprint arXiv:2505.12575 , 2025b. NeurIPS 2025.\n",
       "- Jasper Dekoninck, Ivo Petrov, Kristian Minchev, Mislav Balunovic, Martin Vechev, Miroslav Marinov, Maria Drencheva, Lyuba Konova, Milen Shumanov, Kaloyan Tsvetkov, Nikolay Drenchev, Lazar Todorov, Kalina Nikolova, Nikolay Georgiev, Vanesa Kalinkova, and Margulan Ismoldayev. The open proof corpus: A large-scale study of llm-generated mathematical proofs. arXiv preprint arXiv:2506.21621 , 2025.\n",
       "- Ronen Eldan and Yuanzhi Li. TinyStories: How small can language models be and still speak coherent English? arXiv preprint arXiv:2305.07759 , 2023.\n",
       "- Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of GPT-4. arXiv preprint arXiv:2306.02707 , 2023.\n",
       "- Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926 , 2023.\n",
       "- Patrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. Language models can teach themselves to program better. In The Eleventh International Conference on Learning Representations , 2023. URL https://openreview.net/forum?id=SaRj2ka1XZ3 .\n",
       "- Lawrence C Paulson and Kong Woei Susanto. Source-level proof reconstruction for interactive theorem proving. In International Conference on Theorem Proving in Higher Order Logics , pages 232-245. Springer, 2007.\n",
       "- Jia Meng and Lawrence C Paulson. Translating higher-order clauses to first-order clauses. Journal of Automated Reasoning , 40:35-60, 2008.\n",
       "- Jiewen Hu, Thomas Zhu, and Sean Welleck. miniCTX: Neural theorem proving with (long-)contexts, 2024. URL https://arxiv.org/abs/2408.03350 .\n",
       "- Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, and Yuhuai Wu. LISA: Language models of ISAbelle proofs. In 6th Conference on Artificial Intelligence and Theorem Proving , pages 378-392, 2021.\n",
       "\n",
       "- Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzygóźdź, Piotr Miłoś, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language models and automated theorem provers. Advances in Neural Information Processing Systems , 35:8360-8373, 2022.\n",
       "- Maciej Mikuła, Szymon Tworkowski, Szymon Antoniak, Bartosz Piotrowski, Albert Q. Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, and Yuhuai Wu. Magnushammer: A transformer-based approach to premise selection. In The Twelfth International Conference on Learning Representations , 2024. URL https://openreview.net/forum?id=oYjPk8mqAV .\n",
       "- Cameron Freer. lean4-skills: Lean 4 theorem-proving skill notes and reference patterns. https://github.c om/cameronfreer/lean4-skills , 2025. GitHub repository. Accessed: 2025-12-18.\n",
       "- Andrej Karpathy. makemore. https://github.com/karpathy/makemore , 2022.\n",
       "- Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages 5360-5370, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/d8e1344e27a 5b08cdfd5d027d9b8d6de-Abstract.html .\n",
       "- Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning, 2024. URL https://arxiv.org/abs/2403.04642 .\n",
       "- Lawrence C. Evans. Partial Differential Equations , volume 19 of Graduate Studies in Mathematics . American Mathematical Society, Providence, RI, 2010.\n",
       "- Robion Kirby. A calculus for framed links in S 3 . Invent. Math. , 45(1):35-56, 1978. ISSN 0020-9910,1432-1297. doi: 10.1007/BF01406222. URL https://doi.org/10.1007/BF01406222 .\n",
       "- András Juhász. Differential and low-dimensional topology , volume 104 of London Mathematical Society Student Texts . Cambridge University Press, Cambridge, 2023.\n",
       "- Kurt Reidemeister. Elementare begründung der knotentheorie. Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg , 5(1):24-32, 1927. doi: 10.1007/BF02952507. URL https://doi.org/ 10.1007/BF02952507 .\n",
       "- Sergei Gukov, James Halverson, Fabian Ruehle, and Piotr Sułkowski. Learning to unknot. Machine Learning: Science and Technology , 2(2):025035, apr 2021. doi: 10.1088/2632-2153/abe91f. URL https://dx.doi.org/10.1088/2632-2153/abe91f .\n",
       "- Marc Culler, Nathan M. Dunfield, Matthias Goerner, and Jeffrey R. Weeks. SnapPy, a computer program for studying the geometry and topology of 3-manifolds. Available at http://snappy.computop.org (18/12/2024), 2024.\n",
       "- Mikhail Khovanov. A categorification of the Jones polynomial. Duke Math. J. , 101(3):359-426, 2000. ISSN 0012-7094,1547-7398. doi: 10.1215/S0012-7094-00-10131-7. URL https://doi.org/10.1215/S0012-709 4-00-10131-7 .\n",
       "- Peter Ozsváth and Zoltán Szabó. Holomorphic disks and topological invariants for closed three-manifolds. Ann. of Math. (2) , 159(3):1027-1158, 2004. ISSN 0003-486X,1939-8980. doi: 10.4007/annals.2004.159.1027. URL https://doi.org/10.4007/annals.2004.159.1027 .\n",
       "- Edward Witten. Monopoles and four-manifolds. Math. Res. Lett. , 1(6):769-796, 1994. ISSN 1073-2780. doi: 10.4310/MRL.1994.v1.n6.a13. URL https://doi.org/10.4310/MRL.1994.v1.n6.a13 .\n",
       "\n",
       "- The Sage Developers. SageMath, the Sage Mathematics Software System (Version 10.5) , 2024. https://www.sagemath.org .\n",
       "- Robert Lipshitz and Sucharit Sarkar. Spatial refinements and Khovanov homology. In Proceedings of the International Congress of Mathematicians-Rio de Janeiro 2018. Vol. II. Invited lectures , pages 1153-1173. World Sci. Publ., Hackensack, NJ, 2018.\n",
       "- Sergei Gukov, James Halverson, Ciprian Manolescu, and Fabian Ruehle. Searching for ribbons with machine learning, 2023. URL https://arxiv.org/abs/2304.09304 .\n",
       "- Taylor Applebaum, Sam Blackwell, Alex Davies, Thomas Edlich, András Juhász, Marc Lackenby, Nenad Tomašev, and Daniel Zheng. The unknotting number, hard unknot diagrams, and reinforcement learning, 2024. URL https://arxiv.org/abs/2409.09032 .\n",
       "- Jacob Rasmussen. Khovanov homology and the slice genus. Invent. Math. , 182(2):419-447, 2010. ISSN 00209910,1432-1297. doi: 10.1007/s00222-010-0275-6. URL https://doi.org/10.1007/s00222-010-0275-6 .\n",
       "- Michael Hartley Freedman. The topology of four-dimensional manifolds. J. Differential Geometry , 17(3): 357-453, 1982. ISSN 0022-040X,1945-743X. URL http://projecteuclid.org/euclid.jdg/1214437136 .\n",
       "- Clifford Henry Taubes. The Seiberg-Witten invariants and symplectic forms. Math. Res. Lett. , 1(6):809-822, 1994. ISSN 1073-2780. doi: 10.4310/MRL.1994.v1.n6.a15. URL https://doi.org/10.4310/MRL.1994.v 1.n6.a15 .\n",
       "- Ronald Fintushel and Ronald J. Stern. Knots, links, and 4-manifolds. Invent. Math. , 134(2):363-400, 1998. ISSN 0020-9910,1432-1297. doi: 10.1007/s002220050268. URL https://doi.org/10.1007/s00222005026 8 .\n",
       "- John Morgan and Gang Tian. Ricci flow and the Poincaré conjecture , volume 3 of Clay Mathematics Monographs . American Mathematical Society, Providence, RI; Clay Mathematics Institute, Cambridge, MA, 2007.\n",
       "- Benjamin A. Burton. Introducing Regina, the 3-manifold topology software. Experiment. Math. , 13(3): 267-272, 2004. ISSN 1058-6458,1944-950X. URL http://projecteuclid.org/euclid.em/1103749834 .\n",
       "- J.W.S. Cassels, A. Frölich, London Mathematical Society, and International Mathematical Union. Algebraic Number Theory: Proceedings of an Instructional Conference. Edited by J. W. S. Cassels and A. Fröhlich . 1976. URL https://books.google.co.uk/books?id=0xltHQAACAAJ .\n",
       "- Louis H Kauffman and Sofia Lambropoulou. Hard unknots and collapsing tangles. Introductory lectures on knot theory, Ser. Knots Everything , 46:187-247, 2012.\n",
       "- C.H. Dowker and Morwen B. Thistlethwaite. Classification of knot projections. Topology and its Applications , 16(1):19-31, 1983. ISSN 0166-8641. doi: https://doi.org/10.1016/0166-8641(83)90004-4. URL https://www.sciencedirect.com/science/article/pii/0166864183900044 .\n",
       "- I. A. Dynnikov. Arc-presentations of links: Monotonic simplification. Fundamenta Mathematicae , 190(1): 29-76, 2006. URL http://eudml.org/doc/283163 .\n",
       "- L. H. Kauffman, N. E. Russkikh, and I. A. Taimanov. Rectangular knot diagrams classification with deep learning. Journal of Knot Theory and Its Ramifications , 31(11):2250067, 2022. doi: 10.1142/S02182165225 00675. URL https://doi.org/10.1142/S0218216522500675 .\n",
       "- Sergei Gukov, James Halverson, and Fabian Ruehle. Rigor with machine learning from field theory to the poincaréconjecture. Nature Reviews Physics , 6(5):310-319, 2024. doi: 10.1038/s42254-024-00709-0. URL https://doi.org/10.1038/s42254-024-00709-0 .\n",
       "\n",
       "- Zenan Li, Zhaoyu Li, Kaiyu Yang, Xiaoxing Ma, and Zhendong Su. Learning to disprove: Formal counterexample generation with large language models. OpenReview preprint , 2025b. ICLR 2026 submission.\n",
       "- Suozhi Huang, Peiyang Song, Robert Joseph George, and Anima Anandkumar. Leanprogress: Guiding search for neural theorem proving via proof progress prediction. arXiv preprint arXiv:2502.17925 , 2025.\n",
       "- Michael Kohlhase. OMDoc - An Open Markup Format for Mathematical Documents [version 1.2] , volume 4180 of Lecture Notes in Computer Science . Springer, 2006. doi: 10.1007/11826095. URL https: //doi.org/10.1007/11826095 .\n",
       "- Marc Berges, Jonas Betzendahl, Abhishek Chugh, Michael Kohlhase, Dominic Lohr, and Dennis Müller. Learning support systems based on mathematical knowledge management. In Catherine Dubois and Manfred Kerber, editors, Intelligent Computer Mathematics - 16th International Conference, CICM 2023, Cambridge, UK, September 5-8, 2023, Proceedings , volume 14101 of Lecture Notes in Computer Science , pages 84-97. Springer, 2023. doi: 10.1007/978-3-031-42753-4\\_6. URL https://doi.org/10.1007/97 8-3-031-42753-4\\_6 .\n",
       "- Hanoona Rasheed, Abdelrahman Shaker, Anqi Tang, Muhammad Maaz, Ming-Hsuan Yang, Salman Khan, and Fahad Shahbaz Khan. Videomathqa: Benchmarking mathematical reasoning via multimodal understanding in videos. arXiv preprint arXiv:2506.05349 , 2025.\n",
       "- George Pólya. With, or without, motivation? The American Mathematical Monthly , 56(10):684-691, 1949.\n",
       "- Rebecca Lea Morris. Motivated proofs: What are they, why they matter and how to write them. The Review of Symbolic Logic , 13(1):23-46, 2019. doi: 10.1017/S1755020319000583.\n",
       "- H. Kumar, David M. Rothschild, Daniel G. Goldstein, and Jake M. Hofman. Math education with large language models: Peril or promise? In Alexandra I. Cristea, Erin Walker, Yu Lu, Olga C. Santos, and Seiji Isotani, editors, Artificial Intelligence in Education (AIED 2025) , volume 15880 of Lecture Notes in Computer Science , Cham, 2025. Springer. doi: 10.1007/978-3-031-98459-4\\_5. URL https: //doi.org/10.1007/978-3-031-98459-4\\_5 .\n",
       "\n",
       "## A Appendix: Problems for Motivated Proof Experiments\n",
       "\n",
       "For the motivated proof experiments in 4.4, we used five problems including the two examples. These are presented how they were presented to the models, including the human-written motivated and unmotivated proof used for in-context examples.\n",
       "\n",
       "Cantor's theorem There is no surjection from a set X to its power set P ( X ).\n",
       "\n",
       "/negationslash\n",
       "\n",
       "MOTIVATED PROOF: Let f : X → P ( X ) be our function, we need to find some S ⊆ X not in the image of f . Not knowing which subset of X to take, we can treat the subset as an unknown, just as we do when solving an equation, and try to narrow down the possibilities. The most general subset of X can be expressed as { x ∈ X : P ( x ) } for some as yet unspecified property P . We now want to prove, for an arbitrary element y of X , that f ( y ) = { x ∈ X : P ( x ) } . To obtain our contradiction, we need either an element x of f ( y ) such that ¬ P ( x ) or an element x of the complement of f ( y ) such that P ( x ). There are not many elements around, so trying y is one of the first things to do, and then we find that we need either y ∈ f ( y ) and ¬ P ( y ) or y / ∈ f ( y ) and P ( y ). The choice P ( x ) = x / ∈ f ( x ) satisfies this, so the set S = { x ∈ X : x / ∈ f ( x ) } has the desired property.\n",
       "\n",
       "UNMOTIVATED PROOF: Let f : X → P ( X ) be our function, we claim that the set S = { x ∈ X : x / ∈ f ( x ) } is not in the image of f , hence f is not surjective. Suppose there exists y ∈ X such that f ( y ) = S . But then y ∈ f ( y ) iff y ∈ S iff y / ∈ f ( y ) by the definition of S, giving a contradiction.\n",
       "\n",
       "Small doubling There exists a subset A of the natural numbers with cardinality n, such that the set A + A = a 1 + a 2 : a i , a j ∈ A has cardinality 2 n -1.\n",
       "\n",
       "MOTIVATED PROOF: We don't know which set to take for A so we treat it as an unknown, just as we do when solving an equation, and try to narrow down the possibilities. Since we know that | A | = n , we can write A = { a 1 , ..., a n } for a 1 , ..., a n distinct, so we have A + A = a i + a j : i, j = 1 , ..., n . This isn't immediately helpful since we don't know how many collisions there will be in A + A , but we try to narrow down the possibilities. The simplest way to distinguish natural numbers is by ordering them, so we assume without loss of generality that a 1 &lt; ... &lt; a n . Applying this monotonicity to our sums, we deduce that a i + a j &lt; a i + a k whenever j &lt; k .\n",
       "\n",
       "Trying to distinguish as many elements as possible, we fix some i and take this inequality to its logical conclusion, finding that a 1 + a i &lt; .... &lt; a n + a i , or that we have n distinct elements. However, we can further extend this chain to the left if 1 &lt; i or the right if i &lt; n , giving us the longer chain a 1 + a 1 &lt; ... &lt; a 1 + a i &lt; ... &lt; a n + a 1 &lt; ... &lt; a n + a n , which has 2 n -1 distinct elements. If we want to have | A + A | = 2 n -1, then these are all the elements of A + A , but we still have this free parameter i . Repeating the construction with some j &gt; i and observing the first syntactic different element, we notice that a 1 + a i +1 = a 2 + a i , and isolating the variable i we notice that a 2 -a 1 = a i +1 -a i . This defines an arithmetic progression with first element a 1 and common difference a 2 -a 1 , so let's see if that enough.\n",
       "\n",
       "Suppose A is an arithmetic progression of length n , then A = a, a + d, . . . , a +( n -1) d , then A + A = 2 a, 2 a + d, . . . , 2 a +(2 n -2) d has size 2 n -1, completing the proof.\n",
       "\n",
       "UNMOTIVATED PROOF: Let A = { 1 , ..., n } , which has cardinality n, then A + A = { 2 , ..., 2 n } has cardinality 2 n -1, completing the proof.\n",
       "\n",
       "Integer sums There is a polynomial P ( x ) with rational coefficients such that P ( n ) = ∑ n k =0 k for all non-negative integers n .\n",
       "\n",
       "MOTIVATED PROOF: We can write a generic polynomial as P ( n ) = ∑ m j =0 a j n j . However, since m is unknown the equation is reasonably unwieldy. Consequently, we wish to find an upper bound on the degree of P ( n ). We do this by applying the trivial bound, P ( n ) = ∑ n k =0 k ≤ ∑ n k =0 n = n 2 + n , and since this equation holds for arbitrarily large n , we have that m = deg P ≤ 2, so we can write P ( n ) = a 0 + a 1 n + a 2 n 2 . To find\n",
       "\n",
       "a 0 , a 1 and a 2 , we can substitute some small values of n , so we have a 0 = P (0) = 0, a 0 + a 1 + a 2 = P (1) = 1 and a 0 +2 a 1 +4 a 2 = P (2) = 3. Solving the resulting linear equation, we have that a 0 = 0, a 1 = a 2 = 1 / 2 so P ( n ) = 1 2 ( n 2 + n ).\n",
       "\n",
       "To confirm that this works for all n , we can use induction. We have checked the base case already so we only need to do the induction step, so suppose that ∑ n k =0 k = 1 2 ( n 2 + n ), then we need to show that ∑ n +1 k =0 k = 1 2 (( n +1) 2 +( n +1)). In order to use the induction assumption, we split the LHS to give us ∑ n k =0 k +( n +1) and substitute in the induction assumption. By expanding and simplifying, we find that both sides of the equation are equal, completing our proof.\n",
       "\n",
       "UNMOTIVATED PROOF: We show that P ( n ) = 1 2 n ( n +1) works using induction. First we observe that P (0) = 0 = ∑ 0 k =0 k . Then suppose that ∑ n k =0 k = 1 2 n ( n +1), then we have that ∑ n +1 k =0 k = ∑ n k =0 k +( n +1) = 1 2 n ( n +1) + ( n +1) = ( n +1)( n 2 +1) = 1 2 ( n +1)( n +2), completing the proof.\n",
       "\n",
       "Nilpotent units Let R be a commutative ring, and let x ∈ R be nilpotent. Then (1 + x ) is a unit.\n",
       "\n",
       "MOTIVATED PROOF: To show that 1 + x is a unit, we need to find an inverse element. Not knowing which element to take, we parametrize the most generic element we can. Since the only known elements of R are 1 and x , the most generic element is an integer polynomial in x , ∑ m k =0 a k x k . For this to be a right inverse of 1 + x , we must have that 1 = (1 + x )( ∑ m k =0 a k x k ) = a 0 + ∑ m k =1 ( a k + a k -1 ) x k + a m x m +1 . For this to hold, we must eliminate all coefficients of the polynomial besides the constant term, which should be 1. This gives us that a 0 = 1, a k = a k -1 for 1 ≤ k ≤ m and a m = 0. The first two equations give us that a k = ( -1) k for all k , but this contradicts the last equation.\n",
       "\n",
       "But we also know that x is nilpotent, so if we have r such that x r = 0, then all coefficients from x r onwards can be ignored. This solves our issue, as we can let m = r -1 to remove the a m = 0 condition, and we are left with a k = ( -1) k , so (1 + x ) is a unit with inverse ∑ r -1 k =0 ( -1) k · x k .\n",
       "\n",
       "UNMOTIVATED PROOF: Let r be such that x r = 0 from nilpotency, and observe that (1 + x )( ∑ r -1 k =0 ( -1) k · x k ) = 1 + -x r = 1 by the formula for summing geometric progressions, hence ∑ r -1 k =0 ( -1) k · x k is an inverse of 1 + x and 1 + x is a unit.\n",
       "\n",
       "Large totients We define Euler's totient function φ as follows. If n = ∏ m i =1 p k i i is its prime factorisation, then φ ( n ) = ∏ m i =1 p k i -1 i ( p i -1). Show that for all /epsilon1 &gt; 0, φ ( n ) /n can take values in (1 -/epsilon1, 1).\n",
       "\n",
       "MOTIVATED PROOF: We first try to simplify φ ( n ) /n . Expressing n in its prime factorisation to match the definition of φ ( n ), we have φ ( n ) /n = ∏ m i =1 ( p k i -1 i ( p i -1) /p k i i ) = ∏ m i =1 (1 -1 /p i ). We notice that each factor is between 0 and 1, so the product is large where there are few factors so we let m = 1 (or equivalently, letting n = p k ), giving us φ ( n ) /n = (1 -1 /p ). Since this is clearly less than 1, we only need to choose p such that 1 -/epsilon1 &lt; 1 -1 /p , which simplifies easily to p &gt; 1 //epsilon1 . Since there are infinitely many prime numbers, we can always choose such a p .\n",
       "\n",
       "UNMOTIVATED PROOF: Let p be the smallest prime number such that p &gt; 1 //epsilon1 , which exists as there are infinitely many primes. Then we have φ ( p ) /p = 1 -1 /p , and as p &gt; 1 //epsilon1 , we see that 1 -/epsilon1 &lt; 1 -1 /p &lt; 1 as required."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f908d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76d0d3eb-8af4-432f-a207-728ff62358ee",
   "metadata": {},
   "source": [
    "### UnstructuredLoader\n",
    "- 다양한 비정형 문서들을 읽어 오는 Unstrctured 를 사용해, 다양한 형식의 문서들을 load 해 RAG, 모델 파인튜닝에 적용할 수있게 한다.\n",
    "  - 지원 파일 형식: \"csv\", \"doc\", \"docx\", \"epub\", \"image\", \"md\", \"msg\", \"odt\", \"org\", \"pdf\", \"ppt\", \"pptx\", \"rtf\", \"rst\", \"tsv\", \"xlsx\"\n",
    "- **다양한 형식의 파일로 부터 text를 로딩**해야 할 경우 유용하다. \n",
    "- Local에 library를 설치해서 사용하거나,  Unstructured 가 제공하는 API service를 사용할 수 있다.\n",
    "  - https://docs.unstructured.io\n",
    "- 텍스트 파일, PDF, 이미지, HTML, XML, ms-office(word, ppt), epub 등 다양한 비정형 데이터 파일을 처리할 수 있다.\n",
    "  - 설치, 지원 문서: https://docs.unstructured.io/open-source/installation/full-installation\n",
    "  - Langchain 문서: https://python.langchain.com/docs/integrations/document_loaders/unstructured_file\n",
    "\n",
    "> - UnstructuredLoader PDF Load 시 Document 분할 기준\n",
    ">     -  문서의 구조와 콘텐츠를 기반으로 텍스트를 분할해 Document에 넣는다.\n",
    ">     -  분할 기준\n",
    ">        - 헤더(Header): 문서의 제목이나 섹션 제목 등\n",
    ">        - 본문 텍스트(NarrativeText): 일반적인 문단이나 설명문\n",
    ">        - 표(Table): 데이터가 표 형식으로 구성된 부분\n",
    ">        - 리스트(List): 순서가 있거나 없는 목록\n",
    ">        - 이미지(Image): 사진이나 그래픽 요소"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87686d9-03d9-401a-9573-d57a2aacf965",
   "metadata": {},
   "source": [
    "#### 설치할 프로그램\n",
    "- poppler\n",
    "  - pdf 파일을 text로 변환하기 위해 필요한 프로그램\n",
    "  - windows: https://github.com/oschwartz10612/poppler-windows/releases/ 에서 최신 버전 다운로드 후 압축 풀어서 설치.\n",
    "    - 환경변수 Path에 \"설치경로\\Library\\bin\" 을 추가. (설치 후 IDE를 다시 시작한다.)\n",
    "  - macOS: `brew install poppler`\n",
    "  - Linux: `sudo apt-get install poppler-utils`\n",
    "- tesseract-ocr\n",
    "  - OCR 라이브러리로 pdf 이미지를 text로 변환하기 위해 필요한 프로그램 \n",
    "  - windows: https://github.com/UB-Mannheim/tesseract/wiki 에서 다운받아 설치. \n",
    "    - 환경변수 Path에 설치 경로(\"C:\\Program Files\\Tesseract-OCR\") 추가 한다. (설치 후 IDE를 다시 시작한다.)\n",
    "  - macOS: `brew install tesseract`\n",
    "  - linux(unbuntu): `sudo apt install tesseract-ocr`\n",
    "- 설치 할 패키지\n",
    "  - **libmagic 설치**\n",
    "      - windows: `pip install python-magic-bin -qU`\n",
    "      - macOS: `brew install libmagic`\n",
    "      - linux(ubuntu): `sudo apt-get install libmagic-dev`\n",
    "  - `pip install \"unstructured[pdf]\" -qU`\n",
    "      - 문서 형식별로 sub module을 설치한다. (pdf, docx ..)\n",
    "      - 모든 sub module 설치: `pip install unstructured[all-docs]`\n",
    "      - https://docs.unstructured.io/open-source/installation/full-installation\n",
    "  - `pip install langchain-unstructured -qU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98a32d3-b64c-427f-8663-86e00ee88f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 38ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 68ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 51ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-magic-bin\u001b[0m\u001b[2m==0.4.14\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install python-magic-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372606d9-8859-4d08-a488-dcd5e81fe9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m122 packages\u001b[0m \u001b[2min 826ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m fonttools \u001b[2m(2.2MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m rapidfuzz \u001b[2m(1.5MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m onnxruntime \u001b[2m(12.8MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m matplotlib \u001b[2m(7.8MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pikepdf \u001b[2m(3.6MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pi-heif \u001b[2m(1.8MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m grpcio \u001b[2m(4.5MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m onnx \u001b[2m(15.7MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m unstructured \u001b[2m(1.7MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m rapidfuzz\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pi-heif\n",
      "   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m langdetect\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pikepdf\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m grpcio\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m unstructured\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m fonttools\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m onnxruntime\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m matplotlib\n",
      "      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m langdetect\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m onnx\n",
      "\u001b[2mPrepared \u001b[1m41 packages\u001b[0m \u001b[2min 19.14s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m49 packages\u001b[0m \u001b[2min 3.76s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiofiles\u001b[0m\u001b[2m==25.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbackoff\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcoloredlogs\u001b[0m\u001b[2m==15.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdeprecated\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1meffdet\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1memoji\u001b[0m\u001b[2m==2.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mflatbuffers\u001b[0m\u001b[2m==25.12.19\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.61.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-api-core\u001b[0m\u001b[2m==2.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-cloud-vision\u001b[0m\u001b[2m==3.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogleapis-common-protos\u001b[0m\u001b[2m==1.72.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgrpcio\u001b[0m\u001b[2m==1.76.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgrpcio-status\u001b[0m\u001b[2m==1.76.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhtml5lib\u001b[0m\u001b[2m==1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhumanfriendly\u001b[0m\u001b[2m==10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangdetect\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkdown\u001b[0m\u001b[2m==3.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.8\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mml-dtypes\u001b[0m\u001b[2m==0.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmsoffcrypto-tool\u001b[0m\u001b[2m==5.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnltk\u001b[0m\u001b[2m==3.9.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1molefile\u001b[0m\u001b[2m==0.47\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1monnx\u001b[0m\u001b[2m==1.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1monnxruntime\u001b[0m\u001b[2m==1.23.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpdf2image\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpi-heif\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpikepdf\u001b[0m\u001b[2m==10.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mproto-plus\u001b[0m\u001b[2m==1.27.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpycocotools\u001b[0m\u001b[2m==2.0.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpypandoc\u001b[0m\u001b[2m==1.16.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.2.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyreadline3\u001b[0m\u001b[2m==3.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-iso639\u001b[0m\u001b[2m==2025.11.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-magic\u001b[0m\u001b[2m==0.4.27\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-multipart\u001b[0m\u001b[2m==0.0.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-oxmsg\u001b[0m\u001b[2m==0.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrapidfuzz\u001b[0m\u001b[2m==3.14.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtimm\u001b[0m\u001b[2m==1.0.22\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1munstructured\u001b[0m\u001b[2m==0.18.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1munstructured-client\u001b[0m\u001b[2m==0.42.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1munstructured-inference\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1munstructured-pytesseract\u001b[0m\u001b[2m==0.3.15\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebencodings\u001b[0m\u001b[2m==0.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwrapt\u001b[0m\u001b[2m==2.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxlrd\u001b[0m\u001b[2m==2.0.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install unstructured[all-docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c52fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m41 packages\u001b[0m \u001b[2min 121ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m onnxruntime \u001b[2m(10.6MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m onnxruntime\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 528ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 104ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 492ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-unstructured\u001b[0m\u001b[2m==1.0.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1monnxruntime\u001b[0m\u001b[2m==1.23.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1monnxruntime\u001b[0m\u001b[2m==1.19.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install langchain-unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0953ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "INFO: pikepdf C++ to Python logger bridge initialized\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    }
   ],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "path = ['data/olympic_wiki.md', 'data/novel/메밀꽃_필_무렵_이효석.pdf']\n",
    "\n",
    "loader=UnstructuredLoader(path)\n",
    "docs = loader.load() # 문단 단위로 문서를 split 해서 Document에 넣어 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e10ae81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134e073d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/olympic_wiki.md',\n",
       " 'category_depth': 0,\n",
       " 'languages': ['kor'],\n",
       " 'file_directory': 'data',\n",
       " 'filename': 'olympic_wiki.md',\n",
       " 'filetype': 'text/markdown',\n",
       " 'last_modified': '2025-12-12T17:39:02',\n",
       " 'category': 'Title',\n",
       " 'element_id': '869efdd92ae840d110075ad507174066'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "940e1e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올림픽(영어: Olympic Games, 프랑스어: Jeux olympiques)은 전 세계 각 대륙 각국에서 모인 수천 명의 선수가 참가해 여름과 겨울에 스포츠 경기를 하는 국제적인 대회이다. 전 세계에서 가장 큰 지구촌 최대의 스포츠 축제인 올림픽은 세계에서 가장 인지도있는 국제 행사이다. 올림픽은 2년마다 하계 올림픽과 동계 올림픽이 번갈아 열리며, 국제 올림픽 위원회(IOC)가 감독하고 있다. 또한 오늘날의 올림픽은 기원전 8세기부터 서기 5세기에 이르기까지 고대 그리스 올림피아에서 열렸던 올림피아 제전에서 비롯되었다. 그리고 19세기 말에 피에르 드 쿠베르탱 남작이 고대 올림피아 제전에서 영감을 얻어, 근대 올림픽을 부활시켰다. 이를 위해 쿠베르탱 남작은 1894년에 IOC를 창설했으며, 2년 뒤인 1896년에 그리스 아테네에서 제 1회 올림픽이 열렸다. 이때부터 IOC는 올림픽 운동의 감독 기구가 되었으며, 조직과 활동은 올림픽 헌장을 따른다. 오늘날 전 세계 대부분의 국가에서 올림픽 메달은 매우 큰 영예이며, 특히 올림픽 금메달리스트는 국가 영웅급의 대우를 받으며 스포츠 스타가 된다. 국가별로 올림픽 메달리스트들에게 지급하는 포상금도 크다. 대부분의 인기있는 종목들이나 일상에서 쉽게 접하고 즐길 수 있는 생활스포츠 종목들이 올림픽이라는 한 대회에서 동시에 열리고, 전 세계 대부분의 국가 출신의 선수들이 참여하는 만큼 전 세계 스포츠 팬들이 가장 많이 시청하는 이벤트이다. 2008 베이징 올림픽의 모든 종목 누적 시청자 수만 47억 명에 달하며, 이는 인류 역사상 가장 많은 수의 인구가 시청한 이벤트였다.\n"
     ]
    }
   ],
   "source": [
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a8da213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14. ↑ 念, 무엇을 하려고 하는 생각이나 마음 15. ↑ 흐뭇하게 16. ↑ ⼟房, 방에 들어가는 문 앞에 약간 높게 다진 흙바닥 17. ↑ 집안의 살림을 팔려고 나가야 할 18. ↑ 한 장날에서 다음 장날 사이를 세는 단위 19. ↑ 恒⽤, 흔히 늘 20. ↑ 사시장천(四時⻑天), 사계절 쉬지 않고 연달아 21. ↑ 견디기가 힘들고 고단하여 22. ↑ 돈망나니, 돈이라면 사족을 못 쓰고 못된 짓을 하는 사람 23. ↑ 나이로는 24. ↑ 철듦 25. ↑ \"가볍게\"의 방언 26. ↑ 다 자란 암말. 빈마(牝⾺)라고도 한다. 27. ↑ 어둠의 귀신, 어두워서 사물을 제대로 분간하지 못하\n"
     ]
    }
   ],
   "source": [
    "print(docs[-20].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acfcbe1-cc26-4e87-8c41-d6fa7d461701",
   "metadata": {},
   "source": [
    "### Directory 내의 문서파일들 로딩\n",
    "- DirectoryLoader 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b8eb4d8-3c1d-418d-a499-ee181d54b759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/21 [00:00<?, ?it/s]WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "  5%|▍         | 1/21 [00:00<00:19,  1.02it/s]WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      " 10%|▉         | 2/21 [00:01<00:15,  1.21it/s]WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      " 14%|█▍        | 3/21 [00:02<00:15,  1.16it/s]WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      " 19%|█▉        | 4/21 [00:03<00:16,  1.01it/s]WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      " 24%|██▍       | 5/21 [00:05<00:18,  1.13s/it]WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      " 29%|██▊       | 6/21 [00:06<00:16,  1.12s/it]WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      " 33%|███▎      | 7/21 [00:07<00:15,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9/21 [00:37<01:25,  7.16s/it]WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      " 48%|████▊     | 10/21 [00:38<00:58,  5.31s/it]WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      "WARNING: Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n",
      " 52%|█████▏    | 11/21 [00:39<00:40,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 12/21 [00:42<00:32,  3.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 13/21 [00:46<00:29,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 14/21 [00:55<00:38,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Cannot set gray stroke color because /'P4' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P4' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P6' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P6' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P8' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P8' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P9' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P9' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P10' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P10' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P14' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P14' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P15' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P15' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P21' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P21' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P22' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P22' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P23' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P23' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P4' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P4' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P5' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P5' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P4' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P4' is an invalid float value\n",
      "WARNING: Cannot set gray stroke color because /'P4' is an invalid float value\n",
      "WARNING: Cannot set gray non-stroke color because /'P4' is an invalid float value\n",
      " 71%|███████▏  | 15/21 [00:56<00:24,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 16/21 [00:57<00:15,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 17/21 [00:58<00:10,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 18/21 [01:00<00:07,  2.44s/it]WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "WARNING: libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [01:01<00:00,  2.91s/it]\n"
     ]
    }
   ],
   "source": [
    "# Unstructured 기반 - 관련 lib가 설치 되어 있어야 한다.\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path=\"./data\", # 문서를 찾을 root directory\n",
    "    glob=[\"*.docx\",\"*.pdf\",\"*.txt\"], # 찾을 문서 파일 명의 패턴을 glob 패턴으로 지정.\n",
    "    recursive=True, #False : Path 경로에서만 찾는다. True:Path 의 하위 경로도 모두 찾는다.\n",
    "    show_progress=True, # 진행 프로그래스바가 나온다.\n",
    ")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ebc232-48ae-4cd2-ab3a-f6e26bd95ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tqdm\n",
      "Version: 4.67.1\n",
      "Location: C:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\n",
      "Requires: colorama\n",
      "Required-by: docling, docling-ibm-models, huggingface-hub, mpire, nltk, openai, rapidocr, semchunk, transformers, unstructured\n"
     ]
    }
   ],
   "source": [
    "!uv pip show tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04a1e8eb-2569-43f3-a458-dd020a322c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data\\\\novel\\\\금_따는_콩밭_김유정.pdf'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c89b81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "금 따는 콩밭\n",
      "\n",
      "Exported from Wikisource on 2024년 11월 24일\n",
      "\n",
      "2\n",
      "\n",
      "위키백과에 이 글 과 관련된 자료가 있습니다. 금 따는 콩밭\n",
      "\n",
      "🙝🙟 땅속 저 밑은 늘 음침하 다.\n",
      "\n",
      "위키백과\n",
      "\n",
      "고달픈 간드렛불, 맥없이 푸르끼하다.\n",
      "\n",
      "밤과 달라서 낮엔 되우 흐릿하였다.\n",
      "\n",
      "겉으로 황토 장벽으로 앞뒤좌우가 콕 막힌 좁직한 구뎅이. 흡사히 무덤 속같이 귀중중하다. 싸늘한 침묵, 쿠더브레한 흙내와 징그러운 냉기만이 그 속에 자욱하다.\n",
      "\n",
      "곡괭이는 뻔질 흙을 이르집는다. 암팡스러이 내려쪼며,\n",
      "\n",
      "퍽 퍽 퍼억.\n",
      "\n",
      "이렇게 메떨어진 소리뿐. 그러나 간간 우수수 하고 벽이 헐 린다.\n",
      "\n",
      "영식이는 일손을 놓고 소맷자락을 끌어당기어 얼굴의 땀을 훑는다. 이놈의 줄이 언제나 잡힐는지 기가 찼다. 흙 한줌을 집어 코밑에 바짝 들여대고 손가락으로 샅샅이 뒤져본다. 완 연히 버력은 좀 변한 듯싶다. 그러나 불통버력이 아주 다 풀 린 것도 아니었다. 밀똥버력이라야 금이 온다는데 왜 이리 안 나오는지.\n",
      "\n",
      "곡괭이를 다시 집어든다. 땅에 무릎을 꿇고 궁뎅이를 번쩍 든 채 식식거린다. 곡괭이는 무작정 내려찍는다. 바닥에서\n",
      "\n",
      "3\n",
      "\n",
      "물이 스미어 무르팍이 흔건히 젖었다. 굿엎은 천판에서 흙방 울은 내리며 목덜미로 굴러든다. 어떤 때에는 웃벽의 한쪽이 떨어지며 등을 탕 때리고 부서진다.\n",
      "\n",
      "그러나 그는 눈도 하나 깜짝하지 않는다. 금을 캔다고 콩밭 하나를 다 잡쳤다. 약이 올라서 죽을둥 살둥 눈이 뒤집힌 이 판이다. 손바닥에 침을 탁 뱉고 곡괭이 자루를 한번 꼰아잡 더니 쉴 줄 모른다.\n",
      "\n",
      "등뒤에서는 흙 긁는 소리가 드윽드윽 난다. 아직도 버력을 다 못 친 모양. 이 자식이 일을 하나 시졸 하나. 남은 속이 바 직바직 타는데 웬 뱃심이 이리도 좋아.\n",
      "\n",
      "영식이는 살기 띤 시선으로 고개를 돌렸다. 암 말 없이 수재 를 노려본다. 그제야 꾸물꾸물 바지게에 흙을 담고 등에 메 고 사다리를 올라간다.\n",
      "\n",
      "굿이 풀리는지 벽이 우찔하였다. 흙이 부서져 내린다. 전날 이라면 이곳에서 아내 한번 못하고 생죽음이나 안 할까 털끝 까지 쭈볏할 게다. 그러나 이젠 그렇게 되고도 싶다. 수재란 놈하고 흙더미에 묻히어 한껍에 죽는다면 그게 오히려 날 게 다.\n",
      "\n",
      "이렇게까지 몹시 몹시 미웠다.\n",
      "\n",
      "이놈 풍치는 바람에 애꿎은 콩밭 하나만 결딴을 냈다. 뿐만 아니라 모두가 낭패다. 세 벌 논도 못 맸다. 논둑의 풀은 성 큼 자란 채 어지러이 널려 있다. 이 기미를 알고 지주는 대로 하였다. 내년부터는 농사질 생각을 말라고 발을 굴렀다. 땅 은 암만을 파도 지수가 없다.\n",
      "\n",
      "4\n",
      "\n",
      "이만해도 다섯 길은 훨썩 넘었으리라. 좀더 지펴야 옳을지 혹은 북으로 밀어야 옳을지, 우두머니 망설거린다. 금점 일 에는 푸뜸이다. 입때껏 수재의 지휘를 받아 일을 하여왔고, 앞으로도 역 그러해야 금을 딸 것이다. 그러나 그런 칙칙한 짓은 안 한다.\n",
      "\n",
      "“이리 와 이것 좀 파게.”\n",
      "\n",
      "그는 어쓴 위풍을 보이며 이렇게 분부하였다. 그리고 저는 일어나 손을 털며 뒤로 물러선다. 수재는 군말 없이 고분하 였다. 시키는 대로 땅에 무릎을 꿇고 벽채로 군버력을 긁어 낸 다음 다시 파기 시작한다.\n",
      "\n",
      "영식이는 치다 나머지 버력을 짊어진다. 커단 걸대를 뒤툭거 리며 사다리로 기어오른다. 굿문을 나와 버력더미에 흙을 마 악 내칠려 할 제,\n",
      "\n",
      "“왜 또 파. 이것들이 미쳤나 그래!”\n",
      "\n",
      "산에서 내려오는 마름과 맞닥뜨렸다. 정신이 떠름하여 그대 로 벙벙히 섰다. 오늘은 또 무슨 포악을 들을려는가.\n",
      "\n",
      "“말라니까 왜 또 파는 게야.” 하고 영식이의 바지게 뒤를 지 팡이로 콱 찌르더니,\n",
      "\n",
      "“갈아먹으라는 밭이지 흙 쓰고 들어가라는 거야, 이 미친것 들아. 콩밭에서 웬 금이 나온다구 이 지랄들이야 그래.” 하고 목에 핏대를 올린다. 밭을 버리면 간수 잘못한 자기 탓이다. 날마다 와서 그 북새를 피고 금하여도 담날 보면 또 여전히 파는 것이다.\n",
      "\n",
      "5\n",
      "\n",
      "“오늘로 이 구뎅이를 도로 묻어놔야지 낼로 당장 징역 갈 줄 알게.”\n",
      "\n",
      "너무 감정에 격하여 말도 잘 안 나오고 떠듬떠듬거린다. 주 먹은 곧 날아들 듯이 허구리게서 불불 떤다.\n",
      "\n",
      "“오늘만 좀 해보고 고만두겠어유.”\n",
      "\n",
      "영식이는 낯이 붉어지며 가까스로 한마디하였다. 그리고 무 턱대고 빌었다. 마름은 들은 척도 안하고 가버린다. 그 뒷모 양을 영식이는 멀거니 배웅하였다. 그러나 콩밭 낯짝을 들여 다보니 무던히 애통 터진다. 멀쩡한 밭에가 구멍이 사면 풍 풍 뚫렸다.\n",
      "\n",
      "예제없이 버력은 무데기 무데기 쌓였다. 마치 사태 만난 공 동 묘지와도 같이 귀살쩍고 되우 을씨년스럽다. 그다지 잘되 었던 콩 포기는 거반 버력더미에 다아 깔려버리고 군데군데 어쩌다 남은 놈들만이 고개를 나풀거린다. 그 꼴을 보는 것 도 자식 죽는 걸 보는 게 낫지 차마 못할 경상이었다.\n",
      "\n",
      "🙝🙟 농토는 모조리 떨어질 것이다. 그러나 대관절 올 밭도지 벼 두 섬 반은 뭘로 해내야 좋을지. 게다 밭을 망쳤으니 자칫하 면 징역을 갈는지도 모른다. 영식이가 구뎅이 안으로 들어왔 을 때 동무는 땅에 주저앉아 쉬고 있었다. 태연무심히 담배 만 뻑뻑 피는 것이다.\n",
      "\n",
      "“언제나 줄을 잡는 거야.”\n",
      "\n",
      "6\n",
      "\n",
      "“인제 차차 나오겠지.”\n",
      "\n",
      "“인제 나온다.” 하고 코웃음치고 엇먹더니 조금 지나매,\n",
      "\n",
      "“이 새끼.”\n",
      "\n",
      "흙덩이를 집어들고 골통을 내려친다.\n",
      "\n",
      "수재는 어쿠 하고 그대로 폭 엎드린다. 그러다 벌떡 일어선 다. 눈에 띄는 대로 곡괭이를 잡자 대뜸 달겨들었다. 그러나 강약이 부동. 왁살스러운 팔뚝에 튕겨져 벽에 가서 쿵 하고 떨어졌다. 그 순간에 제가 빼앗긴 곡괭이가 정백이를 겨누고 날아드는 걸 보았다. 고개를 홱 돌린다. 곡괭이는 흙벽을 퍽 찍고 다시 나간다.\n",
      "\n",
      "수재 이름만 들어도 영식이는 이가 갈렸다. 분명히 홀딱 속 은 것이다.\n",
      "\n",
      "영식이는 본디 금전에 이력이 없었다. 그리고 흥미도 없었 다. 다만 밭고랑에 웅크리고 앉아서 땀을 흘려가며 꾸벅꾸벅 일만 하였다. 올엔 콩도 뜻밖에 잘 열리고 맘이 좀 놓였다. 하루는 홀로 김을 매고 있노라니까,\n",
      "\n",
      "“여보게, 덥지 않은가. 좀 쉬었다 하게.”\n",
      "\n",
      "고개를 들어보니 수재다. 농사는 안 짓고 금전으로만 돌아다 니더니 무슨 바람에 또 왔는지 싱글벙글한다. 좋은 수나 걸 렸나 하고,\n",
      "\n",
      "“돈 좀 많이 벌었나. 나 좀 주게.”\n",
      "\n",
      "7\n",
      "\n",
      "“벌구 말구, 맘껏 먹고 맘껏 쓰고 했네.”\n",
      "\n",
      "술에 거나한 얼굴로 신껏 주적거린다. 그리고 밭머리에 쭈그 리고 앉아 한참 객설을 부리더니,\n",
      "\n",
      "“자네, 돈벌이 좀 안할려나. 이 밭에 금이 묻혔네 금이.”\n",
      "\n",
      "“뭐?” 하니까,\n",
      "\n",
      "바로 이 산 너머 큰골에 광산이 있다. 광부를 삼백여 명이나 부리는 노다지판인데 매일 소출되는 금이 칠십 냥을 넘는다. 돈으로 치면 칠천 원. 그 줄맥이 큰 산허리를 뚫고 이 콩밭으 로 뻗어나왔다는 것이다. 둘이서 파면 불과 열흘 안에 줄을 잡을 게고, 적어도 하루 서너 돈씩은 따리라.\n",
      "\n",
      "우선 삼십만 원만 해도 얼마냐. 소를 산대도 만 필이 아니냐 고. 그러나 영식이는 귀담아듣지 않았다. 금점이란 칼 물고 뜀뛰기다, 잘되면이어니와 못되면 신세만 조핀다, 이렇게 전 일부터 들은 소리가 있어서였다. 그 담날도 와서 꾀송거리다 갔다.\n",
      "\n",
      "셋째 번에는 집으로 찾아왔는데 막걸리 한 병을 손에 떡 들 고 영을 피운다. 몸이 달아서 또 온 것이었다. 봉당에 걸터앉 아서 저녁상을 물끄러미 바라보더니 조당수는 몸을 훑는다 는 둥 일꾼은 든든히 먹어야 한다는 둥 남들은 논을 사느니 밭을 사느니 떠드는데 요렇게 지내다 그만둘 테냐는 둥 일쩌 웁게 지껄인다.\n",
      "\n",
      "“아주머니, 이것 좀 먹게 해주시게유.”\n",
      "\n",
      "8\n",
      "\n",
      "그리고 비로소 영식이 아내에게 술병을 내놓는다. 그들은 밥 상을 끼고 앉아서 즐거웁게 술을 마셨다. 몇 잔이 들어가고 보니 영식이의 생각도 저으기 돌아섰다. 딴은 일년 고생하고 끽 콩 몇 섬 얻어먹느니보다는 금을 캐는 것이 슬기로운 짓 이다.\n",
      "\n",
      "하루에 잘만 캔다면 한 해 줄곧 공들인 그 수확보다 훨썩 이 익이다. 올 봄 보낼 제 비료값, 품삯, 빚해 빚진 칠 원 까닭에 나날이 졸리는 이판이다. 이렇게 지지하게 살고 말 바에는 차라리 가로지나 세로지나 사내자식이 한번 해볼 것이다.\n",
      "\n",
      "“내일부터 우리 파보세. 돈만 있으면이야 그까진 콩은…”\n",
      "\n",
      "수재가 안달스리 재우쳐 보채일 제 선뜻 응낙하였다.\n",
      "\n",
      "“그래 보세. 빌어먹을 거 안됨 고만이지.”\n",
      "\n",
      "그러나 꽁무니에서 죽을 마시고 있던 아내가 허구리를 쿡쿡 찔렀게 망정이지 그렇지 않았더면 좀 주저할 뻔도 하였다.\n",
      "\n",
      "아내는 아내대로의 심이 빨랐다. 시체는 금점이 판을 잡았 다. 섣부르게 농사만 짓고 있다간 결국 비렁뱅이밖에는 더 못된다. 얼마 안 있으면 산이고 논이고 밭이고 할 것 없이 다 금쟁이 손에 구멍이 뚫리고 뒤집히고 뒤죽박죽이 될 것이다. 그때는 뭘 파먹고 사나.\n",
      "\n",
      "자, 보아라. 머슴들은 짜위나 한 듯이 일하다 말고 후딱하면 금점으로들 내빼지 않는가. 일꾼이 없어서 올엔 농사를 질 수 없느니 마느니 하고 동리에서는 떠들썩하다. 그리고 번동 포농이 쫓아 호미를 내어던지고 강변으로 개울로 사금을 캐\n",
      "\n",
      "9\n",
      "\n",
      "러 달아난다. 그러나 며칠 뒤에는 다비신에다 옥당목을 떨치 고 히짜를 뽑는 것이 아닌가.\n",
      "\n",
      "🙝🙟 아내는 콩밭에서 금이 날 줄은 아주 꿈밖이었다. 놀라고도 또 기뻤다. 올해는 노냥 침만 삼키던 그놈 코다리(명태)를 짜 장 먹어보겠구나, 만 하여도 속이 메질 듯이 짜릿하였다. 뒷 집 양근댁은 금점 덕택에 남편이 사다준 흰 고무신을 신고 나릿나릿 걷는 것이 무척 부러웠다. 저도 얼른 금이나 펑펑 쏟아지면 흰 고무신도 신고 얼굴에 분도 바르고 하리라.\n",
      "\n",
      "“그렇게 해보지 뭐. 저 양반 하잔 대로만 하면 어련히 잘될라 구.”\n",
      "\n",
      "얼뚤하여 앉았는 남편을 이렇게 추겼던 것이다.\n",
      "\n",
      "동이 트기 무섭게 콩밭으로 모였다. 수재는 진언이나 하는 듯 이리대고 중얼거리고 저리대고 중얼거리고 하였다. 그리 고 덤벙거리며 이리 왔다가 저리 왔다가 하였다. 제 딴은 땅 속에 누운 줄맥을 어림하여 보는 맥이었다.\n",
      "\n",
      "한참을 밭을 헤매다가 산 쪽으로 붙은 한구석에 딱 서며 손 가락을 펴들고 설명한다. 큰 줄이란 본시 산운 산을 끼고 도 는 법이다. 이 줄이 노다지임에는 필시 이켠으로 버듬히 누 웠으리라. 그러니 여기서부터 파 들어가자는 것이었다.\n",
      "\n",
      "영식이는 그 말이 무슨 소린지 새기지는 못했다. 마는 금점 에는 난다는 수재이니 그 말대로 하기만 하면 영낙없이 금퇴\n",
      "\n",
      "10\n",
      "\n",
      "야 나겠지 하고 그것만 꼭 믿었다. 군말 없이 지시해 받은 곳 에다 삽을 폭 꽂고 파헤치기 시작하였다.\n",
      "\n",
      "금도 금이면 애써 키워온 콩도 콩이었다. 거진 다 자란 허울 멀쑥한 놈들이 삽 끝에 으스러지고 흙에 묻히고 하는 것이 다. 그걸 보는 것은 썩 속이 아팠다. 애틋한 생각이 물밀 때 가끔 삽을 놓고 허리를 구부려서 콩잎의 흙을 털어주기도 하 였다.\n",
      "\n",
      "“아, 이 사람아, 맥적게 그건 봐 뭘해, 금을 캐자니깐.”\n",
      "\n",
      "“아니야, 허리가 좀 아파서!”\n",
      "\n",
      "핀잔을 얻어먹고는 좀 열쩍었다. 하기는 금만 잘 터져나오면 이까진 콩밭쯤이야. 이 밭을 풀어 논도 만들 수 있을 것이다. 눈을 감아버리고 삽의 흙을 아무렇게나 콩잎 위로 홱홱 내어 던진다.\n",
      "\n",
      "“구구루 땅이나 파먹지 이게 무슨 지랄들이야!”\n",
      "\n",
      "동리 노인은 뻔질 찾아와서 귀 거친 소리를 하고 하였다.\n",
      "\n",
      "밭에 구멍을 셋이나 뚫었다. 그리고 대구 뚫는 길이었다. 금 인가 난장을 맞을 건가 그것 때문에 농꾼은 버렸다. 이게 필 연코 세상이 망하려는 징조이리라. 그 소중한 밭에다 구멍을 뚫고 이 지랄이니 그놈이 온전할 겐가.\n",
      "\n",
      "노인은 제물 화에 지팡이를 들어 삿대질을 아니할 수 없었 다.\n",
      "\n",
      "11\n",
      "\n",
      "“벼락맞느니 벼락맞어.”\n",
      "\n",
      "“염려 말아유. 누가 알래지유.”\n",
      "\n",
      "영식이는 그럴 적마다 데퉁스리 쏘았다. 골김에 흙을 되는대 로 내꼰지고는 침을 탁 뱉고 구뎅이로 들어간다. 그러나 마 음 한구석에는 언제나 끄은하였다. 줄을 찾는다고 콩밭을 통 히 뒤집어놓았다. 그리고 줄이 언제나 나올지 아직 까맣다. 논도 못 매고 물도 못 보고 벼가 어이 되었는지 그것조차 모 른다. 밤에는 잠이 안 와 멀뚱하니 애를 태웠다.\n",
      "\n",
      "수재는 낙담하는 기색도 없이 늘 하냥이었다. 땅에 웅숭그리 고 시적시적 노량으로 땅만 판다.\n",
      "\n",
      "“줄이 꼭 나오겠나?” 하고 목이 말라서 물으면,\n",
      "\n",
      "“이번에 안 나오거든 내 목을 비게.” 서슴지 않고 장담을 하 고는 꿋꿋하였다.\n",
      "\n",
      "이걸 보면 영식이도 마음이 좀 뇌는 듯싶었다. 전들 금이 없 다면 무슨 멋으로 이 고생을 하랴. 반드시 금은 나올 것이다. 그제서는 이왕 손해는 하릴없거니와 고만두리라는 절망이 스스로 사라지고 다시금 주먹이 쥐어지는 것이었다.\n",
      "\n",
      "캄캄하게 밤은 어두웠다. 어디선가 뭇개가 요란히 짖어대인 다.\n",
      "\n",
      "남편은 진흙투성이를 하고 산에서 내려왔다. 풀이 죽어서 몸 을 잘 가누지도 못하고 아랫묵에 축 늘어진다.\n",
      "\n",
      "12\n",
      "\n",
      "이 꼴을 보니 아내는 맥이 다시 풀린다. 오늘도 또 글렀구나. 금이 터지며는 집을 한 채 사간다고 자랑을 하고 왔더니 이 내 헛일이었다. 인제 좌지가 나서 낯을 들고 나아갈 염의조 차 없어졌다.\n",
      "\n",
      "남편에게 저녁을 갖다주고 딱하게 바라본다.\n",
      "\n",
      "“인젠 꿔온 양식도 다 먹었는데…”\n",
      "\n",
      "“새벽에 산제를 좀 지낼 텐데 한번만 더 꿔와.”\n",
      "\n",
      "남의 말에는 대답 없고 유하게 흘개늦은 소리뿐 그리고 드러 누운 채 눈을 지그시 감아버린다.\n",
      "\n",
      "“죽거리두 없는데 산제는 무슨…”\n",
      "\n",
      "“듣기 싫어, 요망맞은 년 같으니.”\n",
      "\n",
      "이 호통에 아내는 고만 멈씰하였다. 요즘 와서는 무턱대고 공연스리 골만 내는 남편이 역 딱하였다. 환장을 하는지 밤 잠도 아니 자고 소리만 뻑뻑 지르며 덤벼들려고 든다. 심지 어 어린것이 좀 울어도 이 자식 갖다 내꾼지라고 북새를 피 는 것이다.\n",
      "\n",
      "저녁을 아니 먹으므로 그냥 치워버렸다. 남편의 영을 거역키 어려워 양근댁한테로 또다시 안 갈 수 없다. 그간 양식은 줄 곧 꾸어다먹고 갚지도 못하였는데 또 무슨 면목으로 입을 벌 릴지 난처한 노릇이었다.\n",
      "\n",
      "🙝🙟\n",
      "\n",
      "13\n",
      "\n",
      "그는 생각다 끝에 있는 염치를 보째 쏟아던지고 다시 한번 찾아가는 것이다. 마는 딱 맞닥뜨리어 입을 열고,\n",
      "\n",
      "“낼 산제를 지낸다는데 쌀이 있어야지유.” 하자니 역 낯이 화끈하고 모닥불이 날아든다.\n",
      "\n",
      "그러나 그들은 어지간히 착한 사람이었다.\n",
      "\n",
      "“암 그렇지요. 산신이 벗나면 죽도 글릅니다.” 하고 말을 받 으며 그 남편은 빙그레 웃는다. 워낙 이 금점에 장구 닳아난 몸인 만치 이런 일에는 적잖이 속이 틔었다. 손수 쌀 닷 되를 떠다주며,\n",
      "\n",
      "“산제란 안 지냄 몰라두 이왕 지낼려면 아주 정성껏 해야 됩 니다. 산신이란 노하길 잘하니까유.”\n",
      "\n",
      "하고 그 비방까지 깨쳐 보낸다.\n",
      "\n",
      "쌀을 받아들고 나오며 영식이 처는 고마움보다 먼저 미안에 질리어 얼굴이 다시 빨갰다. 그리고 그들 부부 살아가는 살 림이 참으로 참으로 몹시 부러웠다. 양근댁 남편은 날마다 금점으로 감돌며 버력더미를 뒤지고 토록을 줏어온다.\n",
      "\n",
      "그걸 온종일 장판돌에다 갈면 수가 좋으면 이삼 원, 옥아도 칠팔십 전 꼴은 매일 심이 되는 것이었다. 그러면 쌀을 산다, 피륙을 끊는다, 떡을 한다, 장리를 놓는다 - 그런데 우리는 왜 늘 요 꼴인지 생각만 하여도 가슴이 메이는 듯 맥맥한 한 숨이 연발을 하는 것이었다.\n",
      "\n",
      "14\n",
      "\n",
      "아내는 집에 돌아와 떡쌀을 담그었다. 낼은 뭘로 죽을 쑤어 먹을는지. 웃목에 웅크리고 앉아서 맞은쪽에 자빠져 있는 남 편을 곁눈으로 살짝 할퀴어본다. 남들은 돌아다니며 잘두 금 을 줏어오련만 저 망나니 제 밭 하나를 다 버려도 금 한 톨 못 줏어오나. 에에, 변변치도 못한 사나이. 저도 모르게 얕은 한숨이 거푸 두 번을 터진다.\n",
      "\n",
      "밤이 이슥하여 그들 양주는 떡을 하러 나왔다. 남편은 절구 에 쿵쿵 빻았다. 그러나 체가 없다. 동네로 돌아다니며 빌려 오느라고 아내는 다리에 불풍이 났다.\n",
      "\n",
      "“왜 이리 앉었수, 불 좀 지피지.”\n",
      "\n",
      "떡을 찧다가 얼이 빠져서 멍하니 앉았는 남편이 밉쌀스럽다. 남은 이래저래 애를 죄는데 저건 무슨 생각을 하고 저리 있 는 건지. 낫으로 삭정이를 탁탁 조겨서 던져주며 아내는 은 근히 훅닥이었다. 닭이 두 홰를 치고 나서야 떡은 되었다. 아 내는 시루를 이고 남편은 겨드랑이에 자리때기를 꼈다. 그리 고 캄캄한 산길을 올라간다.\n",
      "\n",
      "비탈길을 얼마 올라가서야 콩밭은 놓였다. 전면이 우뚝한 검 은 산에 둘리어 막힌 곳이었다. 가생이로 느티 대추나무들은 머리를 풀었다. 밭머리 조금 못미처 남편은 걸음을 멈추자 뒤의 아내를 돌아본다.\n",
      "\n",
      "“인내, 그리구 여기 가만히 섰어.”\n",
      "\n",
      "시루를 받아 한 팔로 껴안고 그는 혼자서 콩밭으로 올라섰 다. 앞에 쌓인 것이 모두 흙더미, 그 흙더미를 마악 돌아설려\n",
      "\n",
      "15\n",
      "\n",
      "할 제 아마 돌을 찼나보다. 몸이 쓰러지려고 우찔끈하니 아 내가 기겁을 하여 뛰어오르며 그를 부축하였다.\n",
      "\n",
      "“부정 타라구 왜 올라와, 요망맞은 년.”\n",
      "\n",
      "남편은 몸을 고루잡자 소리를 뻑 지르며 아내 얼뺨을 붙인 다. 가뜩이나 죽으라 죽으라 하는데 불길하게도 계집년이. 그는 마뜩지 않게 두덜거리며 밭으로 들어간다. 밭 한가운데 다 자리를 펴고 그 위에 시루를 놓았다. 그리고 시루 앞에다 공손하고 정성스레 재배를 커다랗게 한다.\n",
      "\n",
      "“우리를 살려줍시사. 산신께서 거들어주지 않으면 저희는 죽을 밖에 꼼짝 수 없읍니다유.”\n",
      "\n",
      "그는 손을 모으고 이렇게 축원하였다.\n",
      "\n",
      "아내는 이 꼴을 바라보며 독이 뾰록 같이 올랐다. 금점을 합 네 하고 금 한 톨 못 캐는 것이 버릇만 점점 글러간다. 그전 에는 없더니 요새로 건듯하면 탕탕 때리는 못된 버릇이 생긴 것이다. 금을 캐랬지 뺨을 치랬나. 제발 덕분에 고놈의 금 좀 나오지 말았으면. 그는 뺨 맞은 앙심으로 맘껏 방자하였다.\n",
      "\n",
      "하긴 아내의 말 고대로 되었다. 열흘이 썩 넘어도 산신은 깜 깜 무소식이었다. 남편은 밤낮으로 눈을 까뒤집고 구덩이에 묻혀 있었다. 어쩌다 집엘 내려오는 때이면 얼굴이 헐떡하고 어깨가 축 늘어지고 거반 병객이었다. 그리고서 잠자코 커단 몸집을 방고래에다 큉, 하고 내던지고 하는 것이다.\n",
      "\n",
      "“제이미 붙을, 죽어나 버렸으면.”\n",
      "\n",
      "16\n",
      "\n",
      "혹은 이렇게 탄식하기도 하였다.\n",
      "\n",
      "아내는 바가지에 점심을 이고서 집을 나섰다. 젖먹이는 등을 두드리며 좋다고 끽끽거린다.\n",
      "\n",
      "이젠 흰 고무신이고 코다리고 생각조차 물렸다. 그리고 금 하는 소리만 들어도 입에 신물이 날 만큼 되었다. 그건 고사 하고 꿔다먹은 양식에 졸리지나 말았으면 그만도 좋으리마 는.\n",
      "\n",
      "가을은 논으로 밭으로 누으렇게 내리었다. 농꾼들은 기꺼운 낯을 하고 서로 만나면 흥겨운 농담, 그러나 남편은 앰한 밭 만 망치고 논조차 건살 못하였으니 이 가을에는 뭘 거둬들이 고 뭘 즐겨할는지. 그는 동리 사람의 이목이 부끄러워 산길 로 돌았다.\n",
      "\n",
      "🙝🙟 솔숲을 나서서 멀리 밖에를 바라보니 둘이 다 나와 있다. 오 늘도 또 싸운 모양. 하나는 이쪽 흙더미에 앉았고 하나는 저 쪽에 앉았고. 서로들 외면하여 담배만 뻑뻑 피운다.\n",
      "\n",
      "“점심들 잡숫게유.”\n",
      "\n",
      "남편 앞에 바가지를 내려놓으며 가만히 맥을 보았다.\n",
      "\n",
      "남편은 적삼이 찢어지고 얼굴에 생채기를 내었다. 그리고 두 팔을 걷고 먼 산을 향하여 묵묵히 앉았다.\n",
      "\n",
      "17\n",
      "\n",
      "수재는 흙에 박혔다 나왔는지 얼굴은커녕 귓속드리 흙투성 이다. 코밑에는 피딱지가 말라붙었고 아직도 조금씩 피가 흘 러내린다. 영식이 처를 보더니 열쩍은 모양. 고개를 돌리어 모로 떨어치며 입맛만 쩍쩍 다신다.\n",
      "\n",
      "금을 캐라니까 밤낮 피만 내다 말라는가. 빚에 졸리어 남은 속을 볶는데 무슨 호강에 이지랄들인구. 아내는 못마땅하여 눈가에 살을 모았다.\n",
      "\n",
      "“산제 지낸다구 꿔온 것은 은제나 갚는다지유?”\n",
      "\n",
      "뚱하고 있는 남편을 향하여 말끝을 꼬부린다. 그러나 남편은 눈썹 하나 까딱하지 않는다. 이번에는 어조를 좀 돋으며,\n",
      "\n",
      "“갚지도 못할 걸 왜 꿔오라 했지유!” 하고 얼추 호령이었다.\n",
      "\n",
      "이 말은 남편의 채 가라앉지도 못한 분통을 다시 건드린다. 그는 벌떡 일어서며 황밤주먹을 쥐어 창낭할 만치 아내의 골 통을 후렸다.\n",
      "\n",
      "“계집년이 방정맞게.”\n",
      "\n",
      "다른 것은 모르나 주먹에는 아찔이었다. 멋없이 덤비다간 골 통이 부서진다. 암상을 참고 바르르 하다가 이윽고 아내는 등에 업은 언내를 끌러들었다. 남편에게로 그대로 밀어던지 니 아이는 까르륵 하고 숨 모는 소리를 친다. 그리고 아내는 돌아서서 혼잣말로,\n",
      "\n",
      "“콩밭에서 금을 딴다는 숭맥도 있담.” 하고 빗대놓고 비양거 린다.\n",
      "\n",
      "18\n",
      "\n",
      "“이년아, 뭐!”\n",
      "\n",
      "남편은 대뜸 달겨들며 그 볼치에다 다시 올찬 황밤을 주었 다. 저그나면 계집이니 위로도 하여주련만 요건 분만 폭폭 질러놓려나. 예이, 빌어먹을 거, 이판새판이다.\n",
      "\n",
      "“너허구 안 산다. 오늘루 가거라.”\n",
      "\n",
      "아내를 와락 떠다밀어 논뚝에 제켜놓고 그 허구리를 발길로 퍽 질렀다.\n",
      "\n",
      "아내는 입을 헉 하고 벌린다.\n",
      "\n",
      "“네가 허라구 옆구리를 쿡쿡 찌를 제는 은제냐, 요 집안 망할 년.”\n",
      "\n",
      "그리고 다시 퍽 질렀다. 연하여 또 퍽.\n",
      "\n",
      "이 꼴들을 보니 수재는 조바심이 일었다. 저러다가 그 분풀 이가 다시 제게로 슬그머니 옮아올 것을 지르채었다. 인제 걸리면 죽는다. 그는 비슬비슬하다 어느 틈엔가 구뎅이 속으 로 시나브로 없어져버린다. 볕은 다스로운 가을 향취를 풍긴 다. 주인을 잃고 콩은 무거운 열매를 둥글둥글 흙에 굴린다. 맞은쪽 산밑에서 벼들을 베며 기뻐하는 농꾼의 노래.\n",
      "\n",
      "“터졌네, 터져.”\n",
      "\n",
      "수재는 눈이 휘둥그렇게 굿문을 뛰어나오며 소리를 친다. 손 에는 흙 한줌이 잔뜩 쥐었다.\n",
      "\n",
      "19\n",
      "\n",
      "“뭐?” 하다가,\n",
      "\n",
      "“금줄 잡았어, 금줄.”\n",
      "\n",
      "“응!” 하고 외마디를 뒤남기자 영식이는 수재 앞으로 살같이 달려들었다. 허겁지겁 그 흙을 받아들고 샅샅이 헤쳐보니 딴 은 재래에 보지 못하던 불그죽죽한 황토이었다. 그는 눈에 눈물이 핑 돌며,\n",
      "\n",
      "“이게 원줄인가?”\n",
      "\n",
      "“그럼 이것이 곱색줄이라네. 한 포에 댓 돈씩은 넉넉잡히 대.”\n",
      "\n",
      "영식이는 기쁨보다 먼지 기가 탁 막혔다. 웃어야 옳을지 울 어야 옳을지. 다만 입을 반쯤 벌린 채 수재의 얼굴만 멍하니 바라본다.\n",
      "\n",
      "“이리 와봐. 이게 금이래.”\n",
      "\n",
      "이윽고 남편은 아내를 부른다. 그리고 내 뭐랬어, 그러게 해 보라고 그랬지, 하고 설면설면 덤벼오는 아내가 한결 어여뻤 다. 그는 엄지가락으로 아내의 눈물을 지워주고 그리고 나서 껑충거리며 구뎅이로 들어간다.\n",
      "\n",
      "“그 흙 속에 금이 있지요?”\n",
      "\n",
      "영식이처가 너무 기뻐서 코다리에 고래등 같은 집까지 연상 할 제 수재는 시원스러이,\n",
      "\n",
      "20\n",
      "\n",
      "“네, 한 포대에 오십 원씩 나와유.” 하고 대답하고 오늘밤에 는 꼭 정녕코 꼭 달아나리라 생각하였다.\n",
      "\n",
      "거짓말이란 오래 못 간다. 봉이 나서 뼉다귀도 못 추리기 전 에 훨훨 벗어나는 게 상책이겠다.\n",
      "\n",
      "라이선스\n",
      "\n",
      "21\n",
      "\n",
      "이 저작물은 저자가 사망한 지 70년이 넘었으므 로, 저자가 사망한 후 70년(또는 그 이하)이 지나 면 저작권이 소멸하는 국가에서 퍼블릭 도메인입 니다.\n",
      "\n",
      "1923년에서 1977년 사이에 출판되었다면 미국에서 퍼블 릭 도메인이 아닐 수 있습니다. 미국에서 퍼블릭 도메인인 저작물에는 {{PD-1996}}를 사용하십시오.\n",
      "\n",
      "주의\n",
      "\n",
      "22\n",
      "\n",
      "About this digital edition\n",
      "\n",
      "This e-book comes from the online library Wikisource[1]. This multilingual digital library, built by volunteers, is committed to developing a free accessible collection of publications of every kind: novels, poems, magazines, letters...\n",
      "\n",
      "We distribute our books for free, starting from works not copyrighted or published under a free license. You are free to use our e-books for any purpose (including commercial exploitation), under the terms of the Creative Commons Attribution-ShareAlike 3.0 Unported[2] license or, at your choice, those of the GNU FDL[3].\n",
      "\n",
      "Wikisource is constantly looking for new members. During the realization of this book, it's possible that we made some errors. You can report them at this page[4].\n",
      "\n",
      "The following users contributed to this book:\n",
      "\n",
      "Salamander724 Sotiale Caﬀelice~kowikisource Hibm98 Mineralsab Sjsws1078\n",
      "\n",
      "23\n",
      "\n",
      "Kwamikagami Tene~commonswiki Rocket000 Fleshgrinder Bastique Andux Amgine Boris23 KABALINI Bromskloss AzaToth Bender235 PatríciaR Vanished user 24kwjf10h32h\n",
      "\n",
      "1. ↑ https://wikisource.org 2. ↑ https://www.creativecommons.org/licenses/by-sa/3.0 3. ↑ https://www.gnu.org/copyleft/fdl.html 4. ↑ https://wikisource.org/wiki/Wikisource:Scriptorium\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300bddc2",
   "metadata": {},
   "source": [
    "# Chunking (문서 분할)\n",
    "\n",
    "![rag_split](figures/rag_split.png)\n",
    "\n",
    "- Load 한 문서를 지정한 기준의 덩어리(chunk)로 나누는 작업을 진행한다.\n",
    "\n",
    "## 나누는 이유\n",
    "1. **임베딩 모델의 컨텍스트 길이 제한**\n",
    "    - 대부분의 언어 모델은 한 번에 처리할 수 있는 토큰 수에 제한이 있다. 전체 문서를 통째로 입력하면 이 제한을 초과할 수 있어 처리가 불가능해진다.\n",
    "2. **검색 정확도 향상**\n",
    "    - 큰 문서 전체보다는 특정 주제나 내용을 다루는 작은 chunk가 사용자 질문과 더 정확하게 매칭된다. 예를 들어, 100페이지 매뉴얼에서 특정 기능에 대한 질문이 있을 때, 해당 기능을 설명하는 몇 개의 문단만 검색되는 것이 더 효과적이다.\n",
    "    - 사용자 질문에 대해 문서의 모든 내용이 다 관련있는 것은 아니다. Chunking을 통해 가장 관련성 높은 부분만 선별적으로 활용할 수 있어 답변의 품질이 향상된다.\n",
    "    - 전체 문서에는 질문과 무관한 내용들이 많이 포함되어 있어 모델이 혼란을 겪을 수 있다. 적절한 크기의 chunk는 이런 노이즈를 줄여준다.\n",
    "3. **계산 효율성**\n",
    "    - 벡터 유사도 계산, 임베딩 생성 등의 작업이 작은 chunk 단위로 수행될 때 더 빠르고 효율적이다. 메모리 사용량도 줄일 수 있다.\n",
    "\n",
    "## 주요 Splitter\n",
    "- **Splitter**는 문서를 분할(chunking)을 처리해주는 도구들이다. Langchain은 분할 대상, 방법에 따라 다양한 splitter를 제공한다.\n",
    "- **Splitter 의 목표**\n",
    "  - 가능한 한 **의미 있는 덩어리를 유지**하면서, **최대 길이(chunk_size)**를 넘지 않도록 나누기.\n",
    "- https://reference.langchain.com/python/langchain_text_splitters/\n",
    "\n",
    "### CharacterTextSplitter\n",
    "가장  기본적인 Text spliter\n",
    "- 한개의 구분자를 기준으로 분리한다. (default: \"\\n\\n\")\n",
    "    - 분리된 조각이 chunk size 보다 작으면 다음 조각과 합칠 수 있다.\n",
    "        - 합쳤을때 chuck_size 보다 크면 안 합친다. chuck_size 이내면 합친다.\n",
    "    - 나누는 기준은 구분자이기 때문에 chunk_size 보다 글자수가 많을 수 있다.\n",
    "- chunk size: 분리된 문서(chunk) 글자수 이내에서 분리되도록 한다.\n",
    "    -  구분자를 기준으로 분리한다. 구분자를 기준으로 분리한 문서 조각이 chunk size 보다 크더라도 그대로 유지한다. 즉 chunk_size가 우선이 아니라 **seperator** 가 우선이다.\n",
    "- 주요 파라미터\n",
    "    - chunk_size: 각 조각의 최대 길이를 지정.\n",
    "    - seperator: 구분 문자열을 지정. (default: '\\n\\n')\n",
    "- CharacterTextSplitter는 단순 스플리터로 overlap기능을 지원하지는 않는다. 단 seperator가 빈문자열(\"\") 일 경우에는 overlap 기능을 지원한다. overlap이란 각 이전 청크의 뒷부분의 문자열을 앞에 붙여 문맥을 유지하는 것을 말한다.\n",
    "  \n",
    "### RecursiveCharacterTextSplitter\n",
    "- RecursiveCharacterTextSplitter는 **긴 텍스트를 지정된 최대 길이(chunk_size) 이하로 나누는 데 효과적인 텍스트 분할기**(splitter)이다.\n",
    "- 여러 **구분자(separators)를 순차적으로 적용**하여, 가능한 한 자연스러운 문단/문장/단어 단위로 분할하고, 최종적으로는 크기 제한을 만족시킨다.\n",
    "- 분할 기준 문자\n",
    "    1. 두 개의 줄바꿈 문자 (\"\\n\\n\")\n",
    "    2. 한 개의 줄바꿈 문자 (\"\\n\")\n",
    "    3. 공백 문자 (\" \")\n",
    "    4. 빈 문자열 (\"\")\n",
    "- 작동 방식\n",
    "    1. 먼저 가장 높은 우선순위의 구분자(\"\\n\\n\")를 기준으로 분리한다.\n",
    "    2. 분할된 조각 중 **chunk_size를 초과하는 조각**에 대해 다음 우선순위 구분자(\"\\n\" → \" \" → \"\")로 재귀적으로 재분할한다.\n",
    "    3. 이 과정을 통해 모든 조각(chunk)이 chunk_size를 초과하지 않도록 만든다.  \n",
    "- 주요 파라미터\n",
    "    - chunk_size: 각 조각의 최대 길이를 지정.\n",
    "    - chunk_overlap: 연속된 청크들 간의 겹치는 문자 수를 설정. 새로운 청크 생성 시 이전 청크의 마지막 부분에서 지정된 수만큼의 문자를 가져와서 새 청크의 앞부분에 포함시켜, 청크 경계에서 문맥의 연속성을 유지한다.\n",
    "      - 구분자에 의해 청크가 나눠지면 정상적인 분리이므로 overlap이 적용되지 않는다.\n",
    "      - 정상적 구분자로 나눌 수 없어 chunk_size에 맞춰 잘라진 경우 문맥의 연결성을 위애 overlap을 적용한다.\n",
    "    - separators(list): 구분자를 지정한다. 지정하면 기본 구분자가 지정한 것으로 변경된다.\n",
    "\n",
    "#### 메소드\n",
    "- `split_documents(Iterable[Document]) : List[Document]`\n",
    "    - Document 목록을 받아 split 처리한다.\n",
    "- `split_text(str) : List[str]`\n",
    "    - string text를 받아서 split 처리한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a3f4cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 1.43s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b8f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35a08d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Created a chunk of size 69, which is longer than the specified 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "text = \"\"\"123456789012345678901234567890123456789012345678901234567890123456789\n",
    "\n",
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\n",
    "가나다라마바사아자차카타파하\n",
    "\n",
    "아야어여오요우유으이\n",
    "\n",
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\"\"\"\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=60,\n",
    "    chunk_overlap=10, # default:200, chunk_size 보다 chunk_overlap 이 더 클 수 없음.\n",
    "    # separator=\"\" # chunk_size 기준으로 나누기 => chunk_overlap 적용\n",
    ")\n",
    "results = splitter.split_text(text)\n",
    "print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3725f8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60-123456789012345678901234567890123456789012345678901234567890\n",
      "-----------------------------------\n",
      "60-1234567890123456789\n",
      "\n",
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLM\n",
      "-----------------------------------\n",
      "60-DEFGHIJKLMNOPQRSTUVWXYZ\n",
      "\n",
      "가나다라마바사아자차카타파하\n",
      "\n",
      "아야어여오요우유으이\n",
      "\n",
      "abcdefg\n",
      "-----------------------------------\n",
      "55-이\n",
      "\n",
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for txt in results:\n",
    "    print(len(txt), txt, sep=\"-\")\n",
    "    print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee0c8840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Created a chunk of size 69, which is longer than the specified 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# str -> Document 객체로 변환.\n",
    "doc = Document(page_content=text, metadata={\"category\":\"split\"}) \n",
    "\n",
    "docs2 = splitter.split_documents([doc])\n",
    "len(docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76b42eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'category': 'split'}, page_content='123456789012345678901234567890123456789012345678901234567890123456789')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db78687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 123456789012345678901234567890123456789012345678901234567890123456789\n",
      "52 abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      "26 가나다라마바사아자차카타파하\n",
      "\n",
      "아야어여오요우유으이\n",
      "52 abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n"
     ]
    }
   ],
   "source": [
    "for doc in docs2:\n",
    "    print(len(doc.page_content), doc.page_content) # document 객체도 동일하게 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57ae7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40-1234567890123456789012345678901234567890\n",
      "---------------------------------------\n",
      "29-12345678901234567890123456789\n",
      "---------------------------------------\n",
      "49-abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVW\n",
      "---------------------------------------\n",
      "13-NOPQRSTUVWXYZ\n",
      "---------------------------------------\n",
      "26-가나다라마바사아자차카타파하\n",
      "\n",
      "아야어여오요우유으이\n",
      "---------------------------------------\n",
      "43-abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQ\n",
      "---------------------------------------\n",
      "9-RSTUVWXYZ\n",
      "---------------------------------------\n",
      "27-abcdefghijklmnopqrstuvwxyzA\n",
      "---------------------------------------\n",
      "25-BCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text2 = \"\"\"1234567890123456789012345678901234567890\n",
    "12345678901234567890123456789\n",
    "\n",
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\n",
    "가나다라마바사아자차카타파하\n",
    "\n",
    "아야어여오요우유으이\n",
    "\n",
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQ RSTUVWXYZ\n",
    "abcdefghijklmnopqrstuvwxyzA BCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 50,\n",
    "    chunk_overlap = 10,\n",
    "    seperators=[\"\\n\\n\", '\\n','[.?!]', ' ','' ],\n",
    "    is_separator_regex=True #seperatpr 가 정규표현\n",
    ")\n",
    "result2 = splitter.split_text(text2)\n",
    "\n",
    "for txt in result2 :\n",
    "    print(len(txt), txt, sep=\"-\")\n",
    "    print(\"---------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5b25cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "path = \"data/olympic.txt\"\n",
    "\n",
    "#Loading -> Split\n",
    "loader = TextLoader(path, encoding=\"utf-8\")\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=20\n",
    ")\n",
    "\n",
    "load_docs = loader.load()\n",
    "docs = splitter.split_documents(load_docs)\n",
    "docs = [doc for doc in docs if len(doc.page_content)>10]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb96813e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "따른다. 오늘날 전 세계 대부분의 국가에서 올림픽 메달은 매우 큰 영예이며, 특히 올림픽 금메달리스트는 국가 영웅급의 대우를 받으며 스포츠 스타가 된다. 국가별로 올림픽 메달리스트들에게 지급하는 포상금도 크다. 대부분의 인기있는 종목들이나 일상에서 쉽게 접하고 즐길 수 있는 생활스포츠 종목들이 올림픽이라는 한 대회에서 동시에 열리고, 전 세계 대부분의 국가 출신의 선수들이 참여하는 만큼 전 세계 스포츠 팬들이 가장 많이 시청하는 이벤트이다. 2008 베이징 올림픽의 모든 종목 누적 시청자 수만 47억 명에 달하며, 이는 인류 역사상 가장 많은 수의 인구가 시청한 이벤트였다.\n"
     ]
    }
   ],
   "source": [
    "print(docs[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cd8206f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load_and_split(splitter) #splitter를 이용해서 load 와 split을 한번에 처리.\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed87439d",
   "metadata": {},
   "source": [
    "## Token 수 기준으로 나누기\n",
    "\n",
    "- LLM 언어 모델들은 입력 토큰 수 제한이 있어서 요청시 제한 토큰수 이상의 프롬프트는 전송할 수 없다.\n",
    "- 따라서 텍스트를 chunk로 분할할 때는 글자수 보다 **토큰 수를 기준으로 크기를 지정하는 것**이 좋다.  \n",
    "- 토큰기반 분할은 텍스트의 의미를 유지하면서 분할하는 방식이므로 문자 기반 분할과 같이 단어가 중간잘리는 것들을 방지할 수 있다. \n",
    "- 토큰 수 계산할 때는 사용하는 언어 모델에 사용된 것과 동일한 tokenizer를 사용하는 것이 좋다.\n",
    "  - 예를 들어 OpenAI의 GPT 모델을 사용할 경우 tiktoken 라이브러리를 활용하여 토큰 수를 정확하게 계산할 수 있다.\n",
    "\n",
    "### [tiktoken](https://github.com/openai/tiktoken) tokenizer 기반 분할\n",
    "- OpenAI에서 GPT 모델을 학습할 때 사용한 `BPE` 방식의 tokenizer. **OpenAI 언어모델을 사용할 경우 이것을 사용하는 것이 좀 더 정확하게  토큰dmf 계산할 수 있다.**\n",
    "- Splitter.from_tiktoken_encoder() 메소드를 이용해 생성\n",
    "  - `RecursiveCharacterTextSplitter.from_tiktoken_encoder()`\n",
    "  - `CharacterTextSplitter.from_tiktoken_encoder()`\n",
    "- 파라미터\n",
    "  - encode_name: 인코딩 방식(토큰화 규칙)을 지정. OpenAI는 GPT 모델들 마다 다른 방식을 사용했다. 그래서 사용하려는 모델에 맞는 인코딩 방식을 지정해야 한다.\n",
    "    - `cl100k_base`: GPT-4 및 GPT-3.5-Turbo 모델에서 사용된 방식.\n",
    "    - `r50k_base:` GPT-3 모델에서 사용된 방식 \n",
    "  - chunk_size, chunk_overlap, separators 파라미터 (위와 동일)\n",
    "- tiktoken 설치\n",
    "  - `pip install tiktoken`\n",
    "\n",
    "### HuggingFace Tokenizer\n",
    "- HuggingFace 모델을 사용할 경우 그 모델이 사용한 tokenizer를 이용해 토큰 기반으로 분할 한다.\n",
    "  - 다른 tokenizer를 이용해 분할 할 경우 토큰 수 계산이 다르게 될 수있다.\n",
    "- `from_huggingface_tokenizer()` 메소드를 이용.\n",
    "  - 파라미터\n",
    "    - tokenizer: HuggingFace tokenizer 객체\n",
    "    - chunk_size, chunk_overlap, separators 파라미터 (위와 동일)\n",
    "- `transformers` 라이브러리를 설치해야 한다.\n",
    "  - `pip install transformers` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b8b7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tiktoken\n",
      "Version: 0.12.0\n",
      "Location: C:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\n",
      "Requires: regex, requests\n",
      "Required-by: langchain-openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'uf'��(��) ���� �Ǵ� �ܺ� ����, ������ �� �ִ� ���α׷�, �Ǵ�\n",
      "��ġ ������ �ƴմϴ�.\n"
     ]
    }
   ],
   "source": [
    "!uv pip show tiktoken\n",
    "!uf pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d99796ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "path = \"data/olympic.txt\"\n",
    "loader = TextLoader(path, encoding=\"utf8\")\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-5-mini\", # gpt-5-mini에서 사용한 tokenizer 기준으로 한다.\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(splitter)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbf9ddbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올림픽\n",
      "올림픽(영어: Olympic Games, 프랑스어: Jeux olympiques)은 전 세계 각 대륙 각국에서 모인 수천 명의 선수가 참가해 여름과 겨울에 스포츠 경기를 하는 국제적인 대회이다. 전 세계에서 가장 큰 지구촌 최대의 스포츠 축제인 올림픽은 세계에서 가장 인지도있는 국제 행사이다. 올림픽은 2년마다 하계 올림픽과 동계 올림픽이 번갈아 열리며, 국제 올림픽 위원회(IOC)가 감독하고 있다. 또한 오늘날의 올림픽은 기원전 8세기부터 서기 5세기에 이르기까지 고대 그리스 올림피아에서 열렸던 올림피아 제전에서 비롯되었다. 그리고 19세기 말에 피에르 드 쿠베르탱 남작이 고대 올림피아 제전에서 영감을 얻어, 근대 올림픽을 부활시켰다. 이를 위해 쿠베르탱 남작은 1894년에 IOC를 창설했으며, 2년 뒤인 1896년에 그리스 아테네에서 제 1회 올림픽이 열렸다. 이때부터 IOC는 올림픽 운동의 감독 기구가 되었으며, 조직과 활동은 올림픽 헌장을 따른다. 오늘날 전 세계 대부분의 국가에서 올림픽 메달은 매우 큰 영예이며, 특히 올림픽 금메달리스트는 국가 영웅급의 대우를 받으며 스포츠 스타가 된다. 국가별로 올림픽 메달리스트들에게 지급하는 포상금도 크다. 대부분의 인기있는 종목들이나 일상에서 쉽게 접하고 즐길 수 있는 생활스포츠 종목들이 올림픽이라는 한 대회에서 동시에 열리고, 전 세계 대부분의 국가 출신의 선수들이 참여하는 만큼 전 세계 스포츠 팬들이 가장 많이 시청하는 이벤트이다. 2008 베이징 올림픽의 모든 종목 누적 시청자 수만 47억 명에 달하며, 이는 인류 역사상 가장 많은 수의 인구가 시청한 이벤트였다.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72878af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0623b3712c44c5b90a7a3ad3202c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--google--gemma-3-4b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41a25911c524154a33d2cb30fdc9a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2821acad256a40caab7d2b4b22a540be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec21922d6084601acf209f9745f6e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32564eaa878a4363b4836b257da330cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gemma.tokenization_gemma_fast.GemmaTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "#hugginface tokenizer 이용\n",
    "from transformers import AutoTokenizer\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24516afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter2 = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "docs = loader.load_and_split(splitter2)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 이름 뒤에 it, instruct 가 있으면 chatbot 모델 : 질문-답변 형식으로 학습을 시킨 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c366a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1a15cd4",
   "metadata": {},
   "source": [
    "## MarkdownHeaderTextSplitter\n",
    "- Markdown Header 기준으로 Splitter\n",
    "- Loading한 문서가 Markdown 문서이고 Header를 기준으로 문서의 내용이 나눠질때 사용.\n",
    "- https://reference.langchain.com/python/langchain_text_splitters/#langchain_text_splitters.MarkdownTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe3b1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "# 대주제1\n",
    "- 동물\n",
    "\n",
    "## 중주제1\n",
    "- 포유류\n",
    "\n",
    "- 조류\n",
    "\n",
    "### 소주제1\n",
    "- 개\n",
    "- 고양이\n",
    "- 까치\n",
    "- 독수리\n",
    "\n",
    "# 대주제2\n",
    "## 중주제2\n",
    "- 기차\n",
    "- 배\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c15132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# <h1>~<h6> # ~#*6\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "# header 정보는 metadata에 저장, 내용은 page_content에 저장\n",
    "headers_to_split = [\n",
    "    (\"#\", \"대주제\"),\n",
    "    (\"##\", \"중주제\"),\n",
    "    (\"###\", \"소주제\") # metadata의 정보가 된다.\n",
    "]\n",
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split, # 어떤 헤더를 기준으로 나눌지\n",
    "    strip_headers=True, # default : True - Header(#)을 내용에 포함시킬지에 대한 여부. True=포함시키지 않음\n",
    "    return_each_line=False # True: 각 라인을 별도의 Document로 생성.\n",
    ")\n",
    "\n",
    "# MarkdownHeaderTextSplitter는 split_documents() 메소드가 없다. split_text(str)\n",
    "docs = splitter.split_text(text)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4583b342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'대주제': '대주제1'}\n",
      "- 동물\n",
      "===================================\n",
      "{'대주제': '대주제1', '중주제': '중주제1'}\n",
      "- 포유류  \n",
      "- 조류\n",
      "===================================\n",
      "{'대주제': '대주제1', '중주제': '중주제1', '소주제': '소주제1'}\n",
      "- 개\n",
      "- 고양이\n",
      "- 까치\n",
      "- 독수리\n",
      "===================================\n",
      "{'대주제': '대주제2', '중주제': '중주제2'}\n",
      "- 기차\n",
      "- 배\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "for doc in docs :\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)\n",
    "    print(\"===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e9967de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "path = \"data/olympic_wiki.md\"\n",
    "#load = # 기준으로 split\n",
    "# with open(path, 'rt',encoding=\"utf8\") as fr:\n",
    "#     contnet = fr.read()\n",
    "# content\n",
    "\n",
    "loader = TextLoader(path, encoding=\"utf8\")\n",
    "\n",
    "headers_to_split= [\n",
    "    (\"#\", \"Header1\"),\n",
    "    (\"##\", \"Header2\"),\n",
    "    (\"###\", \"Header3\")\n",
    "]\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cab62bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and split\n",
    "\n",
    "docs = loader.load() #[Doc, Doc .....]\n",
    "doc_txt = '\\n'.join(doc.page_content for doc in docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c313422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs = splitter.split_text(doc_txt)\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0e53d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Header1': '올림픽'}, page_content='올림픽(영어: Olympic Games, 프랑스어: Jeux olympiques)은 전 세계 각 대륙 각국에서 모인 수천 명의 선수가 참가해 여름과 겨울에 스포츠 경기를 하는 국제적인 대회이다. 전 세계에서 가장 큰 지구촌 최대의 스포츠 축제인 올림픽은 세계에서 가장 인지도있는 국제 행사이다. 올림픽은 2년마다 하계 올림픽과 동계 올림픽이 번갈아 열리며, 국제 올림픽 위원회(IOC)가 감독하고 있다. 또한 오늘날의 올림픽은 기원전 8세기부터 서기 5세기에 이르기까지 고대 그리스 올림피아에서 열렸던 올림피아 제전에서 비롯되었다. 그리고 19세기 말에 피에르 드 쿠베르탱 남작이 고대 올림피아 제전에서 영감을 얻어, 근대 올림픽을 부활시켰다. 이를 위해 쿠베르탱 남작은 1894년에 IOC를 창설했으며, 2년 뒤인 1896년에 그리스 아테네에서 제 1회 올림픽이 열렸다. 이때부터 IOC는 올림픽 운동의 감독 기구가 되었으며, 조직과 활동은 올림픽 헌장을 따른다. 오늘날 전 세계 대부분의 국가에서 올림픽 메달은 매우 큰 영예이며, 특히 올림픽 금메달리스트는 국가 영웅급의 대우를 받으며 스포츠 스타가 된다. 국가별로 올림픽 메달리스트들에게 지급하는 포상금도 크다. 대부분의 인기있는 종목들이나 일상에서 쉽게 접하고 즐길 수 있는 생활스포츠 종목들이 올림픽이라는 한 대회에서 동시에 열리고, 전 세계 대부분의 국가 출신의 선수들이 참여하는 만큼 전 세계 스포츠 팬들이 가장 많이 시청하는 이벤트이다. 2008 베이징 올림픽의 모든 종목 누적 시청자 수만 47억 명에 달하며, 이는 인류 역사상 가장 많은 수의 인구가 시청한 이벤트였다.  \\n또한 20세기에 올림픽 운동이 발전함에 따라, IOC는 변화하는 세계의 사회 환경에 적응해야 했다. 이러한 변화의 예로는 얼음과 눈을 이용한 경기 종목을 다루는 동계 올림픽, 장애인이 참여하는 패럴림픽, 스페셜 올림픽, 데플림픽, 10대 선수들이 참여하는 유스 올림픽 등을 들 수 있다. 그 뿐만 아니라 IOC는 20세기의 변화하는 경제, 정치, 기술 환경에도 적응해야 했다. 그리하여 올림픽은 피에르 드 쿠베르탱이 기대했던 순수한 아마추어 정신에서 벗어나서, 프로 선수도 참가할 수 있게 되었다. 올림픽은 점차 대중 매체의 중요성이 커짐에 따라 올림픽의 상업화와 기업 후원을 놓고도 논란이 생겨났다. 또한 올림픽을 치르며 발생한 보이콧, 도핑, 심판 매수, 테러와 같은 수많은 일들은 올림픽이 더욱 굳건히 성장할 수 있는 원동력이 되었다.  \\n올림픽은 국제경기연맹(IF), 국가 올림픽 위원회(NOC), 각 올림픽의 위원회(예-벤쿠버동계올림픽조직위원회)로 구성된다. 의사 결정 기구인 IOC는 올림픽 개최 도시를 선정하며, 각 올림픽 대회마다 열리는 올림픽 종목도 IOC에서 결정한다. 올림픽 경기 개최 도시는 경기 축하 의식이 올림픽 헌장에 부합하도록 조직하고 기금을 마련해야 한다. 올림픽 축하 행사로는 여러 의식과 상징을 들 수 있는데 올림픽기나 성화가 그 예이다.  \\n올림픽은 거의 모든 국가가 참여할 정도로 규모가 커졌다. 하계 올림픽은 33개의 종목과 약 400개의 세부종목에서 13,000명이 넘는 선수들이 겨루고 그중 각 종목별 1, 2, 3위는 각각 금/은/동을 수여받는다. 전 세계 언론에서 각각 4년마다 열리는 올림픽 경기를 중계하기 때문에 이름 없는 선수가 개인적, 국가적, 세계적으로 명성을 얻을 수 있는 기회가 된다. 이와 더불어 올림픽 경기는 개최지와 개최국에게도 전 세계에 그 이름을 널리 알리는 좋은 기회가 된다.')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
