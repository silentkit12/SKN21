{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJM11xQ_qWWz"
      },
      "source": [
        "## 0. í™˜ê²½ì„¤ì •"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ttQhY7GjOOji",
        "outputId": "d69a614c-ba0f-4b4e-b0b6-c402b3a1bd6a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 24ms\u001b[0m\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 33ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install langchain_openai\n",
        "!uv pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTxPYf8IRefE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyTg2PGOqbsi"
      },
      "source": [
        "## 1. Langchain ì œê³µ ê¸°ëŠ¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AItlxxNmRLeF"
      },
      "source": [
        "ì°¸ê³  ìë£Œ : https://python.langchain.com/v0.2/docs/how_to/chatbots_memory/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5XYv0azqf3i"
      },
      "source": [
        "### 1) Message passing\n",
        "- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ chainì— ë„˜ê²¨ì£¼ëŠ” ê²ƒ\n",
        "\n",
        "- The simplest form of memory is simply passing chat history messages into a chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bnr2xJAmRM5z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat = ChatOpenAI(model=\"gpt-4o\") # gpt-3.5-turbo-0125"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mphlBET1RY8U",
        "outputId": "db12cc1f-a5b4-4465-9989-1a369a64d11a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë‹¹ì‹ ì€ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.'ë¥¼ ì¼ë³¸ì–´ë¡œ ì–´ë–»ê²Œ ë§í•˜ëŠ”ì§€ ë¬¼ì–´ë³´ì…¨ìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability. YOU MUST ANSWER IN KOREAN.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "ai_msg = chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            (\n",
        "                \"human\",\n",
        "                \"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\",\n",
        "            ),\n",
        "            (\"ai\",  \"ã„ãŸã ãã¾ã™ (ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤)\"),\n",
        "            (\"human\", \"ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  í–ˆì§€?\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "print(ai_msg.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-nxFkBPTnL_",
        "outputId": "06c9ef97-8fa3-4d78-a96b-41e219124fcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ë‹¹ì‹ ì€ AIë¥¼ ê³µë¶€í•˜ê³  ìˆë‹¤ê³  í•˜ì…¨ìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "ai_msg = chain.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            (\n",
        "                \"human\",\n",
        "                \"ë‚´ ì´ë¦„ì€ ë¯¼ì •ì´ê³ , AIë¥¼ ê³µë¶€í•˜ëŠ” ì¤‘ì´ì•¼.\",\n",
        "            ),\n",
        "            (\"ai\",  \"ê·¸ë ‡êµ°ìš”. í¥ë¯¸ë¡œìš´ë°ìš”?\"),\n",
        "            (\"human\", \"ë‚´ê°€ ë­ í•˜ê³  ìˆë‹¤ê³ ?\"),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "print(ai_msg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzZlHV2vMP5p"
      },
      "source": [
        "- ì‘ìš© : ëŒ€í™” ìŒ“ì¼ ë•Œë§ˆë‹¤ íˆìŠ¤í† ë¦¬ ì €ì¥í•˜ë„ë¡ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnGLhdn1LxJI",
        "outputId": "40353192-7f77-4132-f6cb-74fda196ff58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## CHAT_HISTORY ##\n",
            "[('human', 'ì•ˆë…•')] \n",
            "\n",
            "AI Says :  ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n",
            "## CHAT_HISTORY ##\n",
            "[('human', 'ì•ˆë…•'), ('ai', 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?'), ('human', 'quit')] \n",
            "\n",
            "AI Says :  ëŒ€í™”ë¥¼ ì¢…ë£Œí•˜ì‹œë ¤ë©´ \"ì¢…ë£Œ\" ë˜ëŠ” \"ë\"ì´ë¼ê³  ë§ì”€í•´ ì£¼ì‹œë©´ ë©ë‹ˆë‹¤. ë‹¤ë¥¸ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”!\n"
          ]
        }
      ],
      "source": [
        "history_list = []\n",
        "while(True):\n",
        "    user_input = input()\n",
        "    if user_input == \"ì¢…ë£Œ\": break\n",
        "    history_list.append(\n",
        "        (\n",
        "            \"human\",\n",
        "            user_input,\n",
        "        )\n",
        "    )\n",
        "    print(\"## CHAT_HISTORY ##\")\n",
        "    print(history_list, \"\\n\")\n",
        "    ai_msg = chain.invoke(\n",
        "        {\n",
        "            \"messages\": history_list,\n",
        "        }\n",
        "    )\n",
        "    print(\"AI Says : \",ai_msg.content)\n",
        "\n",
        "    history_list.append(\n",
        "        (\n",
        "            \"ai\",\n",
        "            ai_msg.content,\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHsjha-JTVMn"
      },
      "source": [
        "### 2) Chat history\n",
        "- ChatMessageHistory() í´ë˜ìŠ¤ ì‚¬ìš©í•˜ê¸°\n",
        "- It's perfectly fine to store and pass messages directly as an array, but we can use LangChain's built-in message history class to store and load messages as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EZYo9IoXgMB"
      },
      "source": [
        "https://api.python.langchain.com/en/latest/chat_message_histories/langchain_community.chat_message_histories.in_memory.ChatMessageHistory.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82OuUOtETIpI",
        "outputId": "1d75e123-2254-46bf-966c-b01481241bd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='ã„ãŸã ãã¾ã™ (ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤)', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "\n",
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "chat_history.add_user_message(\n",
        "    \"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\"\n",
        ")\n",
        "\n",
        "chat_history.add_ai_message(\"ã„ãŸã ãã¾ã™ (ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤)\")\n",
        "\n",
        "chat_history.messages\n",
        "# chat_history.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLOjjo9sWKdf"
      },
      "source": [
        "- ëŒ€í™”ë‚´ìš© ë°”ë¡œë°”ë¡œ ì €ì¥í•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0msG3hgTVvF",
        "outputId": "6818c7dd-a98a-4581-c9ff-eab36c4c1c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì²«ë²ˆì§¸ ì…ë ¥ í›„ history :  [HumanMessage(content=\"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\", additional_kwargs={}, response_metadata={})]\n",
            "ì²«ë²ˆì§¸ ë‹µë³€ í›„ history :  [HumanMessage(content=\"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\", additional_kwargs={}, response_metadata={}), AIMessage(content='ì¼ë³¸ì–´ë¡œ \\'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.\\'ëŠ” \"ã„ãŸã ãã¾ã™\"ë¼ê³  ë§í•©ë‹ˆë‹¤. í•œê¸€ ë°œìŒì€ \"ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤\"ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 59, 'total_tokens': 93, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2f406b9113', 'finish_reason': 'stop', 'logprobs': None}, id='run-1ed0525d-6e28-4e28-9c7a-a8970c5b9c49-0', usage_metadata={'input_tokens': 59, 'output_tokens': 34, 'total_tokens': 93, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]\n",
            "\n",
            "ë‘ë²ˆì§¸ ì…ë ¥ í›„ history :  [HumanMessage(content=\"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\", additional_kwargs={}, response_metadata={}), AIMessage(content='ì¼ë³¸ì–´ë¡œ \\'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.\\'ëŠ” \"ã„ãŸã ãã¾ã™\"ë¼ê³  ë§í•©ë‹ˆë‹¤. í•œê¸€ ë°œìŒì€ \"ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤\"ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 59, 'total_tokens': 93, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_2f406b9113', 'finish_reason': 'stop', 'logprobs': None}, id='run-1ed0525d-6e28-4e28-9c7a-a8970c5b9c49-0', usage_metadata={'input_tokens': 59, 'output_tokens': 34, 'total_tokens': 93, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content='ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  ë¬¼ì–´ë´¤ì§€?', additional_kwargs={}, response_metadata={})]\n",
            "[ë‘ë²ˆì§¸ ì…ë ¥ì— ëŒ€í•œ ë‹µë³€]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "AIMessage(content='ë‹¹ì‹ ì€ \"ì¼ë³¸ì–´ë¡œ \\'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.\\' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\"ë¼ê³  ë¬¼ì–´ë³´ì…¨ìŠµë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 112, 'total_tokens': 146, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e5e4913e83', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a35d5c0-d930-4eef-a5fa-11cf78860163-0', usage_metadata={'input_tokens': 112, 'output_tokens': 34, 'total_tokens': 146, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "################ USER INPUT - 1 ################\n",
        "input1 = \"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\"\n",
        "\n",
        "# ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì…ë ¥(user_input) ì €ì¥\n",
        "chat_history.add_user_message(input1)\n",
        "print(\"ì²«ë²ˆì§¸ ì…ë ¥ í›„ history : \", chat_history.messages)\n",
        "\n",
        "################ AI RESPONSE - 1 ################\n",
        "# ë‹µë³€ ìƒì„±(response)\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"messages\": chat_history.messages,\n",
        "    }\n",
        ")\n",
        "\n",
        "# ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë‹µë³€(response) ì €ì¥\n",
        "chat_history.add_ai_message(response)\n",
        "print(\"ì²«ë²ˆì§¸ ë‹µë³€ í›„ history : \", chat_history.messages)\n",
        "print()\n",
        "\n",
        "################ USER INPUT - 2 ################\n",
        "input2 = \"ë‚´ê°€ ë°©ê¸ˆ ë­ë¼ê³  ë¬¼ì–´ë´¤ì§€?\"\n",
        "\n",
        "# ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì…ë ¥(user_input) ì €ì¥\n",
        "chat_history.add_user_message(input2)\n",
        "print(\"ë‘ë²ˆì§¸ ì…ë ¥ í›„ history : \", chat_history.messages)\n",
        "\n",
        "################ AI RESPONSE - 2 ################\n",
        "# ë‹µë³€ ìƒì„±(response)\n",
        "print(\"[ë‘ë²ˆì§¸ ì…ë ¥ì— ëŒ€í•œ ë‹µë³€]\")\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"messages\": chat_history.messages,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lNztnwgY5G3"
      },
      "source": [
        "- ì‘ìš© : ëŒ€í™” ìŒ“ì¼ ë•Œë§ˆë‹¤ íˆìŠ¤í† ë¦¬ ì €ì¥í•˜ë„ë¡ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atmi65POYM12",
        "outputId": "5842e088-b8d0-4b7b-bdde-8bbeca9aeefd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì´ˆì½œë¦¿ì€ ì •ë§ ë§›ìˆì£ ! íŠ¹ë³„íˆ ì¢‹ì•„í•˜ëŠ” ì´ˆì½œë¦¿ ì¢…ë¥˜ê°€ ìˆë‚˜ìš”? ë‹¤í¬ ì´ˆì½œë¦¿, ë°€í¬ ì´ˆì½œë¦¿, ì•„ë‹ˆë©´ í™”ì´íŠ¸ ì´ˆì½œë¦¿ì„ ë” ì„ í˜¸í•˜ì„¸ìš”?\n",
            "ë‹¤í¬ ì´ˆì½œë¦¿ì„ ì¢‹ì•„í•˜ì‹ ë‹¤ë©´ ëª‡ ê°€ì§€ ì¶”ì²œë“œë¦´ê²Œìš”:\n",
            "\n",
            "1. **ë‹¤í¬ ì´ˆì½œë¦¿ íŠ¸ëŸ¬í”Œ**: ë¶€ë“œëŸ½ê³  ì§„í•œ ë§›ì„ ëŠë‚„ ìˆ˜ ìˆì–´ìš”.\n",
            "2. **ë‹¤í¬ ì´ˆì½œë¦¿ê³¼ ì•„ëª¬ë“œ**: ì•„ëª¬ë“œì˜ ê³ ì†Œí•¨ê³¼ ì´ˆì½œë¦¿ì˜ ìŒ‰ì‹¸ë¦„í•œ ë§›ì´ ì˜ ì–´ìš¸ë ¤ìš”.\n",
            "3. **ë‹¤í¬ ì´ˆì½œë¦¿ ë”¸ê¸°**: ìƒí¼í•œ ë”¸ê¸°ì™€ ë‹¤í¬ ì´ˆì½œë¦¿ì˜ ì¡°í™”ê°€ í›Œë¥­í•©ë‹ˆë‹¤.\n",
            "4. **ë‹¤í¬ ì´ˆì½œë¦¿ ë¸Œë¼ìš°ë‹ˆ**: ì´‰ì´‰í•˜ê³  ì§„í•œ ë¸Œë¼ìš°ë‹ˆë¡œ ì´ˆì½œë¦¿ì˜ í’ë¯¸ë¥¼ ì¦ê¸¸ ìˆ˜ ìˆì–´ìš”.\n",
            "5. **ë‹¤í¬ ì´ˆì½œë¦¿ ì¹© ì¿ í‚¤**: ìŒ‰ì‹¸ë¦„í•œ ì´ˆì½œë¦¿ ì¹©ì´ ë“¤ì–´ê°„ ì¿ í‚¤ëŠ” í•­ìƒ ì¸ê¸° ìˆì£ .\n",
            "\n",
            "ì–´ë–¤ ê²ƒì´ë“  ë‹¤í¬ ì´ˆì½œë¦¿ì˜ ê¹Šì€ ë§›ì„ ì˜ ì‚´ë¦´ ìˆ˜ ìˆëŠ” ê°„ì‹ë“¤ì´ë‹ˆ í•œë²ˆ ì‹œë„í•´ ë³´ì„¸ìš”!\n",
            "ì•„, ì²˜ìŒì—ëŠ” ë‹¤í¬ ì´ˆì½œë¦¿ì„ ì¢‹ì•„í•œë‹¤ê³  ë§ì”€í•˜ì…¨ëŠ”ë°, ë°€í¬ ì´ˆì½œë¦¿ë„ ì¢‹ì•„í•˜ì‹œë‚˜ìš”? ë°€í¬ ì´ˆì½œë¦¿ë„ ë¶€ë“œëŸ½ê³  ë‹¬ì½¤í•´ì„œ ë§ì€ ë¶„ë“¤ì´ ì¢‹ì•„í•˜ì‹œì£ . í˜¹ì‹œ ì¢‹ì•„í•˜ëŠ” ë°€í¬ ì´ˆì½œë¦¿ ê°„ì‹ì´ ìˆìœ¼ì‹ ê°€ìš”?\n"
          ]
        }
      ],
      "source": [
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "while(True):\n",
        "    user_input = input()\n",
        "    if user_input == \"ì¢…ë£Œ\": break\n",
        "    chat_history.add_user_message(user_input)\n",
        "    response = chain.invoke(\n",
        "        {\n",
        "            \"messages\": chat_history.messages,\n",
        "        }\n",
        "    )\n",
        "    chat_history.add_ai_message(response)\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5IW96A8WNYS"
      },
      "source": [
        "### 3) Automatic history management\n",
        "- RunnableWithMessageHistory() í™œìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ê´€ë¦¬í•˜ê¸°\n",
        "- The previous examples pass messages to the chain explicitly. This is a completely acceptable approach, but it does require external management of new messages. LangChain also includes an wrapper for LCEL chains that can handle this process automatically called RunnableWithMessageHistory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ECXuHzfWWNol"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(model=\"gpt-4o\")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability. YOU MUST ANSWER IN KOREAN.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y2GV7xqWWPGk"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "chat_history_for_chain = ChatMessageHistory()\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain, # ì‹¤í–‰í•  Runnable ê°ì²´\n",
        "    lambda session_id: chat_history_for_chain, # ì„¸ì…˜ ê¸°ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜\n",
        "    input_messages_key=\"input\", # ì…ë ¥ ë©”ì‹œì§€ì˜ Key\n",
        "    history_messages_key=\"chat_history\", # ëŒ€í™” íˆìŠ¤í† ë¦¬ ë©”ì‹œì§€ì˜ Key\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0QE2_qxWPkl",
        "outputId": "2a7fd8b4-bb9e-49e6-b992-5f9445971d06"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='ì¼ë³¸ì–´ë¡œ \\'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.\\'ëŠ” \"ã„ã ã ãã¾ã™\"ë¼ê³  ë§í•©ë‹ˆë‹¤. í•œê¸€ ë°œìŒì€ \\'ì´íƒ€ë‹¤í‚¤ë§ˆìŠ¤\\'ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 59, 'total_tokens': 95, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e1dc348028', 'id': 'chatcmpl-CnLhNtz73mN4VxoP93v8Esi4nzkjF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b2680-8b33-7311-9bce-65c6b30a9515-0', usage_metadata={'input_tokens': 59, 'output_tokens': 36, 'total_tokens': 95, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"ì¼ë³¸ì–´ë¡œ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.' ì–´ë–»ê²Œ ë§í•´? í•œê¸€ ë°œìŒë„ í•¨ê»˜ ë§í•´ì¤˜.\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5898B2zWRQT",
        "outputId": "8a0777ca-7493-4564-fe0f-ff730ad68a6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"ë‹¹ì‹ ì€ 'ì˜ ë¨¹ê² ìŠµë‹ˆë‹¤.'ë¥¼ ì¼ë³¸ì–´ë¡œ ì–´ë–»ê²Œ ë§í•˜ëŠ”ì§€ ë¬¼ì–´ë³´ì…¨ìŠµë‹ˆë‹¤. ë˜í•œ í•œê¸€ ë°œìŒë„ í•¨ê»˜ ìš”ì²­í•˜ì…¨ìŠµë‹ˆë‹¤.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 112, 'total_tokens': 146, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e1dc348028', 'id': 'chatcmpl-CnLhWULKr1a6icoWJ8R2ky8pUfo2E', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b2680-aaaf-76f0-82c2-0ce653913d88-0', usage_metadata={'input_tokens': 112, 'output_tokens': 34, 'total_tokens': 146, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"ë‚´ê°€ ë­ë¼ê³  ë¬¼ì–´ë´¤ì§€?\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwFQUG_PgvGv"
      },
      "source": [
        "íˆìŠ¤í† ë¦¬ ì €ì¥ í•¨ìˆ˜ë¥¼ ë”°ë¡œ í˜¸ì¶œí•  í•„ìš”ì—†ì´ invoke ì‹¤í–‰ë§Œìœ¼ë¡œ ëŒ€í™” ê¸°ë¡ì´ ìë™ ì €ì¥ë˜ëŠ” ê²ƒì„ í™•ì¸ ê°€ëŠ¥!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9HtRAXzfutg"
      },
      "source": [
        "### 4) Modifying chat history\n",
        "- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê°€ê³µí•˜ì—¬ ë„˜ê²¨ì£¼ê¸°\n",
        "- Modifying stored chat messages can help your chatbot handle a variety of situations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJicGgSXWvkL",
        "outputId": "9b8deca3-bf20-49d8-e38f-049c1d22bd3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ë‚˜ì—°ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, ë‚˜ì—°ë‹˜! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='ë‚ ì”¨ ì¢‹ì€ ë‚  ë“¤ì„ë§Œ í•œ ë…¸ë˜ ì¶”ì²œí•´ì£¼ì„¸ìš”.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Carpenters - Close to you ë¥¼ ì¶”ì²œí•´ìš”.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "chat_history.add_user_message(\"ì•ˆë…•í•˜ì„¸ìš”. ì œ ì´ë¦„ì€ ë‚˜ì—°ì…ë‹ˆë‹¤.\")\n",
        "chat_history.add_ai_message(\"ì•ˆë…•í•˜ì„¸ìš”, ë‚˜ì—°ë‹˜! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\")\n",
        "chat_history.add_user_message(\"ë‚ ì”¨ ì¢‹ì€ ë‚  ë“¤ì„ë§Œ í•œ ë…¸ë˜ ì¶”ì²œí•´ì£¼ì„¸ìš”.\")\n",
        "chat_history.add_ai_message(\"Carpenters - Close to you ë¥¼ ì¶”ì²œí•´ìš”.\")\n",
        "\n",
        "chat_history.messages\n",
        "# chat_history.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1f7vAPaCWyyE"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability. The provided chat history includes facts about the user you are speaking with. YOU MUST ANSWER IN KOREAN.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NEvcOsdqWzsZ"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "def summarize_messages(chain_input):\n",
        "    stored_messages = chat_history.messages\n",
        "    if len(stored_messages) == 0:\n",
        "        return False\n",
        "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"placeholder\", \"{chat_history}\"),\n",
        "            (\n",
        "                \"user\",\n",
        "                \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    summarization_chain = summarization_prompt | chat\n",
        "\n",
        "    # chat_history ì— ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì„ ìš”ì•½í”„ë¡¬í”„íŠ¸ì— ì…ë ¥ & ê²°ê³¼ ì €ì¥\n",
        "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
        "\n",
        "    # chat_history ì— ì €ì¥ë˜ì–´ìˆë˜ ê¸°ë¡ ì§€ìš°ê¸°\n",
        "    chat_history.clear()\n",
        "\n",
        "    # ìƒì„±ëœ ìƒˆë¡œìš´ ìš”ì•½ë‚´ìš©ìœ¼ë¡œ ê¸°ë¡ ì±„ìš°ê¸°\n",
        "    chat_history.add_message(summary_message)\n",
        "\n",
        "    return True\n",
        "\n",
        "chain_with_summarization = (\n",
        "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
        "    | chain_with_message_history\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ed_Kcek8kvT"
      },
      "source": [
        "- summarize_messages í•¨ìˆ˜ì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ messages_summarized ë¼ëŠ” ì´ë¦„ì˜ Key ë¡œ ì €ì¥\n",
        "\n",
        "    - ì •ì˜í•œ chain ì•ˆì— \"messages_summarized\" Key ê°€ ì •ì˜ë˜ê³  ìˆì§€ ì•Šìœ¼ë¯€ë¡œ, ì´ëŠ” `summarize_messages` í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•œ ë¬´ì‹œ ê°€ëŠ¥í•œ Key\n",
        "    - í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ë©´ chat_history ì— ìš”ì•½ëœ íˆìŠ¤í† ë¦¬ê°€ ì €ì¥ë¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq4jhbBBW17O",
        "outputId": "a2a1c7ba-3052-484f-ad32-cf00b70e69ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='ë„¤, ë‚˜ì—°ë‹˜ì´ë¼ê³  í•˜ì…¨ì£ . ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 102, 'total_tokens': 119, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e1dc348028', 'id': 'chatcmpl-CnLhz9T44hdhCP7pnVNTnGvXelSga', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b2681-1893-7d23-8c9e-e542b65469bd-0', usage_metadata={'input_tokens': 102, 'output_tokens': 17, 'total_tokens': 119, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain_with_summarization.invoke(\n",
        "    {\"input\": \"ì œ ì´ë¦„ì„ ê¸°ì–µí•˜ê³  ìˆë‚˜ìš”?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtwtUrBcW6eK",
        "outputId": "f1a2913b-265e-4ce4-a1c3-d38588178034"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='CarpentersëŠ” ë‚¨ë§¤ ë“€ì˜¤ë¡œ, ë¦¬ì²˜ë“œ ì¹´íœí„°ì™€ ì¹´ë Œ ì¹´íœí„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¹´ë Œ ì¹´íœí„°ëŠ” ì—¬ì„± ë³´ì»¬ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 109, 'total_tokens': 153, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_e5e4913e83', 'finish_reason': 'stop', 'logprobs': None}, id='run-20fa421b-37ee-49f2-8af2-dbc6ea408fd6-0', usage_metadata={'input_tokens': 109, 'output_tokens': 44, 'total_tokens': 153, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain_with_summarization.invoke(\n",
        "    {\"input\": \"ê·¸ ê°€ìˆ˜ëŠ” ë‚¨ìì¸ê°€ìš” ì—¬ìì¸ê°€ìš”?\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}},\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-goCIryhxk1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn0LO7Bdr8nT"
      },
      "source": [
        "## 2. ëë§ì‡ê¸° ê²Œì„ êµ¬í˜„í•˜ê¸°\n",
        "\n",
        "- 4) Modifying chat history ì˜ í™œìš©!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpnDi8ZFhx6y",
        "outputId": "27ed50a9-b5bb-458c-db66-6c2550074ded"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='ëë§ì‡ê¸° í•˜ì', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='ì¢‹ìŠµë‹ˆë‹¤. ì œê°€ ë¨¼ì € ì‹œì‘í• ê²Œìš”. ë°”ë‚˜ë‚˜!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='ë‚˜ì´í…Œ', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='í…Œì´í”„', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  íˆìŠ¤í† ë¦¬ í´ë˜ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "chat_history = ChatMessageHistory()\n",
        "\n",
        "chat_history.add_user_message(\"ëë§ì‡ê¸° í•˜ì\")\n",
        "chat_history.add_ai_message(\"ì¢‹ìŠµë‹ˆë‹¤. ì œê°€ ë¨¼ì € ì‹œì‘í• ê²Œìš”. ë°”ë‚˜ë‚˜!\")\n",
        "chat_history.add_user_message(\"ë‚˜ì´í…Œ\")\n",
        "chat_history.add_ai_message(\"í…Œì´í”„\")\n",
        "\n",
        "chat_history.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "dQoRukC8hx6z"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(model=\"gpt-4o\")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"ë‹¹ì‹ ì€ ëë§ì‡ê¸° ê²Œì„ì„ ì§„í–‰í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ê²Œì„ ê·œì¹™ì…ë‹ˆë‹¤. ë‹¹ì‹ ê³¼ user ì˜ ì…ë ¥ì—ì„œ ì•„ë˜ ê·œì¹™ì´ ê¼­ ì§€ì¼œì ¸ì•¼ í•˜ë©°, ì§€í‚¤ì§€ ì•Šì€ ì‚¬ëŒì—ê²Œ íŒ¨ë°°ë¥¼ ì•Œë¦° ë’¤, ëë§ì‡ê¸° ê²Œì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n",
        "                1. ì£¼ì–´ì§„ ëŒ€í™” ê¸°ë¡ì—ì„œ ì´ë¯¸ ë‚˜ì™”ë˜ ë‹¨ì–´ë¥¼ ë‹¤ì‹œ ë§í–ˆì„ ê²½ìš° íŒ¨ë°°í•©ë‹ˆë‹¤.\n",
        "                2. ë‘ìŒë²•ì¹™ì„ í—ˆìš©í•©ë‹ˆë‹¤. (ex. ë¦¬ -> ì´, ë ¥ -> ì—­, ë½ -> ë‚™)\n",
        "                3. êµ­ì–´ì‚¬ì „ì— ì¡´ì¬í•˜ëŠ” ë‹¨ì–´ì´ì, ëª…ì‚¬ì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
        "            \"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"user\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | chat\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    lambda session_id: chat_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BLMCkKMphx6z"
      },
      "outputs": [],
      "source": [
        "def summarize_messages(chain_input):\n",
        "    stored_messages = chat_history.messages\n",
        "    if len(stored_messages) == 0:\n",
        "        return False\n",
        "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"placeholder\", \"{chat_history}\"),\n",
        "            (\n",
        "                \"user\",\n",
        "                \"ìœ„ ì±„íŒ… ë©”ì‹œì§€ëŠ” ëë§ì‡ê¸° ê²Œì„ì„ ì§„í–‰í•œ ëŒ€í™”ë‚´ìš©ì…ë‹ˆë‹¤. ì–¸ê¸‰í•œ ë‹¨ì–´ë“¤ë§Œ ë‚˜ì—´í•˜ì—¬ ì €ì¥í•´ì£¼ì„¸ìš”.\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    summarization_chain = summarization_prompt | chat\n",
        "\n",
        "    # chat_history ì— ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì„ ìš”ì•½í”„ë¡¬í”„íŠ¸ì— ì…ë ¥ & ê²°ê³¼ ì €ì¥\n",
        "    summary_message = summarization_chain.invoke({\"chat_history\": stored_messages})\n",
        "\n",
        "    # chat_history ì— ì €ì¥ë˜ì–´ìˆë˜ ê¸°ë¡ ì§€ìš°ê¸°\n",
        "    chat_history.clear()\n",
        "\n",
        "    # ìƒì„±ëœ ìƒˆë¡œìš´ ìš”ì•½ë‚´ìš©ìœ¼ë¡œ ê¸°ë¡ ì±„ìš°ê¸°\n",
        "    chat_history.add_message(summary_message)\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "chain_with_summarization = (\n",
        "    RunnablePassthrough.assign(messages_summarized=summarize_messages)\n",
        "    | chain_with_message_history\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDn2jJJGiw1a",
        "outputId": "4e539b43-a597-44dd-d41e-e5b959614c01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ™‹â€â™‚ï¸ YOUR TURN :  ë°”ë‹¤\n",
            "âœ AI TURN :  ë‹¤ë¦¬\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN :  ë¦¬ì–´ì¹´\n",
            "âœ AI TURN :  ì¹´ë©”ë¼\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN :  ë¼ë””ì˜¤\n",
            "âœ AI TURN :  ì˜¤í† ë°”ì´\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN :  ì´ë°œì†Œ\n",
            "âœ AI TURN :  ì†Œë°©ì°¨\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN :  ì°¨ë¦¼í‘œ\n",
            "âœ AI TURN :  í‘œì§€íŒ\n",
            "ğŸ™‹â€â™‚ï¸ YOUR TURN :  ì¢…ë£Œ\n"
          ]
        }
      ],
      "source": [
        "while(True):\n",
        "    user_input = input(\"ğŸ™‹â€â™‚ï¸ YOUR TURN : \")\n",
        "    print(\"ğŸ™‹â€â™‚ï¸ YOUR TURN : \", user_input)\n",
        "    if user_input == \"ì¢…ë£Œ\": break\n",
        "    response = chain_with_summarization.invoke(\n",
        "                {\"input\": user_input},\n",
        "                {\"configurable\": {\"session_id\": \"unused\"}},\n",
        "            )\n",
        "    print(\"âœ AI TURN : \", response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feZwCT4zsEWh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
