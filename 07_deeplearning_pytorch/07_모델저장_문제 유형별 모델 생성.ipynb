{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 모델 저장\n",
    "\n",
    "- 학습이 완료된 모델을 파일로 저장하여, 이후 추가 학습이나 예측 서비스에 사용할 수 있도록 한다.\n",
    "- 파이토치(PyTorch)는 **모델의 파라미터만 저장**하는 방법과 **모델의 구조와 파라미터를 모두 저장**하는 두 가지 방식을 제공한다.\n",
    "- 저장 함수\n",
    "  - `torch.save(저장할 객체, 저장 경로)`\n",
    "- 보통 저장 파일의 확장자는 `.pt`나 `.pth`를 사용한다.\n",
    "\n",
    "## 모델 전체 저장 및 불러오기\n",
    "\n",
    "- 저장하기\n",
    "  - `torch.save(model, 저장 경로)`\n",
    "- 불러오기\n",
    "  - `load_model = torch.load(저장 경로)`\n",
    "- 모델 저장 시 **피클(pickle)**을 사용해 직렬화되므로, 모델을 불러오는 실행 환경에도 저장할 때 사용한 클래스 정의가 필요하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델의 파라미터만 저장\n",
    "\n",
    "-   모델을 구성하는 파라미터만 저장한다.\n",
    "-   모델의 구조는 저장하지 않기 때문에 불러올 때 **모델을 먼저 생성하고 생성한 모델에 불러온 파라미터를 덮어씌운다.**\n",
    "-   모델의 파라미터는 **state_dict** 형식으로 저장한다.\n",
    "\n",
    "### state_dict\n",
    "\n",
    "-   모델의 파라미터 Tensor들을 레이어 단위별로 나누어 저장한 Ordered Dictionary (OrderedDict)\n",
    "-   `모델객체.state_dict()` 메소드를 이용해 조회한다.\n",
    "-   모델의 state_dict을 조회 후 저장한다.\n",
    "    -   `torch.save(model.state_dict(), \"저장경로\")`\n",
    "-   생성된 모델에 읽어온 state_dict를 덮어씌운다.\n",
    "    -   `new_model.load_state_dict(torch.load(\"state_dict저장경로\"))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint 저장 및 불러오기\n",
    "\n",
    "- 학습이 끝나지 않은 모델을 저장하고, 나중에 이어서 학습시키려면 모델의 구조와 파라미터뿐만 아니라 optimizer, loss 함수 등 학습에 필요한 객체들도 함께 저장해야 한다.\n",
    "- 딕셔너리(Dictionary)에 저장하려는 값들을 key-value 쌍으로 구성하여 `torch.save()`를 이용해 저장한다.\n",
    "\n",
    "```python\n",
    "# 저장\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': train_loss\n",
    "}, \"저장경로\")\n",
    "\n",
    "# 불러오기\n",
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# 불러온 checkpoint를 이용해 이전 학습 상태 복원\n",
    "checkpoint = torch.load(\"저장경로\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 모델 정의\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(3, 4) # 3 X 4 + 4 \n",
    "        self.lr2 = nn.Linear(4, 2)\n",
    "        self.relu = nn.ReLU() # activation함수->파라미터가 없는 단순 계산함수. relu(X) = max(X, 0)\n",
    "    def forward(self, X):\n",
    "        X = self.lr1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.lr2(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성성\n",
    "model = MyModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3, out_features=4, bias=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################\n",
    "#  모델에 Layer들을 조회. 모델.instance변수명\n",
    "################################################\n",
    "lr_layer = model.lr1\n",
    "lr_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#  Layer의 파라미터(weight/bias) 조회\n",
    "################################################\n",
    "lr1_weight = lr_layer.weight\n",
    "lr1_bias = lr_layer.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0990, -0.2017,  0.3291],\n",
       "        [ 0.3617,  0.4953,  0.0188],\n",
       "        [ 0.1496, -0.5085,  0.0211],\n",
       "        [-0.3762,  0.0578, -0.0185]], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lr1_weight.size())\n",
    "lr1_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.2466,  0.1807, -0.4694, -0.3903], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"saved_models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#  모델을 저장\n",
    "################################################\n",
    "torch.save(model, \"saved_models/my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "#  저장된 모델 Load\n",
    "################################################\n",
    "load_model = torch.load(\"saved_models/my_model.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModel(\n",
       "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
       "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0990, -0.2017,  0.3291],\n",
       "        [ 0.3617,  0.4953,  0.0188],\n",
       "        [ 0.1496, -0.5085,  0.0211],\n",
       "        [-0.3762,  0.0578, -0.0185]], requires_grad=True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.lr1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0990, -0.2017,  0.3291],\n",
       "        [ 0.3617,  0.4953,  0.0188],\n",
       "        [ 0.1496, -0.5085,  0.0211],\n",
       "        [-0.3762,  0.0578, -0.0185]], requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[-0.0990, -0.2017,  0.3291],\n",
       "                      [ 0.3617,  0.4953,  0.0188],\n",
       "                      [ 0.1496, -0.5085,  0.0211],\n",
       "                      [-0.3762,  0.0578, -0.0185]])),\n",
       "             ('lr1.bias', tensor([ 0.2466,  0.1807, -0.4694, -0.3903])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.1697, -0.2790,  0.1483,  0.3806],\n",
       "                      [ 0.0087, -0.2120, -0.2670, -0.0226]])),\n",
       "             ('lr2.bias', tensor([-0.2744,  0.2594]))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######################################################\n",
    "# 모델의 파라미터들(weight들, bias들)만 저장/불러오기\n",
    "######################################################\n",
    "state_dict = model.state_dict()\n",
    "state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['lr1.weight', 'lr1.bias', 'lr2.weight', 'lr2.bias'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# state_dict 저장\n",
    "################### \n",
    "torch.save(state_dict, \"saved_models/my_model_parameter.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[-0.0990, -0.2017,  0.3291],\n",
       "                      [ 0.3617,  0.4953,  0.0188],\n",
       "                      [ 0.1496, -0.5085,  0.0211],\n",
       "                      [-0.3762,  0.0578, -0.0185]])),\n",
       "             ('lr1.bias', tensor([ 0.2466,  0.1807, -0.4694, -0.3903])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.1697, -0.2790,  0.1483,  0.3806],\n",
       "                      [ 0.0087, -0.2120, -0.2670, -0.0226]])),\n",
       "             ('lr2.bias', tensor([-0.2744,  0.2594]))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################\n",
    "# state_dict load\n",
    "#####################\n",
    "sd = torch.load(\"saved_models/my_model_parameter.pt\")  #weight_only=True (default)\n",
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load한 state_dict를 모델 파라미터에 적용(덮어 씌운다.)\n",
    "new_model = MyModel()\n",
    "# new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('lr1.weight',\n",
       "              tensor([[-0.0990, -0.2017,  0.3291],\n",
       "                      [ 0.3617,  0.4953,  0.0188],\n",
       "                      [ 0.1496, -0.5085,  0.0211],\n",
       "                      [-0.3762,  0.0578, -0.0185]])),\n",
       "             ('lr1.bias', tensor([ 0.2466,  0.1807, -0.4694, -0.3903])),\n",
       "             ('lr2.weight',\n",
       "              tensor([[-0.1697, -0.2790,  0.1483,  0.3806],\n",
       "                      [ 0.0087, -0.2120, -0.2670, -0.0226]])),\n",
       "             ('lr2.bias', tensor([-0.2744,  0.2594]))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (lr1): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (lr2): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 48ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 30ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchinfo\u001b[0m\u001b[2m==1.8.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# torchinfo 패키지 설치: 파이토치 모델 구조를 조사해주는 패키지.\n",
    "!uv pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MyModel                                  --\n",
       "├─Linear: 1-1                            16\n",
       "├─Linear: 1-2                            10\n",
       "├─ReLU: 1-3                              --\n",
       "=================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyModel                                  [100, 2]                  --\n",
       "├─Linear: 1-1                            [100, 4]                  16\n",
       "├─ReLU: 1-2                              [100, 4]                  --\n",
       "├─Linear: 1-3                            [100, 2]                  10\n",
       "==========================================================================================\n",
       "Total params: 26\n",
       "Trainable params: 26\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input data 의 shape을 지정하면 각 Layer의 output shape을 출력한다.\n",
    "summary(model, (100, 3))  # input_data=입력Tensor객체, input_data=torch.randn(100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 문제 유형별 MLP 네트워크\n",
    "- 해결하려는 문제 유형에 따라 출력 Layer의 구조가 바뀐다.\n",
    "- 딥러닝 구조에서 **Feature를 추출하는 Layer 들을 Backbone** 이라고 하고 **추론하는 Layer들을 Head** 라고 한다. \n",
    "\n",
    "\n",
    "> - MLP(Multi Layer Perceptron), DNN(Deep Neural Network), ANN(Artificial Neural Network)\n",
    ">     -   Fully Connected Layer(nn.Linear)로 구성된 딥러닝 모델\n",
    ">     -   input feature들 모두에 대응하는 weight들(가중치)을 사용한다.\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Boston Housing Dataset - **Regression(회귀) 문제**\n",
    "\n",
    "보스턴 주택가격 dataset은 다음과 같은 속성을 바탕으로 해당 타운 주택 가격의 중앙값을 예측하는 문제.\n",
    "\n",
    "-   CRIM: 범죄율\n",
    "-   ZN: 25,000 평방피트당 주거지역 비율\n",
    "-   INDUS: 비소매 상업지구 비율\n",
    "-   CHAS: 찰스강에 인접해 있는지 여부(인접:1, 아니면:0)\n",
    "-   NOX: 일산화질소 농도(단위: 0.1ppm)\n",
    "-   RM: 주택당 방의 수\n",
    "-   AGE: 1940년 이전에 건설된 주택의 비율\n",
    "-   DIS: 5개의 보스턴 직업고용센터와의 거리(가중 평균)\n",
    "-   RAD: 고속도로 접근성\n",
    "-   TAX: 재산세율\n",
    "-   PTRATIO: 학생/교사 비율\n",
    "-   B: 흑인 비율\n",
    "-   LSTAT: 하위 계층 비율\n",
    "    <br><br>\n",
    "-   **Target**\n",
    "    -   MEDV: 타운의 주택가격 중앙값(단위: 1,000달러)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/boston_housing.csv')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506, 1))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns='MEDV').values\n",
    "y = df['MEDV'].values.reshape(-1, 1)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 102)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/Test Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train_scaled, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test_scaled, dtype=torch.float32),\n",
    "    torch.tensor(y_test, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "len(trainset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 생성\n",
    "batch_size = 200\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# 모델 정의\n",
    "##########################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class BostonModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(13, 32) # 첫번째 연산\n",
    "        self.lr2 = nn.Linear(32, 16)\n",
    "        self.lr3 = nn.Linear(16, 1)   # 추론 결과를 출력 \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Linear -> ReLU -> Linear -> ReLU -> Liear -> 출력\n",
    "        out = self.lr1(X)\n",
    "        out = self.relu(out)\n",
    "        out = self.lr2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.lr3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "lr = 0.001\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "# 모델 생성\n",
    "boston_model = ().to(device)\n",
    "# Loss 함수BostonModel\n",
    "loss_fn = nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.RMSprop(boston_model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "BostonModel                              [200, 1]                  --\n",
       "├─Linear: 1-1                            [200, 32]                 448\n",
       "├─ReLU: 1-2                              [200, 32]                 --\n",
       "├─Linear: 1-3                            [200, 16]                 528\n",
       "├─ReLU: 1-4                              [200, 16]                 --\n",
       "├─Linear: 1-5                            [200, 1]                  17\n",
       "==========================================================================================\n",
       "Total params: 993\n",
       "Trainable params: 993\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.20\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 0.08\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.09\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(boston_model, (200, 13), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/1000] train loss: 594.42477, validation loss: 515.35468\n",
      "[101/1000] train loss: 23.58760, validation loss: 23.97379\n",
      "[201/1000] train loss: 14.93035, validation loss: 16.34781\n",
      "[301/1000] train loss: 10.74346, validation loss: 12.26301\n",
      "[401/1000] train loss: 8.04014, validation loss: 10.85579\n",
      "[501/1000] train loss: 6.96515, validation loss: 10.33121\n",
      "[601/1000] train loss: 6.27835, validation loss: 10.23465\n",
      "[701/1000] train loss: 5.61462, validation loss: 9.93723\n",
      "[801/1000] train loss: 5.18579, validation loss: 9.94390\n",
      "[901/1000] train loss: 4.64167, validation loss: 9.82463\n",
      "[1000/1000] train loss: 4.28705, validation loss: 9.83201\n"
     ]
    }
   ],
   "source": [
    "# Train -> Train / Validataion 두 단계\n",
    "for epoch in range(epochs):\n",
    "    #####################################\n",
    "    # 학습(train)\n",
    "    #####################################\n",
    "    boston_model.train()\n",
    "    train_loss = 0.0 # 현 에폭의 loss을 저장할 변수.\n",
    "    for X_train, y_train in train_loader:  # 1 배치 학습\n",
    "        # 1. X, y를 device로 이동. (model 과 같은 device로 이동)\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        # 2. 추론\n",
    "        pred_train = boston_model(X_train)\n",
    "        # 3. Loss 계산\n",
    "        loss = loss_fn(pred_train, y_train)\n",
    "        # 4. 역전파로 gradient 계산\n",
    "        loss.backward()\n",
    "        # 5. 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        # 6. gradient 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 7. 현재 batch에대한 loss 계산\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    #####################################\n",
    "    # 검증(valiation) - 1 에폭 학습한 것에 대한 검증\n",
    "    #####################################\n",
    "    boston_model.eval() # 추론만 할 때.\n",
    "\n",
    "    valid_loss = 0.0 # 현재 epoch에 대한 검증 loss값을 저장할 변수\n",
    "    with torch.no_grad():\n",
    "        for X_valid, y_valid in test_loader:\n",
    "            # 1. X, y를 model의 device로 이동\n",
    "            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "            \n",
    "            # 2. 추론\n",
    "            pred_valid = boston_model(X_valid)\n",
    "\n",
    "            # 3. 평가 - MSE\n",
    "            valid_loss  += loss_fn(pred_valid, y_valid).item()\n",
    "        valid_loss = valid_loss / len(test_loader)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "    # 로그 출력 - train/valid loss를 출력\n",
    "    if epoch % 100 == 0 or epoch == epochs-1:\n",
    "        print(f\"[{epoch+1}/{epochs}] train loss: {train_loss:.5f}, validation loss: {valid_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_boston_model_path = \"saved_models/boston_model.pth\"\n",
    "torch.save(boston_model, save_boston_model_path) # 모델 전체 저장\n",
    "torch.save(boston_model.state_dict(), \"saved_models/bost_model_parameter.pth\") # 모델 파라미터만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[26.8538],\n",
      "        [35.7528],\n",
      "        [15.9317],\n",
      "        [24.0498],\n",
      "        [16.7013],\n",
      "        [19.5364],\n",
      "        [17.8356],\n",
      "        [15.4498],\n",
      "        [26.5210],\n",
      "        [18.9611]])\n"
     ]
    }
   ],
   "source": [
    "load_model = torch.load(save_boston_model_path, weights_only=False)\n",
    "load_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(load_model(X_valid)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[26.8538],\n",
      "        [35.7528],\n",
      "        [15.9317],\n",
      "        [24.0498],\n",
      "        [16.7013],\n",
      "        [19.5364],\n",
      "        [17.8356],\n",
      "        [15.4498],\n",
      "        [26.5210],\n",
      "        [18.9611]])\n"
     ]
    }
   ],
   "source": [
    "load_state_dict = torch.load(\"saved_models/bost_model_parameter.pth\", weights_only=True)\n",
    "new_model = BostonModel()\n",
    "new_model.load_state_dict(load_state_dict)\n",
    "new_model.eval()\n",
    "with torch.no_grad():\n",
    "    result = new_model(X_valid)[:10]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 분류 (Classification)\n",
    "\n",
    "### Fashion MNIST Dataset - **다중분류(Multi-Class Classification) 문제**\n",
    "\n",
    "10개의 범주(category)와 70,000개의 흑백 이미지로 구성된 [패션 MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터셋.\n",
    "이미지는 해상도(28x28 픽셀)가 낮고 다음처럼 개별 의류 품목을 나타낸다:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>그림</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">패션-MNIST 샘플</a> (Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Feature**이미지는 28x28 크기이며 Gray scale이다.\n",
    "- **Target**은 총 10개의 class로 구성되어 있으며 각 class의 class 이름은 다음과 같다.\n",
    "\n",
    "| 레이블 | 클래스       |\n",
    "|--------|--------------|\n",
    "| 0      | T-shirt/top |\n",
    "| 1      | Trousers    |\n",
    "| 2      | Pullover    |\n",
    "| 3      | Dress       |\n",
    "| 4      | Coat        |\n",
    "| 5      | Sandal      |\n",
    "| 6      | Shirt       |\n",
    "| 7      | Sneaker     |\n",
    "| 8      | Bag         |\n",
    "| 9      | Ankle boot  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### 학습 도중 모델 저장\n",
    ">\n",
    "> - 학습 도중 가장 좋은 성능을 보이는 모델이 나올 수 있다.\n",
    "> - 학습 도중 모델을 저장하는 방법\n",
    ">   1. 각 에폭이 끝날 때 마다 모델을 저장한다.\n",
    ">   2. 한 에폭 학습 후 성능 개선이 있으면 모델을 저장하여 가장 성능 좋은 모델만 저장되도록 한다.\n",
    ">      - 최고 성능 점수(best score)와 현재 에폭의 성능을 비교하여, 성능이 개선되었을 경우 모델을 저장(덮어쓰기)한다.\n",
    ">\n",
    "> #### 조기 종료(Early Stopping)\n",
    ">\n",
    "> - 학습 도중 성능 개선이 나타나지 않으면, 중간에 학습을 종료하도록 구현한다.\n",
    "> - 에폭 수를 충분히 길게 설정한 뒤, 특정 횟수 동안 성능 개선이 없으면 학습을 조기 종료하도록 구현한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"mps\" if torch.backend.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000, 10000, 195, 40)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset \n",
    "batch_size = 256\n",
    "dataset_path = \"datasets/fashion_mnist\"\n",
    "f_trainset = FashionMNIST(dataset_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "f_testset = FashionMNIST(dataset_path, train=False, download=True, transform=transforms.ToTensor())\n",
    "f_trainset, f_validset = random_split(f_trainset, [50000, 10000])\n",
    "\n",
    "f_train_loader = DataLoader(f_trainset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "f_valid_loader = DataLoader(f_validset, batch_size=batch_size)\n",
    "f_test_loader = DataLoader(f_testset, batch_size=batch_size)\n",
    "\n",
    "len(f_trainset), len(f_testset), len(f_validset), len(f_train_loader), len(f_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T-shirt/top': 0,\n",
       " 'Trouser': 1,\n",
       " 'Pullover': 2,\n",
       " 'Dress': 3,\n",
       " 'Coat': 4,\n",
       " 'Sandal': 5,\n",
       " 'Shirt': 6,\n",
       " 'Sneaker': 7,\n",
       " 'Bag': 8,\n",
       " 'Ankle boot': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_testset.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_testset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([1, 28, 28])\n",
      "tensor(0.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(f_trainset[0][0].dtype)\n",
    "print(f_trainset[0][0].size())\n",
    "print(f_trainset[0][0].min(), f_trainset[0][0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# 모델 정의\n",
    "##################################\n",
    "class FashionMNISTModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lr1 = nn.Linear(784, 2048)\n",
    "        self.lr2 = nn.Linear(2048, 1024)\n",
    "        self.lr3 = nn.Linear(1024, 512)\n",
    "        self.lr4 = nn.Linear(512, 256)\n",
    "        self.lr5 = nn.Linear(256, 128)\n",
    "        self.lr6 = nn.Linear(128, 64)\n",
    "        self.lr7 = nn.Linear(64, 10)\n",
    "        # 출력 Layer out_features 개수: 다중분류문제의 경우 정답 클래스 개수.\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X shape: (batch, 1, 28, 28) -> (batch, 784) torch.flatten(start_dim=1)함수\n",
    "        out = nn.Flatten()(X) # 0축은 그대로 두고 그 이후 축을 flatten 한다.\n",
    "        out = self.lr1(out)  # self.relu(self.lr1(out))\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.lr2(out) \n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.lr3(out) \n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.lr4(out) \n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.lr5(out) \n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.lr6(out) \n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.lr7(out) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "FashionMNISTModel                        [100, 10]                 --\n",
       "├─Linear: 1-1                            [100, 2048]               1,607,680\n",
       "├─ReLU: 1-2                              [100, 2048]               --\n",
       "├─Linear: 1-3                            [100, 1024]               2,098,176\n",
       "├─ReLU: 1-4                              [100, 1024]               --\n",
       "├─Linear: 1-5                            [100, 512]                524,800\n",
       "├─ReLU: 1-6                              [100, 512]                --\n",
       "├─Linear: 1-7                            [100, 256]                131,328\n",
       "├─ReLU: 1-8                              [100, 256]                --\n",
       "├─Linear: 1-9                            [100, 128]                32,896\n",
       "├─ReLU: 1-10                             [100, 128]                --\n",
       "├─Linear: 1-11                           [100, 64]                 8,256\n",
       "├─ReLU: 1-12                             [100, 64]                 --\n",
       "├─Linear: 1-13                           [100, 10]                 650\n",
       "==========================================================================================\n",
       "Total params: 4,403,786\n",
       "Trainable params: 4,403,786\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 440.38\n",
       "==========================================================================================\n",
       "Input size (MB): 0.31\n",
       "Forward/backward pass size (MB): 3.23\n",
       "Params size (MB): 17.62\n",
       "Estimated Total Size (MB): 21.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(FashionMNISTModel(), (100, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model = FashionMNISTModel().to(device)\n",
    "optimizer = torch.optim.Adam(f_model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "# 다중분류의 loss: CrossEntropyLoss()\n",
    "# CrossEntrpyLoss()입력: 정답-class index로 구성 [[3],[8],[0]..], 모델 추정결과: Softmax를 적용하기 전값 - logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100] train loss: 1.16591, valid loss: 0.74194, valid acc: 0.71790\n",
      ">>>>>> 모델 저장: 1 epoch - 이전 score: inf, 개선된 score: 0.7419377461075782\n",
      "[2/100] train loss: 0.64029, valid loss: 0.59728, valid acc: 0.79420\n",
      ">>>>>> 모델 저장: 2 epoch - 이전 score: 0.7419377461075782, 개선된 score: 0.5972842842340469\n",
      "[3/100] train loss: 0.55904, valid loss: 0.54403, valid acc: 0.80960\n",
      ">>>>>> 모델 저장: 3 epoch - 이전 score: 0.5972842842340469, 개선된 score: 0.5440327368676663\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "valid_acc_list = []\n",
    "\n",
    "##################################################################\n",
    "# 1. 성능(valid_loss)이 개선될 때 마다 모델을 저장.\n",
    "#    최종적으로 가장 성능이 좋은 epoch의 모델이 저장되도록한다.\n",
    "# 2. 일정 epoch동안 성능개선이 안되면 학습을 중단.\n",
    "# 이것과 관련된 변수들 정의.\n",
    "##################################################################\n",
    "best_score = torch.inf  # 가장 좋은 valid_loss 값을 저장.\n",
    "# 저장\n",
    "save_model_path = \"saved_models/fashion_mnist_model.pth\"\n",
    "# 조기종료\n",
    "patience = 10  # 성능이 개선되는지 10 에폭 동안 기다려본다.\n",
    "stop_count = 0 # 학습하는 도중 몇 번동안 성능개선이 안되었는지 저장할변수. stop_count == patience 이면 종료.\n",
    "\n",
    "epochs = 100\n",
    "s = time.time()\n",
    "for epoch in range(epochs):\n",
    "    # 학습\n",
    "    f_model.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for X_train, y_train in f_train_loader:\n",
    "        # 1. X, y를 device로 이동\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        # 2. 모델 추론\n",
    "        pred_train = f_model(X_train)\n",
    "        # 3. loss 계산\n",
    "        loss = loss_fn(pred_train, y_train)\n",
    "        # 4. gradient 계산  (역전파)\n",
    "        loss.backward()\n",
    "        # 5. 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        # 6. 파라미터 초기화\n",
    "        optimizer.zero_grad()\n",
    "        # 7. 현재 에폭에서의 train loss 누적\n",
    "        train_loss = train_loss + loss.item()\n",
    "\n",
    "    # train_loss 평균계산\n",
    "    train_loss = train_loss / len(f_train_loader)\n",
    "    train_loss_list.append(train_loss) \n",
    "\n",
    "    ## 검증\n",
    "    f_model.eval()\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_valid, y_valid in f_valid_loader:\n",
    "            # 1. device로 이동\n",
    "            X_valid, y_valid = X_valid.to(device), y_valid.to(device)\n",
    "            \n",
    "            # 2. 모델 추정\n",
    "            pred_valid = f_model(X_valid) # 10개 class에대한 logit값(확률).\n",
    "            pred_label = pred_valid.argmax(dim=-1)# 가장 확률높은 class\n",
    "            # 3. 평가 - loss와 정확도\n",
    "            valid_loss = valid_loss + loss_fn(pred_valid, y_valid).item() # 현 step에서의 loss\n",
    "            valid_acc = valid_acc + torch.sum(pred_label == y_valid).item() # 현 step에서 맞은것 개수.\n",
    "\n",
    "        valid_loss = valid_loss / len(f_valid_loader)\n",
    "        valid_acc = valid_acc / len(f_validset)\n",
    "        valid_loss_list.append(valid_loss)\n",
    "        valid_acc_list.append(valid_acc)\n",
    "\n",
    "        #로그출력 - train_loss, valid_loss, valid_acc\n",
    "        print(f\"[{epoch+1}/{epochs}] train loss: {train_loss:.5f}, valid loss: {valid_loss:.5f}, valid acc: {valid_acc:.5f}\")\n",
    "\n",
    "        ################################\n",
    "        # 모델 저장, 조기종료\n",
    "        ################################\n",
    "        if valid_loss < best_score: # 이번 epoch 학습에서 성능(valid_loss)이 개선되었다.\n",
    "            # 모델 저장\n",
    "            torch.save(f_model, save_model_path)\n",
    "            print(f\">>>>>> 모델 저장: {epoch+1} epoch - 이전 score: {best_score}, 개선된 score: {valid_loss}\")\n",
    "            best_score = valid_loss\n",
    "            # 조기종료 (stop_count 를 초기)\n",
    "            stop_count = 0\n",
    "        else: # 성능개선이 안됨. -> 조기종료관련 처리.\n",
    "            stop_count += 1\n",
    "            if stop_count == patience:\n",
    "                print(f\">>>>>>>>>>>>> 조기종료: valid loss가 {best_score} 보다 개선되지 않음.\")\n",
    "                break\n",
    "\n",
    "\n",
    "e = time.time()\n",
    "print('학습에 걸린시간:', (e-s), \"초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "result_dict = {\n",
    "    \"train_loss\": train_loss_list,\n",
    "    \"valid_acc\": valid_acc_list,\n",
    "    \"valid_loss\": valid_loss_list\n",
    "}\n",
    "\n",
    "with open(\"f_mnist_train_result.pkl\", 'wb') as fo:\n",
    "    pickle.dump(result_dict, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 위스콘신 유방암 데이터셋 - **이진분류(Binary Classification) 문제**\n",
    "\n",
    "-   **이진 분류 문제 처리 모델의 두가지 방법**\n",
    "    1. positive(1)일 확률을 출력하도록 구현\n",
    "        - output layer: units=1, activation='sigmoid'\n",
    "        - loss: binary_crossentropy\n",
    "    2. negative(0)일 확률과 positive(1)일 확률을 출력하도록 구현 => 다중분류 처리 방식으로 해결\n",
    "        - output layer: units=2, activation='softmax', y(정답)은 one hot encoding 처리\n",
    "        - loss: categorical_crossentropy\n",
    "-   위스콘신 대학교에서 제공한 종양의 악성/양성여부 분류를 위한 데이터셋\n",
    "-   Feature\n",
    "    -   종양에 대한 다양한 측정값들\n",
    "-   Target의 class\n",
    "    -   0 - malignant(악성종양)\n",
    "    -   1 - benign(양성종양)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "y = y.reshape(-1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "## 모델의 weight, bias -> float32. X, y는 weight, bias와 계산을 하게 되기 때문에 타입을 맞춰준다.\n",
    "trainset = TensorDataset(\n",
    "    torch.tensor(X_train_scaled, dtype=torch.float32),  \n",
    "    torch.tensor(y_train, dtype=torch.float32)\n",
    ")\n",
    "testset = TensorDataset(\n",
    "    torch.tensor(X_test_scaled, dtype=torch.float32), \n",
    "    torch.tensor(y_test, dtype=torch.float32)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class name <-> class index\n",
    "classes = np.array([\"악성종양\", \"양성종양\"])\n",
    "class_to_idx = {\"악성종양\":0, \"양성종양\":1}\n",
    "\n",
    "trainset.classes = classes\n",
    "trainset.class_to_idx = class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_loader = DataLoader(trainset, batch_size=200, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(testset, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 모델 정의\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######### 저장된 모델 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 추론 함수 #######\n",
    "def predict_bc(model, X, device=\"cpu\"):\n",
    "    # model로 X를 추론한 결과를 반환\n",
    "    # label, 확률\n",
    "    result = []\n",
    "    with torch.no_grad():\n",
    "        pred_proba = model(X)\n",
    "        pred_class = (pred_proba > 0.5).type(torch.int32)\n",
    "        for class_index, proba in zip(pred_class, pred_proba):\n",
    "            # print(class_index, proba if class_index.item() == 1 else 1-proba)\n",
    "            result.append((class_index.item(), proba if class_index.item() == 1 else 1-proba))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = torch.tensor(X_test_scaled[:5], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 유형별 구현 정리\n",
    "\n",
    "## 공통\n",
    "\n",
    "-   Input layer(첫번째 Layer)의 in_features\n",
    "    -   입력데이터의 feature(속성) 개수에 맞춰준다.\n",
    "-   Hidden layer 수\n",
    "    -   경험적(art)으로 정한다.\n",
    "    -   Hidden layer에 Linear를 사용하는 경우 보통 feature 수를 줄여 나간다. (핵심 특성들을 추출해나가는 과정의 개념.)\n",
    "\n",
    "## 회귀 모델\n",
    "\n",
    "-   output layer의 출력 unit개수(out_features)\n",
    "    -   정답의 개수\n",
    "    -   ex\n",
    "        -   집값: 1\n",
    "        -   아파트가격, 단독가격, 빌라가격: 3 => y의 개수에 맞춘다.\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   일반적으로 **None**\n",
    "    -   값의 범위가 설정되 있고 그 범위의 값을 출력하는 함수가 있을 경우\n",
    "        -   ex) 0 ~ 1: logistic(Sigmoid), -1 ~ 1: hyperbolic tangent(Tanh)\n",
    "-   loss함수\n",
    "    -   MSELoss\n",
    "-   평가지표\n",
    "    -   MSE, RMSE, R square($R^2$)\n",
    "\n",
    "## 다중분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   정답 class(고유값)의 개수\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Softmax: 클래스별 확률을 출력\n",
    "-   loss함수\n",
    "    -   **categrocial crossentropy**\n",
    "    -   파이토치 함수\n",
    "        -   **CrossEntropyLoss** = NLLLoss(정답) + LogSoftmax(모델 예측값)\n",
    "        -   **NLLLoss**\n",
    "            -   정답을 OneHot Encoding 처리 후 Loss를 계산한다.\n",
    "            -   입력으로 LogSoftmax 처리한 모델 예측값과 onehot encoding 안 된 정답을 받는다.\n",
    "        -   **LogSoftmax**\n",
    "            -   입력값에 Softmax 계산후 그 Log를 계산한다.\n",
    "                -   NLLLoss의 모델 예측값 입력값으로 처리할 때 사용한다.\n",
    "\n",
    "```python\n",
    "pred = model(input)\n",
    "loss1 = nn.NLLLoss(nn.LogSoftmax(dim=-1)(pred), y)\n",
    "# or\n",
    "loss2 = nn.CrossEntropyLoss()(pred, y)\n",
    "```\n",
    "\n",
    "## 이진분류 모델\n",
    "\n",
    "-   output layer의 unit 개수\n",
    "    -   1개 (positive일 확률)\n",
    "-   출력 Layer에 적용하는 activation 함수\n",
    "    -   Sigmoid(Logistic)\n",
    "-   loss 함수\n",
    "    -   **Binary crossentropy**\n",
    "    -   파이토치 함수: **BCELoss**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
