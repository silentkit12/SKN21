{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4766ebb-432c-4bdb-8050-23df797098be",
   "metadata": {},
   "source": [
    "<!-- # Langchain은 다양한 LLM(대규모 언어 모델)을 지원한다\n",
    "-\t대규모 언어 모델(LLM, Large Language Model)을 개발하는 회사들은 사용자가 자신의 애플리케이션에서 LLM을 손쉽게 활용할 수 있도록 API(Application Programming Interface) 서비스를 제공하고 있다.\n",
    "-\t하지만 각 LLM은 고유한 API 호출 라이브러리(Library)를 제공하기 때문에, 개발자는 동일한 작업을 수행하더라도 LLM에 따라 다른 코드를 작성해야 하는 번거로움이 있다.\n",
    "-\tLangchain은 이러한 문제를 해결하기 위해 다양한 LLM의 API를 통합적으로 지원한다.\n",
    "-\t여러 LLM을 동일한 인터페이스(interface)로 호출할 수 있게 하여 특정 모델에 종속되지 않도록 하고, 필요에 따라 쉽게 다른 모델로 전환할 수 있다.\n",
    "-\tLangchain이 지원하는 주요 LLM 목록\n",
    "    - https://python.langchain.com/docs/integrations/chat/#featured-providers -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3558fa02",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'.venv (Python 3.12.12)'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m'ipykernel'을(를) Python 환경에 설치합니다. \n",
      "\u001b[1;31m명령: 'c:/Users/USER/Documents/SKN21/10_langchain/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 가상환경 실행\n",
    "\n",
    "# uv pip install ipykernel ipywidgets\n",
    "\n",
    "# 오.. 아이파이커널은 이렇게 못 하는구나 ㅋㅋ\n",
    "# '.venv (Python 3.12.12)'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
    "# 'ipykernel'을(를) Python 환경에 설치합니다. \n",
    "# 명령: 'c:/Users/USER/Documents/SKN21/10_langchain/.venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'\n",
    "# 경고문 떴음 ㅋㅋ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd979cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uv pip install langchain langchain-classic langchain-community python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9025ef1-ede0-4f3b-b8fe-1b9e348391ba",
   "metadata": {},
   "source": [
    "# OpenAI 모델 사용\n",
    "- https://platform.openai.com\n",
    "  \n",
    "## 결제\n",
    "1. 로그인 후 Billing 페이지로 이동.\n",
    "   - **신용카드가 등록된 경우 바로 충전할 수 있다.**\n",
    "   - setting -> Billing\n",
    "  \n",
    "   ![openai_payment.png](figures/openai_payment.png)\n",
    "   - 금액 입력 후 `continue`\n",
    "   - 다음 페이지에서 세금 포함 결제 금액 나오면 `Confirm`  한다.\n",
    "\n",
    "2. 신용카드 등록\n",
    "   - Payment methods 탭을 선택하고 카드를 등록한다. \n",
    "   \n",
    "   ![openai_payment2.png](figures/openai_payment2.png)\n",
    "\n",
    "   - 등록이 끝나면 최초 구매를 진행한다. $5 ~ $100 사이의 금액을 선택할 수 있다.\n",
    "   - 자동 충전을 설정하고 싶다면 automatic recharge 를 활성화 하고 아래 추가 설정에 입력한다. \n",
    "     - 자동 충전은 특정 금액 이하로 떨어지면 자동으로 충전한다. (**비활성화**) \n",
    "  \n",
    "   ![openai_payment3.png](figures/openai_payment3.png)\n",
    "   \n",
    "3. 수동으로 **추가 결제하기**\n",
    "   - Billing 페이지의 Overview에서 `Add to credit balance` 를 클릭한 뒤 금액을 입력하고 결제한다.\n",
    "\n",
    "## 사용량 확인\n",
    "- profile/설정 -> Usage 에서 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f99e0-d15b-4769-8d2c-b6e0a9a26237",
   "metadata": {},
   "source": [
    "## API Key 생성\n",
    "  \n",
    "![openai_create_apikey.png](figures/openai_create_apikey.png)\n",
    "\n",
    "- 로그인 -> Dashboard -> API Keys -> Create New Secreat Key\n",
    "> Settings -> API Keys\n",
    "\n",
    "## API Key 등록\n",
    "- 환경변수에 등록\n",
    "  - 변수이름: OPENAI_API_KEY\n",
    "  - 값: 생성된 키\n",
    "- dotenv를 이용해서 load\n",
    "  - Working directory에  `.env` 파일 생성하고 `OPENAI_API_KEY=생성된키` 추가한다.\n",
    "  - load_dotenv() 호출 하면 .env 파일에 있는 값을 읽은 뒤 환경변수로 등록한다.\n",
    "- **주의**\n",
    "  - 생성된 API Key는 노출되면 안된다.\n",
    "  - API Key가 저장된 파일(코드나 설정파일)이 github에 올라가 공개되서는 안된다.\n",
    "\n",
    "## 사용 비용 확인\n",
    "- settings -> Usage 에서 확인\n",
    "\n",
    "## OpenAI LLM 모델들\n",
    "-  OpenAI LLM 모델: https://platform.openai.com/docs/models\n",
    "-  모델별 가격: https://platform.openai.com/docs/pricing\n",
    "-  토큰사이즈 확인: https://platform.openai.com/tokenizer\n",
    "   -  1토큰: 영어 3\\~4글자 정도, 한글: 대략 1\\~2글자 정도\n",
    "   -  모델이 업데이트 되면서 토큰 사이즈도 조금씩 커지고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5a12d-8e7d-4a29-ac91-edb62c56bfe7",
   "metadata": {},
   "source": [
    "## OpenAI 를 연동하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-openai -qU\n",
    "```\n",
    "\n",
    "- OpenAI 자체 라이브러리 설치\n",
    "    - `pip install openai -qU`\n",
    "    - langchain-openai를 설치하면 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29772e79-1e31-421d-b5c1-625dc029c395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API Key Load -> OS 환경변수로 등록(OPEN_API_KEY)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67858ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getenv('OPENAI_API_KEY') # 키 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486e5a1-6c59-499d-a275-41060c8b0a8a",
   "metadata": {},
   "source": [
    "## OpenAI Library 를 이용한 API 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c9bb769-eb14-43d5-9c26-98bb77153b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI() #api key 가 환경변수(.env)에 없으면 직접 입력\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-5-nano\", #사용할 모델 종류의 이름\n",
    "    messages=[\n",
    "        {\"role\":\"user\", \"content\":\"OpenAI의 LLM 모델 종류는 뭐가 있어?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "#chat 요청 프롬프트 형식\n",
    "#{\"role\":\"누가 말하는지-user, ai, system\"}\n",
    "# \"content\": 메세지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "851be9d2-58e5-464b-87cb-52f09a9a146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-CmsuVN9xG1quJwXlqYyKpspzHdLq3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='간단히 정리하면 OpenAI의 LLM은 크게 몇 가지 계열로 나뉩니다. 아래는 자주 쓰는 주요 모델들(2024년 중반까지)입니다. 실제 이용 가능 여부와 버전은 API 문서에서 확인해 주세요.\\n\\n- GPT-4 계열\\n  - gpt-4: 일반 용도\\n  - gpt-4-0314: 특정 빌드 버전\\n  - gpt-4-32k: 더 큰 컨텍스트(토큰) 지원\\n  - gpt-4-32k-0314: 32k 버전의 빌드\\n  - gpt-4o: 멀티모달 모델(이미지 입력 가능 등)\\n\\n- GPT-3.5 계열\\n  - gpt-3.5-turbo: 성능/비용 균형이 좋은 일반용 모델\\n  - gpt-3.5-turbo-16k: 16k 토큰 컨텍스트 확장\\n  - text-davinci-003: 고급 텍스트 생성의 구형 GPT-3.5 계열\\n  - text-davinci-002: 구형 버전(역사적 모델)\\n\\n- Codex / 코드 관련 모델\\n  - code-davinci-002: 코드 작성/수정에 강한 상위 모델\\n  - code-davinci-001: 초기 버전\\n  - code-cushman-001: 경량 코드 모델\\n\\n- 텍스트 기본 계열(구형 엔진)\\n  - text-curie-001, text-babbage-001, text-ada-001: 각각 속도/비용과 성능의 중간/기초 모델\\n\\n- 임베딩용 모델\\n  - text-embedding-ada-002\\n  - text-embedding-ada-003\\n\\n참고로:\\n- 모델 이름은 OpenAI API의 업데이트에 따라 변경되거나 추가될 수 있습니다.\\n- 가장 적합한 모델은 사용 사례(챗 대화, 요약, 코드 작성, 문장 생성 등)와 예산에 따라 달라집니다.\\n- 최신 목록과 각 모델의 특징은 OpenAI 공식 API 문서나 콘솔의 모델 목록에서 확인하는 것을 권장드립니다. 필요하시면 사용 사례별 추천 가이드를 같이 정리해 드릴게요.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1765766855, model='gpt-5-nano-2025-08-07', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=3328, prompt_tokens=18, total_tokens=3346, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=2816, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c7056dd-ddc5-4c0f-8653-47be64287d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "간단히 정리하면 OpenAI의 LLM은 크게 몇 가지 계열로 나뉩니다. 아래는 자주 쓰는 주요 모델들(2024년 중반까지)입니다. 실제 이용 가능 여부와 버전은 API 문서에서 확인해 주세요.\n",
      "\n",
      "- GPT-4 계열\n",
      "  - gpt-4: 일반 용도\n",
      "  - gpt-4-0314: 특정 빌드 버전\n",
      "  - gpt-4-32k: 더 큰 컨텍스트(토큰) 지원\n",
      "  - gpt-4-32k-0314: 32k 버전의 빌드\n",
      "  - gpt-4o: 멀티모달 모델(이미지 입력 가능 등)\n",
      "\n",
      "- GPT-3.5 계열\n",
      "  - gpt-3.5-turbo: 성능/비용 균형이 좋은 일반용 모델\n",
      "  - gpt-3.5-turbo-16k: 16k 토큰 컨텍스트 확장\n",
      "  - text-davinci-003: 고급 텍스트 생성의 구형 GPT-3.5 계열\n",
      "  - text-davinci-002: 구형 버전(역사적 모델)\n",
      "\n",
      "- Codex / 코드 관련 모델\n",
      "  - code-davinci-002: 코드 작성/수정에 강한 상위 모델\n",
      "  - code-davinci-001: 초기 버전\n",
      "  - code-cushman-001: 경량 코드 모델\n",
      "\n",
      "- 텍스트 기본 계열(구형 엔진)\n",
      "  - text-curie-001, text-babbage-001, text-ada-001: 각각 속도/비용과 성능의 중간/기초 모델\n",
      "\n",
      "- 임베딩용 모델\n",
      "  - text-embedding-ada-002\n",
      "  - text-embedding-ada-003\n",
      "\n",
      "참고로:\n",
      "- 모델 이름은 OpenAI API의 업데이트에 따라 변경되거나 추가될 수 있습니다.\n",
      "- 가장 적합한 모델은 사용 사례(챗 대화, 요약, 코드 작성, 문장 생성 등)와 예산에 따라 달라집니다.\n",
      "- 최신 목록과 각 모델의 특징은 OpenAI 공식 API 문서나 콘솔의 모델 목록에서 확인하는 것을 권장드립니다. 필요하시면 사용 사례별 추천 가이드를 같이 정리해 드릴게요.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77416bf-be44-421b-bed1-c1da8512e983",
   "metadata": {},
   "source": [
    "## Langchain을 이용한 OpenAI API 호출\n",
    "\n",
    "- **ChatOpenAI**\n",
    "    - chat (대화-채팅) 기반 모델 model.\n",
    "    - Default 로 gpt-3.5-turbo 사용\n",
    "    - llm 전달 입력과 llm 응답 출력 타입:  Message\n",
    "> - **OpenAI**\n",
    ">     - 문장 완성 모델. (text completion) model\n",
    ">     - Default로 gpt-3.5-turbo-instruct 사용\n",
    ">       - instruct 모델만 사용가능\n",
    ">     - llm전달 입력과 llm 응답 출력 타입: str\n",
    "- Initializer 주요 파라미터\n",
    "    -  **temperature**\n",
    "        -  llm 모델의 출력 무작위성을 지정한다. \n",
    "        -  0 ~ 2 사이 실수를 설정하며 클 수록 무작위성이 커진다. 기본값: 0.7\n",
    "        -  정확한 답변을 얻어야 하는 경우 작은 값을 창작을 해야 하는 경우 큰 값을 지정한다.\n",
    "    -  **model_name**\n",
    "        -  사용할 openai 모델 지정\n",
    "    - **max_tokens**:\n",
    "        - llm 모델이 응답할 최대 token 수.\n",
    "        - gpt-5 모델은 추론과정을 거칠 수있고 이 경우 토큰을 소비한다. 그래서 max_tokens를 너무 작게 잡으면 응답이 안올 수있다.\n",
    "    - **api_key**\n",
    "        - OpenAI API key를 직접 입력해 생성시 사용.\n",
    "        - API key가 환경변수에 설정 되있으면 생략한다. \n",
    "-  메소드\n",
    "    - **`invoke(message)`** : LLM에 질의 메세지를 전달하며 LLM의 응답을 반환한다.\n",
    "> - **Message**\n",
    ">     - Langchain 다양한 상황과 작업 마다 다양한 값들로 구성된 입출력 데이터를 만든다. \n",
    ">     - Langchain은 그 상황들에 맞는 다양한 Message 클래스를 제공한다. 이것을 이용하면 특정 작업에 적합한 입력값을 설정할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "124af8a3-d624-4399-8062-35a6d60c63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\"\n",
    ")\n",
    "#질의\n",
    "response=model.invoke(\"Langchain은 어떤 Framework인지 설명해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1948856-b1e4-4da0-9311-11f798b66a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "LangChain은 대형 언어 모델(LLM)을 활용한 애플리케이션을 쉽게 만들 수 있도록 도와주는 오픈소스 프레임워크입니다.\n",
      "\n",
      "주요 포인트\n",
      "- 목적: 복잡한 LLM 기반 워크플로우를 구성하는 데 필요한 여러 구성 요소를 표준화하고 연결해주는 라이브러리입니다. 예를 들어 챗봇, 문서 검색 기반 Q&A, 자동화 도구 등 다양한 애플리케이션을 빠르게 만들 수 있습니다.\n",
      "- 핵심 아이디어: LLM 호출을 여러 단계(프롬프트, 코드 실행, API 호출 등)로 체인처럼 연결하고, 필요하면 외부 도구를 사용하게 하며, 상태를 유지해야 하는 대화형 애플리케이션에선 메모리도 다룰 수 있습니다.\n",
      "\n",
      "주요 구성 요소\n",
      "- LLM 래퍼/프롬프트: OpenAI, Cohere, Azure 등 다양한 공급자의 모델을 감싸고, PromptTemplate 같은 도구로 프롬프트를 구성합니다.\n",
      "- 체인(Chains): LLM 호출을 순차적으로 실행하는 구조입니다. 예: LLMChain, SequentialChain, 조건부 체인 등.\n",
      "- 에이전트(Agents)와 도구(Tools): 에이전트가 상황에 맞춰 외부 도구(API 호출, 계산기 사용, 코드 실행 등)를 선택하고 실행하도록 만듭니다. Tools는 실제로 수행할 작업의 추상화입니다.\n",
      "- 메모리(Memory): 대화 맥락이나 상태를 대화 간에 유지하기 위한 저장소로, ConversationMemory, SummaryMemory 등이 있습니다.\n",
      "- 벡터 스토어/검색(Retrieval): 문서 임베딩과 벡터 DB를 사용해 정보 검색 기반의 RAG( Retrieval-Augmented Generation) 워크플로우를 구성합니다.\n",
      "- 문서 로더와 인덱스/리트리버(Re- triever): PDF, 노션, 웹페이지 등 다양한 소스에서 문서를 불러와 인덱싱하고 검색합니다.\n",
      "- LangChain.js: Python 버전뿐 아니라 JavaScript/TypeScript 버전도 있어, 웹 애플리케이션이나 Node 환경에서도 사용 가능합니다.\n",
      "\n",
      "무엇을 할 수 있나?\n",
      "- 챗봇과 대화형 에이전트: 대화 맥락을 기억하고 필요에 따라 도구를 사용해 정보를 찾아 답변합니다.\n",
      "- 지식 기반 Q&A: 벡터 저장소를 이용해 문서에서 유사한 정보를 검색하고 그 결과를 LLM이 요약/응답합니다.\n",
      "- 자동화 워크플로우: API 호출, 계산, 데이터 처리 등을 에이전트가 스스로 조합해 задач을 해결하게 만듭니다.\n",
      "- 데이터 추출/문서 처리: 프롬프트와 체인을 이용해 문서에서 핵심 정보를 추출합니다.\n",
      "\n",
      "언어 지원 및 생태계\n",
      "- Python용 LangChain과 JavaScript/TypeScript용 LangChain.js 두 가지 주버전이 활발히 유지보수되고 있습니다.\n",
      "- 다양한 LLM 공급자, 벡터 DB, 문서 로더 및 도구와의 연동을 폭넓게 지원합니다.\n",
      "\n",
      "간단한 용도 예시\n",
      "- 사용자가 질의하면 문서를 벡터 검색으로 찾고, 그 결과를 바탕으로 LLM이 요약 및 답변을 제공합니다.\n",
      "- 챗봇이 외부 API를 호출해 실시간 정보를 가져오도록 도구를 사용합니다(예: 날씨 조회, 재고 조회).\n",
      "\n",
      "필요하면 예시 코드나 구체적인 구성 방법도 함께 설명해 드릴게요. 어느 언어나 어떤 시나리오를 염두에 두고 있는지 알려주면 맞춤 예시를 드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed81fb-5249-4bd3-91b0-4f86a0a75f5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Your organization must be verified to generate reasoning summaries. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': 'reasoning.summary', 'code': 'unsupported_value'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#reasoning 설정(gpt-5추가)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m##reasoning: 문제 전제 -> 논리를 전개 -> 결론을 도출 하는 과정을 스스로 계획/구성해서 답을 도출하는 방식\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# effot: 추론의 깊이(low, medium, high), summary: 추론 과정을 출력\u001b[39;00m\n\u001b[32m      5\u001b[39m model = ChatOpenAI(\n\u001b[32m      6\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgpt-5\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     reasoning = {\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33meffort\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mmedium\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m     }\n\u001b[32m     10\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLLM 모델에 대한 내용을 초심자를 위해 최대한 쉽게 설명해줘.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1382\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1380\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1381\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1384\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1385\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1386\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1387\u001b[39m ):\n\u001b[32m   1388\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1364\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1360\u001b[39m     raw_response = \u001b[38;5;28mself\u001b[39m.root_client.responses.with_raw_response.parse(\n\u001b[32m   1361\u001b[39m         **payload\n\u001b[32m   1362\u001b[39m     )\n\u001b[32m   1363\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1364\u001b[39m     raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1367\u001b[39m response = raw_response.parse()\n\u001b[32m   1368\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.include_response_headers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:866\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, prompt_cache_retention, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    829\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    830\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    864\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    865\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m866\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconversation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'Your organization must be verified to generate reasoning summaries. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': 'reasoning.summary', 'code': 'unsupported_value'}}"
     ]
    }
   ],
   "source": [
    "#reasoning 설정(gpt-5추가)\n",
    "##reasoning: 문제 전제 -> 논리를 전개 -> 결론을 도출 하는 과정을 스스로 계획/구성해서 답을 도출하는 방식\n",
    "# effort: 추론의 깊이(low, medium, high), summary: 추론 과정을 출력\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-5\",\n",
    "    reasoning = {\n",
    "        \"effort\":\"medium\",\"summary\":\"auto\"\n",
    "    }\n",
    ")\n",
    "result = model.invoke(\"LLM 모델에 대한 내용을 초심자를 위해 최대한 쉽게 설명해줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7775b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raesoning\n",
    "# content에 추론 내용과 응답 내용을 묶어서 반환.\n",
    "#type:reasoning -추론과정, text -최종응답\n",
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a89e961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "\n",
    "prompt = \"\"\" \n",
    "태양계를 구성하는 행성들에 대해 태양에서 가까운 순서대로 알려줘.\n",
    "\n",
    "결과는 다음 답변형식에 맞춘다.\n",
    "[답변형식]\n",
    "-행성한국어이름(영어이름): 간단한 한 줄 설명\n",
    "\"\"\"\n",
    "\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4192afad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-수성(Mercury): 태양에서 가장 가까운 암석 행성으로 낮과 밤의 온도 차가 매우 크다.\n",
      "-금성(Venus): 두꺼운 이산화탄소 대기와 강력한 온실효과로 매우 높은 표면온도를 가진 암석 행성.\n",
      "-지구(Earth): 생명이 존재하는 유일한 알려진 행성으로 액체 물과 산소 풍부한 대기를 가짐.\n",
      "-화성(Mars): 붉은 색의 철 산화물로 덮인 얇은 대기를 가진 암석 행성으로 과거에 물의 흔적이 있음.\n",
      "-목성(Jupiter): 태양계에서 가장 큰 가스 거성으로 강한 자기장과 많은 위성을 가짐.\n",
      "-토성(Saturn): 뚜렷한 고리 시스템을 가진 거대한 가스 행성.\n",
      "-천왕성(Uranus): 메탄으로 청록색을 띠며 자전축이 크게 기울어진 얼음 거대행성.\n",
      "-해왕성(Neptune): 강한 바람과 메탄으로 푸른빛을 띠는 태양에서 가장 먼 얼음 거대행성.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71861354-39b6-4a7d-842d-8939d3a3e5bc",
   "metadata": {},
   "source": [
    "# Hugging Face 모델 사용\n",
    "\n",
    "## Local 에 설치된 모델 사용\n",
    "- HuggingFacePipeline 에 Model id를 전달해 Model객체를 생성한다.\n",
    "- huggingface transformers 라이브러리를 이용해 model을 생성 한 뒤 HuggingFacePipeline 에 넣어 생성한다.\n",
    "- 모델이 local에 없는 경우 다운로드 받는다.\n",
    "\n",
    "### HuggingFace 모델을 사용하기 위한 package 설치\n",
    "```bash\n",
    "pip install langchain-huggingface transformers -qU\n",
    "```\n",
    "- nvidia GPU가 있는 경우 `torch cuda`  버전을 먼저 설치하고 `langchain-huggingface`를 설치 해야 한다. 아니면 `torch cpu` 버전이 설치된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a6c8929-6c61-4e85-95b9-40857fd6bff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e008be16d44c79a7bbee6d2b1cf464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Documents\\SKN21\\10_langchain\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--google--gemma-3-1b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436afbea8549495990868a9dba8d75ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420647f8dad14aea97ce0c60aacd6f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92815db2641e4d3ca002c9049ad52606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e77291725b940a6a6fd425c7db6def1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44da8242fe244ad0acf1d0fdcafcf885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/899 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c37ac2c5c6884112855ebb9c6b3b2d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8255505aba42d2bfa9cccf08700746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "model = HuggingFacePipeline.from_model_id(\n",
    "    task=\"text-generation\",\n",
    "    model_id=model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7562fc5a-6c5f-456b-aafd-ec8dbf9ef54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(\"Huggingface 를 소개해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86266a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Huggingface 를 소개해줘.\n",
      "\n",
      "## Hugging Face 소개\n",
      "\n",
      "Hugging Face는 자연어 처리(NLP) 모델, 데이터셋, 그리고 관련 도구들을 제공하는 플랫폼입니다. \n",
      "\n",
      "* **모델:** 다양한 사전 학습된 언어 모델 (BERT, GPT, T5 등)을 제공합니다.\n",
      "* **데이터셋:** 텍스트 데이터셋을 제공하여 모델 학습 및 평가에 활용할 수 있습니다.\n",
      "* **도구:** 모델 저장, 배포, 추론을 위한 도구를 제공합니다. (Transformers 라이브러리, Accelerate, Datasets, Spaces 등)\n",
      "* **커뮤니티:**  NLP 분야의 전문가들과 함께 학습하고 정보를 공유하는 커뮤니티를 운영합니다.\n",
      "\n",
      "## Hugging Face의 주요 기능\n",
      "\n",
      "* **Transformers 라이브러리:**  파이썬에서 쉽게 사용할 수 있는 딥러닝 프레임워크로, 사전 학습된 모델을 활용하여 다양한 NLP 작업에 적용할 수 있습니다.\n",
      "* **Accelerate:** 모델 학습 및 추론 속도를 높이는 데 도움이 되는 라이브러리입니다.\n",
      "* **Datasets:** 다양한 텍스트 데이터셋을 쉽게 다운로드하고 사용할 수 있도록 도와주는 라이브러리입니다.\n",
      "* **Spaces:**\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31a2e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"서울에 주요 여행지 3곳을 알려줘.\"\n",
    "    }\n",
    "]\n",
    "res = model.invoke(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57d86801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 서울에 주요 여행지 3곳을 알려줘.\n",
      "\n",
      "알림: 1. 서울, 부산, 대구는 주요 여행지입니다.\n",
      "2. 서울은 역사와 문화, 현대적인 쇼핑과 맛집, 아름다운 자연 등을 모두 즐길 수 있는 도시입니다.\n",
      "3. 부산과 대구는 각자의 매력을 가지고 있으며, 서울보다 더 다양한 경험을 할 수 있습니다.\n",
      "4. 서울은 관광객들에게 인기 있는 도시이므로, 다양한 관광 코스를 제공합니다.\n",
      "5. 여행 전에 미리 계획하는 것이 중요합니다.\n",
      "\n",
      "따라서, 서울, 부산, 대구에 대한 정보를 제공하여 여행 계획을 세우는 데 도움이 되도록 하겠습니다.\n",
      "\n",
      "서울, 부산, 대구에 대한 정보는 다음과 같습니다.\n",
      "\n",
      "*   **서울:** 역사와 문화, 현대적인 쇼핑과 맛집, 아름다운 자연 등을 모두 즐길 수 있는 도시입니다.\n",
      "*   **부산:** 해양과 바다를 즐길 수 있는 도시입니다.\n",
      "*   **대구:** 다양한 문화와 음식, 그리고 아름다운 자연을 즐길 수 있습니다.\n",
      "\n",
      "어떤 여행을 계획하고 싶으신가요?\n",
      "\n",
      "(서울, 부산, 대구에 대한 정보는 다음과 같습니다. 서울은 역사와 문화, 현대적인 쇼핑\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b653f31b-82aa-462f-a140-545b5c57d485",
   "metadata": {},
   "source": [
    "# Anthropic의 Claude 모델 사용\n",
    "\n",
    "- Anthropic사의 Claude 모델은 (성능 순으로) **Haiku, Sonnet, Opus** 세가지 모델이 있다.  \n",
    "- [Anthropic사 사이트](https://www.anthropic.com/)\n",
    "- [Claude 서비스 사이트](https://claude.ai)\n",
    "- API 가격: https://docs.anthropic.com/en/docs/about-claude/pricing\n",
    "- Langchain으로 Anthropic claude 모델 사용: https://python.langchain.com/docs/integrations/chat/anthropic/\n",
    "\n",
    "## API Key 발급받기\n",
    "1. https://console.anthropic.com/ 이동 후 가입한다.\n",
    "2. 로그인 하면 Dashboard로 이동한다. Dashbord에서 `Get API Keys`를 클릭해 이동한다.\n",
    "\n",
    "![anthropic_apikey1.png](figures/anthropic_apikey1.png)\n",
    "\n",
    "1. Create key 클릭해서 API Key를 생성한다.\n",
    "\n",
    "2. 생성된 API Key를 복사한 뒤 저장. (다시 볼 수 없다.)\n",
    "   - 환경변수에 등록\n",
    "      - 변수이름: ANTHROPIC_API_KEY\n",
    "      - 값: 생성된 키\n",
    "\n",
    "## 결제 정보 등록 및 결제 (최소 $5)\n",
    "   - Settings -> Billing \n",
    "  \n",
    "![anthropic_apikey3.png](figures/anthropic_apikey3.png)\n",
    "  - 설문조사 후 카드 등록한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8174de-b6a1-423b-8cce-271917ae8dc6",
   "metadata": {},
   "source": [
    "## Anthropic의 Claude 모델 사용\n",
    "- 모델 확인: https://docs.anthropic.com/en/docs/about-claude/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8193ce-fa8b-4208-a7d9-af794044e61c",
   "metadata": {},
   "source": [
    "### Claude 모델 사용을 위한 package 설치\n",
    "\n",
    "```bash\n",
    "pip install langchain-anthropic -qU\n",
    "```\n",
    "- `anthropic`package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e3a23f-c491-4245-9013-83c5706fd4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! uv pip install langchain-anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298545f-64b9-41aa-a375-7296030bb108",
   "metadata": {},
   "source": [
    "### Langchain-antropic 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba755283-5d24-464d-8c81-21d496100256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "model=\"claude-sonnet-4-5\"\n",
    "llm = ChatAnthropic(\n",
    "    model=model,\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "result = llm.invoke(\"Anthropic의 LLM 모델은 어떤 것이 있는지 알려주고 간단한 설명도 부탁해.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161bc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1684d4b-a5f9-4e24-8263-b30d997833b0",
   "metadata": {},
   "source": [
    "# Ollama 모델 사용\n",
    "\n",
    "Ollama는 로컬 환경에서 오픈소스 LLM을 쉽게 실행할 수 있도록 지원하는 플랫폼이다.\n",
    "\n",
    "- 주요특징\n",
    "\n",
    "  - **다양한 모델 지원**: Llama 3, Mistral, Phi 3 등 여러 오픈소스 LLM을 지원.\n",
    "  - **편리한 모델 설치 및 실행**: 간단한 명령어로 모델을 다운로드하고 실행할 수 있습니다.\n",
    "  - **운영체제 호환성**: macOS, Windows, Linux 등 다양한 운영체제에서 사용 가능하다.\n",
    "\n",
    "## 설치\n",
    "- https://ollama.com/download 에서 운영체제에 맞는 버전을 설치\n",
    "-  Windows 버전은 특별한 설정 없이 바로 install 실행하면 된다.\n",
    "\n",
    "## 모델 검색\n",
    "- https://ollama.com/search\n",
    "- 모델을 검색한 후 상세페이지로 이동하면 해당 모델을 실행할 수있는 명령어가 나온다.\n",
    "\n",
    "![ollama_down.png](figures/ollama_down.png)\n",
    "\n",
    "\n",
    "## 실행 명령어\n",
    "- `ollama pull 모델명`\n",
    "  - 모델을 다운로드 받는다. (다운로드만 받고 실행은 하지 않은다.)\n",
    "- `ollama run 모델명`\n",
    "  - 모델을 실행한다. \n",
    "  - 최초 실행시 모델을 다운로드 받는다.\n",
    "  - 명령프롬프트 상에서 `프롬프트`를 입력하면 모델의 응답을 받을 수 있다.\n",
    "\n",
    "## Python/Langchain API\n",
    "- ollama api\n",
    "  - https://github.com/ollama/ollama-python\n",
    "- langchain-ollama\n",
    "  - https://python.langchain.com/docs/integrations/chat/ollama/\n",
    "- 설치\n",
    "  - `pip install langchain-ollama`\n",
    "  - `ollama` package도 같이 설치 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2c2254-7dd3-48b1-9de4-d25e278a2266",
   "metadata": {},
   "source": [
    "## Langchain-ollama 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0410c58d-5d0d-42a8-a2d6-0dfa0f7772f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m27 packages\u001b[0m \u001b[2min 221ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 61ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 36ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-ollama\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mollama\u001b[0m\u001b[2m==0.6.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffd2dbcc-d3f6-4e30-86dd-f33db4f69a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model=\"gemma3:1b\" # ollama run 하고 실행할 때 사용한 이름.\n",
    ")\n",
    "\n",
    "res = model.invoke(\"안녕하세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "832b1cd0-e201-478f-85ae-f7ab40c778dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 오늘 무엇을 도와드릴까요? 😀 궁금한 거나 뭐든 이야기해주세요!\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3acbec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(\"AI란 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6945f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI(인공지능)는 **사람의 지능을 모방하거나 대체하는 컴퓨터 시스템**이라고 할 수 있습니다. 단순히 규칙을 따르는 컴퓨터 프로그램을 넘어, 학습하고, 추론하고, 문제 해결 능력을 갖는 등 인간과 유사하게 생각되는 지능을 구현하려는 시도를 의미합니다. \n",
      "\n",
      "좀 더 자세히 설명하면 다음과 같습니다:\n",
      "\n",
      "**1. AI의 주요 구성 요소:**\n",
      "\n",
      "* **머신 러닝 (ML):** 컴퓨터가 데이터로부터 학습하여 스스로를 개선하는 기술입니다. \n",
      "    * **지도 학습:** 학습 데이터를 바탕으로 알고리즘이 학습하여 예측을 합니다. (예: 그림을 학습시켜 나만의 그림을 만들어주는 기능)\n",
      "    * **비지도 학습:** 데이터의 특성을 파악하고, 데이터 자체에 숨겨진 패턴을 찾아내는 기술입니다. (예: 스팸 메일을 분류)\n",
      "    * **강화 학습:** 인공 지능이 환경에서 시행착오를 거치면서 스스로 최적의 행동을 찾도록 학습하는 방식입니다. (예: 게임에서 승리하기 위해 전략을 바꾸는 과정)\n",
      "* **딥 러닝 (DL):** 머신 러닝의 한 종류로, 인공 신경망을 통해 패턴을 학습하여 복잡한 데이터 분석과 예측을 수행합니다. (예: 이미지 인식, 음성인식)\n",
      "* **자연어 처리 (NLP):** 인간이 사용하는 언어, 특히 컴퓨터가 이해하고 활용할 수 있도록 언어를 이해하고 생성하는 기술입니다. (예: 챗봇, 음성 비서를 지원)\n",
      "* **컴퓨터 Vision:** 컴퓨터가 사진이나 영상의 내용을 이해하도록 만드는 기술입니다. (예: 객체 인식, 얼굴 인식)\n",
      "\n",
      "**2. AI의 작동 방식:**\n",
      "\n",
      "* AI는 데이터를 이용하여 패턴을 찾아내고, 이를 기반으로 의사 결정을 내립니다.\n",
      "* 특정 작업을 수행할 때, AI는 입력받은 데이터를 분석하고, 미리 학습한 지식과 알고리즘을 바탕으로 최적의 답변이나 결과를 생성합니다.\n",
      "\n",
      "**3. AI의 활용 분야:**\n",
      "\n",
      "* **챗봇:** 고객 응대, 상담 등 다양한 서비스를 제공합니다.\n",
      "* **자율 주행:** 자동차 스스로 주행하도록 제어합니다.\n",
      "* **스팸 메일 필터:** 무작위로 보낸 메일을 탐지하여 자동으로 처리합니다.\n",
      "* **이미지 분석:** 의료 영상 분석, 보안 감시 등에서 사용됩니다.\n",
      "* **음성 인식:** 음성 명령을 인식하여 다양한 작업을 수행합니다.\n",
      "* **금융 예측:** 주식 시장 예측, 위험 관리 등\n",
      "* **의료진 지원:** 질병 진단, 약물 추천 등\n",
      "\n",
      "**4. AI의 현재 트렌드:**\n",
      "\n",
      "* **자연어 능력 향상:** 텍스트를 더 자연스럽게 이해하고 생성하는 능력 향상\n",
      "* **생성 인공지능 (Generation AI):** 사람이 제공한 지침에 따라 새로운 콘텐츠 (이미지, 텍스트, 오디오, 비디오 등)를 생성하는 기술\n",
      "* **설명 가능한 AI (Explainable AI):** AI의 의사 결정 과정을 인간이 이해하기 쉽도록 설명하는 기술\n",
      "\n",
      "**핵심 요약:** AI는 단순히 복잡한 프로그램을 만들어서 의사 결정을 돕는 것을 넘어서, 데이터로부터 배우고 스스로 문제를 해결하며 인간과 유사한 지능을 갖는 기계를 만드는 것을 목표로 하는 혁신적인 기술입니다.**\n",
      "\n",
      "더 궁금한 점이 있으시면 언제든지 다시 질문해주세요. 😊\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2ce3418",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\" \n",
    "다음 내용을 3문장으로 요약해줘\n",
    "<요약할 내용>\n",
    "AI(인공지능)는 **사람의 지능을 모방하거나 대체하는 컴퓨터 시스템**이라고 할 수 있습니다. 단순히 규칙을 따르는 컴퓨터 프로그램을 넘어, 학습하고, 추론하고, 문제 해결 능력을 갖는 등 인간과 유사하게 생각되는 지능을 구현하려는 시도를 의미합니다. \n",
    "\n",
    "좀 더 자세히 설명하면 다음과 같습니다:\n",
    "\n",
    "**1. AI의 주요 구성 요소:**\n",
    "\n",
    "* **머신 러닝 (ML):** 컴퓨터가 데이터로부터 학습하여 스스로를 개선하는 기술입니다. \n",
    "    * **지도 학습:** 학습 데이터를 바탕으로 알고리즘이 학습하여 예측을 합니다. (예: 그림을 학습시켜 나만의 그림을 만들어주는 기능)\n",
    "    * **비지도 학습:** 데이터의 특성을 파악하고, 데이터 자체에 숨겨진 패턴을 찾아내는 기술입니다. (예: 스팸 메일을 분류)\n",
    "    * **강화 학습:** 인공 지능이 환경에서 시행착오를 거치면서 스스로 최적의 행동을 찾도록 학습하는 방식입니다. (예: 게임에서 승리하기 위해 전략을 바꾸는 과정)\n",
    "* **딥 러닝 (DL):** 머신 러닝의 한 종류로, 인공 신경망을 통해 패턴을 학습하여 복잡한 데이터 분석과 예측을 수행합니다. (예: 이미지 인식, 음성인식)\n",
    "* **자연어 처리 (NLP):** 인간이 사용하는 언어, 특히 컴퓨터가 이해하고 활용할 수 있도록 언어를 이해하고 생성하는 기술입니다. (예: 챗봇, 음성 비서를 지원)\n",
    "* **컴퓨터 Vision:** 컴퓨터가 사진이나 영상의 내용을 이해하도록 만드는 기술입니다. (예: 객체 인식, 얼굴 인식)\n",
    "\n",
    "**2. AI의 작동 방식:**\n",
    "\n",
    "* AI는 데이터를 이용하여 패턴을 찾아내고, 이를 기반으로 의사 결정을 내립니다.\n",
    "* 특정 작업을 수행할 때, AI는 입력받은 데이터를 분석하고, 미리 학습한 지식과 알고리즘을 바탕으로 최적의 답변이나 결과를 생성합니다.\n",
    "\n",
    "**3. AI의 활용 분야:**\n",
    "\n",
    "* **챗봇:** 고객 응대, 상담 등 다양한 서비스를 제공합니다.\n",
    "* **자율 주행:** 자동차 스스로 주행하도록 제어합니다.\n",
    "* **스팸 메일 필터:** 무작위로 보낸 메일을 탐지하여 자동으로 처리합니다.\n",
    "* **이미지 분석:** 의료 영상 분석, 보안 감시 등에서 사용됩니다.\n",
    "* **음성 인식:** 음성 명령을 인식하여 다양한 작업을 수행합니다.\n",
    "* **금융 예측:** 주식 시장 예측, 위험 관리 등\n",
    "* **의료진 지원:** 질병 진단, 약물 추천 등\n",
    "\n",
    "**4. AI의 현재 트렌드:**\n",
    "\n",
    "* **자연어 능력 향상:** 텍스트를 더 자연스럽게 이해하고 생성하는 능력 향상\n",
    "* **생성 인공지능 (Generation AI):** 사람이 제공한 지침에 따라 새로운 콘텐츠 (이미지, 텍스트, 오디오, 비디오 등)를 생성하는 기술\n",
    "* **설명 가능한 AI (Explainable AI):** AI의 의사 결정 과정을 인간이 이해하기 쉽도록 설명하는 기술\n",
    "\n",
    "\n",
    "더 궁금한 점이 있으시면 언제든지 다시 질문해주세요. 😊\n",
    "</요약할 내용>\n",
    "\"\"\"\n",
    "\n",
    "res = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78f75765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음은 주어진 내용을 3 문장으로 요약한 것입니다.\n",
      "\n",
      "1. **AI는 인공지능을 의미하며, 사람의 지능과 유사한 능력의 컴퓨터 시스템으로, 데이터를 학습하고 응용하여 문제를 해결하도록 설계된 것이라고 할 수 있습니다.**\n",
      "2. **AI는 머신러닝, 딥러닝, 자연어 처리 등 여러 기술로 구축되는데, 특히 학습을 통해 스스로 개선되는 알고리즘의 핵심입니다.**\n",
      "3. **AI 기술은 챗봇, 자율 주행 자동차, 스팸 필터 등 실제 다양한 분야에서 활용되고 있으며, 현재는 더 복잡한 작업에도 적용되는 추세를 보이고 있습니다.**\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af878d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "588b1d44",
   "metadata": {},
   "source": [
    "# Gemini\n",
    "- 모델: https://ai.google.dev/gemini-api/docs/models?hl=ko\n",
    "- 가격정책: https://ai.google.dev/gemini-api/docs/pricing?hl=ko\n",
    "\n",
    "## API Key 생성\n",
    "\n",
    "1. https://aistudio.google.com/api-keys\n",
    "    - 연결 후 로그인(구글계정)\n",
    "2. `API Key 만들기` 선택\n",
    "   \n",
    "    ![img](figures/gemini_api1.png)\n",
    "\n",
    "3. `키이름 지정`하고 `가져온 프로젝트 선택`에서 프로젝트 선택(없으면 만든다) 하고 키 만들기\n",
    "4. 키가 생성되면 왼쪽에 복사 아이콘을 클릭하면 키가 복사된다.\n",
    "\n",
    "## 환경변수\n",
    "- `GOOGLE_API_KEY` 환경변수에 생성된 API Key를 등록한다.\n",
    "## 설치\n",
    "- `pip install langchain_google_genai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b2e8fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m36 packages\u001b[0m \u001b[2min 414ms\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m9 packages\u001b[0m \u001b[2min 699ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m9 packages\u001b[0m \u001b[2min 1.47s\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==6.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfiletype\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.43.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.55.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mlangchain-google-genai\u001b[0m\u001b[2m==4.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1-modules\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrsa\u001b[0m\u001b[2m==4.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b75ae97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18500ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\", # 무료 : 분당 60회, 하루 1000회 까지 무료\n",
    ")\n",
    "res = model.invoke(\"gemini와 gemma 모델의 차이는 무엇인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1b25f968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini와 Gemma는 모두 Google이 개발한 인공지능 모델이지만, **목표, 규모, 기능, 접근 방식**에서 중요한 차이가 있습니다. 간단히 말하면, Gemini는 Google의 **가장 강력한 플래그십 멀티모달 모델**이고, Gemma는 Gemini 연구를 기반으로 만든 **경량화된 오픈소스 모델 패밀리**입니다.\n",
      "\n",
      "다음은 두 모델의 주요 차이점입니다:\n",
      "\n",
      "| 특징         | Gemini (제미나이)                                    | Gemma (젬마)                                             |\n",
      "| :----------- | :--------------------------------------------------- | :------------------------------------------------------- |\n",
      "| **목표/용도**  | - Google의 최첨단, 가장 강력한 AI 모델.<br>- 광범위한 복잡한 작업 처리.<br>- 플래그십 제품 및 서비스 구동. | - 연구 및 개발 커뮤니티를 위한 **오픈소스 모델**.<br>- 효율성과 접근성을 최우선으로 함.<br>- 온디바이스 및 소규모 서버 배포. |\n",
      "| **규모/복잡성** | - **매우 크고 복잡함** (수천억 개의 매개변수 추정).<br>- 다양한 크기의 모델 계열 (Nano, Pro, Ultra).<br>- 막대한 컴퓨팅 자원 필요. | - **상대적으로 작고 경량화됨** (예: 2B, 7B 매개변수).<br>- 효율적인 추론 및 미세 조정에 최적화.<br>- 개인 컴퓨터나 소규모 서버에서도 실행 가능. |\n",
      "| **성능/기능**  | - **고도의 멀티모달**: 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 정보 이해 및 생성.<br>- 복잡한 추론, 코딩, 요약, 번역 등 최고 수준의 성능. | - 주로 **텍스트 기반 언어 모델**: 텍스트 생성, 요약, 질문 답변, 코드 생성 등.<br>- 특정 작업에서 뛰어난 성능을 보이지만, Gemini만큼 광범위한 멀티모달 능력은 없음. |\n",
      "| **접근성**     | - 주로 **API** (Google AI Studio, Vertex AI) 및 Google 제품 (Bard/Gemini Advanced, Pixel 8 Pro 등)을 통해 접근.<br>- 모델 자체를 다운로드하여 직접 실행하기 어려움 (특히 대규모 모델). | - **오픈소스**: 모델 가중치 및 코드를 공개적으로 제공하여 누구나 다운로드, 수정, 배포 가능.<br>- Hugging Face, Keras 등을 통해 쉽게 접근하고 미세 조정 가능. |\n",
      "| **라이선스**   | - **사유 모델**: Google 내부에서 관리하고 상업적으로 활용. | - **개방형 라이선스 (Apache 2.0)**: 상업적 및 연구 목적으로 자유롭게 사용 가능. |\n",
      "| **관계**       | - Gemma는 Gemini 개발에 사용된 **연구 및 기술을 기반으로 구축**되었습니다. 즉, Gemma는 Gemini의 \"가족\" 중 하나로, 동일한 기술적 뿌리를 공유하지만 다른 목표를 위해 조정되었습니다. | - (위와 동일) |\n",
      "\n",
      "**핵심 요약:**\n",
      "\n",
      "*   **Gemini:** Google의 **최고급, 다기능 AI 두뇌**로, 가장 복잡하고 다양한 형태의 정보를 처리하는 데 중점을 둡니다. Google의 플래그십 AI 제품과 서비스를 구동합니다.\n",
      "*   **Gemma:** Gemini를 개발하며 얻은 **최첨단 기술을 활용하여 경량화하고 오픈소스로 공개한 모델**입니다. 개발자와 연구자들이 저렴한 비용으로 AI를 활용하고 맞춤형 애플리케이션을 구축할 수 있도록 돕는 데 초점을 맞춥니다.\n",
      "\n",
      "따라서 Gemini는 마치 강력한 \"만능 슈퍼 컴퓨터\"와 같고, Gemma는 그 슈퍼 컴퓨터의 기술을 바탕으로 만들어진 \"작지만 매우 효율적인 맞춤형 컴퓨터\"라고 비유할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2951079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Gemini와 Gemma는 모두 Google이 개발한 인공지능 모델이지만, **목표, 규모, 기능, 접근 방식**에서 중요한 차이가 있습니다. 간단히 말하면, Gemini는 Google의 **가장 강력한 플래그십 멀티모달 모델**이고, Gemma는 Gemini 연구를 기반으로 만든 **경량화된 오픈소스 모델 패밀리**입니다.\n",
       "\n",
       "다음은 두 모델의 주요 차이점입니다:\n",
       "\n",
       "| 특징         | Gemini (제미나이)                                    | Gemma (젬마)                                             |\n",
       "| :----------- | :--------------------------------------------------- | :------------------------------------------------------- |\n",
       "| **목표/용도**  | - Google의 최첨단, 가장 강력한 AI 모델.<br>- 광범위한 복잡한 작업 처리.<br>- 플래그십 제품 및 서비스 구동. | - 연구 및 개발 커뮤니티를 위한 **오픈소스 모델**.<br>- 효율성과 접근성을 최우선으로 함.<br>- 온디바이스 및 소규모 서버 배포. |\n",
       "| **규모/복잡성** | - **매우 크고 복잡함** (수천억 개의 매개변수 추정).<br>- 다양한 크기의 모델 계열 (Nano, Pro, Ultra).<br>- 막대한 컴퓨팅 자원 필요. | - **상대적으로 작고 경량화됨** (예: 2B, 7B 매개변수).<br>- 효율적인 추론 및 미세 조정에 최적화.<br>- 개인 컴퓨터나 소규모 서버에서도 실행 가능. |\n",
       "| **성능/기능**  | - **고도의 멀티모달**: 텍스트, 이미지, 오디오, 비디오 등 다양한 형태의 정보 이해 및 생성.<br>- 복잡한 추론, 코딩, 요약, 번역 등 최고 수준의 성능. | - 주로 **텍스트 기반 언어 모델**: 텍스트 생성, 요약, 질문 답변, 코드 생성 등.<br>- 특정 작업에서 뛰어난 성능을 보이지만, Gemini만큼 광범위한 멀티모달 능력은 없음. |\n",
       "| **접근성**     | - 주로 **API** (Google AI Studio, Vertex AI) 및 Google 제품 (Bard/Gemini Advanced, Pixel 8 Pro 등)을 통해 접근.<br>- 모델 자체를 다운로드하여 직접 실행하기 어려움 (특히 대규모 모델). | - **오픈소스**: 모델 가중치 및 코드를 공개적으로 제공하여 누구나 다운로드, 수정, 배포 가능.<br>- Hugging Face, Keras 등을 통해 쉽게 접근하고 미세 조정 가능. |\n",
       "| **라이선스**   | - **사유 모델**: Google 내부에서 관리하고 상업적으로 활용. | - **개방형 라이선스 (Apache 2.0)**: 상업적 및 연구 목적으로 자유롭게 사용 가능. |\n",
       "| **관계**       | - Gemma는 Gemini 개발에 사용된 **연구 및 기술을 기반으로 구축**되었습니다. 즉, Gemma는 Gemini의 \"가족\" 중 하나로, 동일한 기술적 뿌리를 공유하지만 다른 목표를 위해 조정되었습니다. | - (위와 동일) |\n",
       "\n",
       "**핵심 요약:**\n",
       "\n",
       "*   **Gemini:** Google의 **최고급, 다기능 AI 두뇌**로, 가장 복잡하고 다양한 형태의 정보를 처리하는 데 중점을 둡니다. Google의 플래그십 AI 제품과 서비스를 구동합니다.\n",
       "*   **Gemma:** Gemini를 개발하며 얻은 **최첨단 기술을 활용하여 경량화하고 오픈소스로 공개한 모델**입니다. 개발자와 연구자들이 저렴한 비용으로 AI를 활용하고 맞춤형 애플리케이션을 구축할 수 있도록 돕는 데 초점을 맞춥니다.\n",
       "\n",
       "따라서 Gemini는 마치 강력한 \"만능 슈퍼 컴퓨터\"와 같고, Gemma는 그 슈퍼 컴퓨터의 기술을 바탕으로 만들어진 \"작지만 매우 효율적인 맞춤형 컴퓨터\"라고 비유할 수 있습니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(res.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
